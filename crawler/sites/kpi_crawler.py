

# -*- coding: utf-8 -*-

import os
import asyncio
# json import removed as it is unused
import sys
import re
import psutil
from datetime import datetime
from dotenv import load_dotenv
from playwright.async_api import async_playwright
from upstash_redis import AsyncRedis
from jsonc_parser import parse_jsonc
from data_processor import create_data_processor, log


# ì ˆëŒ€ importë¥¼ ìœ„í•œ ê²½ë¡œ ì„¤ì •
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

# parse_jsonc is already imported at the top; remove duplicate import
# log is already imported at the top; remove duplicate import


# --- 1. ì´ˆê¸° ì„¤ì • ë° í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ---
load_dotenv("../../.env.local")

# Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
SUPABASE_URL = os.environ.get("NEXT_PUBLIC_SUPABASE_URL")
SUPABASE_KEY = os.environ.get("NEXT_PUBLIC_SUPABASE_ANON_KEY")
SUPABASE_SERVICE_KEY = os.environ.get("SUPABASE_SERVICE_KEY") or os.environ.get("SUPABASE_SERVICE_ROLE_KEY")

# Service Role í‚¤ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ anon í‚¤ ì‚¬ìš©
from data_processor import log, create_data_processor, api_monitor as supabase, get_supabase_table


# --- 2. ì›¹ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤ ---


def check_running_crawler():
    """ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ í¬ë¡¤ëŸ¬ê°€ ìˆëŠ”ì§€ í™•ì¸"""
    current_pid = os.getpid()
    current_script = os.path.basename(__file__)

    running_crawlers = []
    for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
        try:
            if proc.info['pid'] == current_pid:
                continue

            cmdline = proc.info['cmdline']
            if cmdline and any(current_script in cmd for cmd in cmdline):
                running_crawlers.append({
                    'pid': proc.info['pid'],
                    'cmdline': ' '.join(cmdline)
                })
        except (psutil.NoSuchProcess, psutil.AccessDenied):
            continue

    return running_crawlers

# --- 3. í¬ë¡¤ë§ ëŒ€ìƒ ì¹´í…Œê³ ë¦¬ ë° ë‹¨ìœ„ ì„¤ì • ---
INCLUSION_LIST_PATH = os.path.join(current_dir, "kpi_inclusion_list_compact.jsonc")
with open(INCLUSION_LIST_PATH, "r", encoding="utf-8") as f:
    jsonc_content = f.read()
INCLUSION_LIST = parse_jsonc(jsonc_content)

# --- 4. Playwright ì›¹ í¬ë¡¤ëŸ¬ í´ë˜ ---
class KpiCrawler:
    def __init__(self, target_major: str = None, target_middle: str = None,
                 target_sub: str = None, crawl_mode: str = "all",
                 start_year: str = None, start_month: str = None, max_concurrent=3):
        """
        KPI í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”

        Args:
            target_major: í¬ë¡¤ë§í•  ëŒ€ë¶„ë¥˜ëª… (Noneì´ë©´ ì „ì²´)
            target_middle: í¬ë¡¤ë§í•  ì¤‘ë¶„ë¥˜ëª… (Noneì´ë©´ ì „ì²´)
            target_sub: í¬ë¡¤ë§í•  ì†Œë¶„ë¥˜ëª… (Noneì´ë©´ ì „ì²´)
            crawl_mode: í¬ë¡¤ë§ ëª¨ë“œ
                - "all": ì „ì²´ í¬ë¡¤ë§ (ê¸°ë³¸ê°’)
                - "major_only": ì§€ì •ëœ ëŒ€ë¶„ë¥˜ë§Œ í¬ë¡¤ë§
                - "middle_only": ì§€ì •ëœ ëŒ€ë¶„ë¥˜ì˜ íŠ¹ì • ì¤‘ë¶„ë¥˜ë§Œ í¬ë¡¤ë§
                - "sub_only": ì§€ì •ëœ ëŒ€ë¶„ë¥˜ì˜ íŠ¹ì • ì¤‘ë¶„ë¥˜ì˜ íŠ¹ì • ì†Œë¶„ë¥˜ë§Œ í¬ë¡¤ë§
            start_year: ì‹œì‘ ì—°ë„ (Noneì´ë©´ í˜„ì¬ ì—°ë„)
            start_month: ì‹œì‘ ì›” (Noneì´ë©´ í˜„ì¬ ì›”)
            max_concurrent: ìµœëŒ€ ë™ì‹œ ì‹¤í–‰ ìˆ˜
        """
        self.base_url = "https://www.kpi.or.kr"
        self.max_concurrent = max_concurrent
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.supabase = supabase  # ì „ì—­ supabase ê°ì²´ ì°¸ì¡°

        # ìƒˆë¡œ ì¶”ê°€ëœ ì†ì„±ë“¤
        self.target_major_category = target_major
        self.target_middle_category = target_middle
        self.target_sub_category = target_sub
        self.crawl_mode = crawl_mode
        self.start_year = start_year or str(datetime.now().year)
        self.start_month = start_month or str(datetime.now().month)

        self.processor = create_data_processor('kpi')

        # ë°°ì¹˜ ì²˜ë¦¬ìš© ë³€ìˆ˜
        self.batch_data = []
        self.batch_size = 5  # ì†Œë¶„ë¥˜ 5ê°œë§ˆë‹¤ ì²˜ë¦¬
        self.processed_count = 0

        # Redis í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (Upstash Redis REST APIë§Œ ì‚¬ìš©)
        try:
            if 'UPSTASH_REDIS_REST_URL' in os.environ and 'UPSTASH_REDIS_REST_TOKEN' in os.environ:
                self.redis = AsyncRedis(
                    url=os.environ.get("UPSTASH_REDIS_REST_URL"),
                    token=os.environ.get("UPSTASH_REDIS_REST_TOKEN")
                )
                log("âœ… Upstash Redis REST API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ")
            else:
                self.redis = None
                log("âš ï¸ UPSTASH_REDIS_REST_URL ë˜ëŠ” UPSTASH_REDIS_REST_TOKEN í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìºì‹œ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.", "WARNING")
        except Exception as e:
            self.redis = None
            log(f"âš ï¸ Redis ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}. ìºì‹œ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.", "WARNING")

        log(f"í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” - í¬ë¡¤ë§ ëª¨ë“œ: {self.crawl_mode}")
        log(f"  íƒ€ê²Ÿ ëŒ€ë¶„ë¥˜: {self.target_major_category}")
        log(f"  íƒ€ê²Ÿ ì¤‘ë¶„ë¥˜: {self.target_middle_category}")
        log(f"  íƒ€ê²Ÿ ì†Œë¶„ë¥˜: {self.target_sub_category}")
        log(f"  ì‹œì‘ë‚ ì§œ: {self.start_year}-{self.start_month}")

    async def clear_redis_cache(self, major_name: str = None, middle_name: str = None, sub_name: str = None):
        if self.redis is None:
            log("  âš ï¸ Redisê°€ ë¹„í™œì„±í™”ë˜ì–´ ìºì‹œ ì‚­ì œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.", "WARNING")
            return
            
        try:
            if major_name and middle_name and sub_name:
                # íŠ¹ì • ì¹´í…Œê³ ë¦¬ ìºì‹œ ì‚­ì œ
                cache_key = f"material_prices:{major_name}:{middle_name}:{sub_name}"
                await self.redis.delete(cache_key)
                log(f"  âœ… Redis ìºì‹œ ì‚­ì œ ì„±ê³µ: {cache_key}")
            else:
                # ëª¨ë“  material_prices ìºì‹œ ì‚­ì œ
                keys = []
                async for key in self.redis.scan_iter("material_prices:*"):
                    keys.append(key)
                if keys:
                    await self.redis.delete(*keys)
                    log(f"  âœ… ëª¨ë“  Redis material_prices ìºì‹œ ì‚­ì œ ì„±ê³µ: {len(keys)}ê°œ")
                else:
                    log("  âœ… ì‚­ì œí•  Redis material_prices ìºì‹œê°€ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            log(f"  âŒ Redis ìºì‹œ ì‚­ì œ ì‹¤íŒ¨: {str(e)}", "ERROR")
    async def run(self):
        """í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        browser = None
        try:
            async with async_playwright() as p:
                # GitHub Actions í™˜ê²½ì—ì„œ ë” ì•ˆì •ì ì¸ ë¸Œë¼ìš°ì € ì„¤ì •
                browser = await p.chromium.launch(
                    headless=True,
                    args=[
                        '--no-sandbox',
                        '--disable-setuid-sandbox',
                        '--disable-dev-shm-usage',
                        '--disable-gpu',
                        '--disable-web-security',
                        '--disable-features=VizDisplayCompositor',
                        '--window-size=1920,1080'
                    ]
                )
                self.context = await browser.new_context()
                self.page = await self.context.new_page()

                await self._login()
                await self._navigate_to_category()
                await self._crawl_categories()

                # ë§ˆì§€ë§‰ ë‚¨ì€ ë°°ì¹˜ ë°ì´í„° ì²˜ë¦¬
                await self._process_final_batch()

                await self.clear_redis_cache()  # ìºì‹œ ì´ˆê¸°í™” ì¶”ê°€

                log(f"\nğŸŸ¢ === í¬ë¡¤ë§ ì™„ë£Œ: ì´ {self.processed_count}ê°œ ì†Œë¶„ë¥˜ ì²˜ë¦¬ë¨ === ğŸŸ¢\n")

                await browser.close()
                return self.processor
        except Exception as e:
            log(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "ERROR")
            if browser:
                try:
                    await browser.close()
                except:
                    pass
            raise

    async def _login(self):
        """ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ì´ë™í•˜ì—¬ ë¡œê·¸ì¸ ìˆ˜í–‰ (íƒ€ì„ì•„ì›ƒ ë° ì¬ì‹œë„ ë¡œì§ ì¶”ê°€)"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                # íƒ€ì„ì•„ì›ƒì„ 60ì´ˆë¡œ ëŠ˜ë¦¬ê³ , ë„¤íŠ¸ì›Œí¬ê°€ ì•ˆì •ë  ë•Œê¹Œì§€ ëŒ€ê¸°
                await self.page.goto(
                    f"{self.base_url}/www/member/login.asp",
                    timeout=60000,
                    wait_until="networkidle"
                )
                log("ë¡œê·¸ì¸ í˜ì´ì§€ ì´ë™ ì„±ê³µ")
                break  # ì„±ê³µ ì‹œ ì¬ì‹œë„ ì¤‘ë‹¨
            except Exception as e:
                log(f"ë¡œê·¸ì¸ í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}", "WARNING")
                if attempt < max_retries - 1:
                    await asyncio.sleep(5 * (attempt + 1))  # 5ì´ˆ, 10ì´ˆ ê°„ê²©ìœ¼ë¡œ ëŒ€ê¸° í›„ ì¬ì‹œë„
                else:
                    log("ë¡œê·¸ì¸ í˜ì´ì§€ ì´ë™ì— ìµœì¢… ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.", "ERROR")
                    raise  # ìµœì¢… ì‹¤íŒ¨ ì‹œ ì˜¤ë¥˜ ë°œìƒ

        username = os.environ.get("KPI_USERNAME")
        password = os.environ.get("KPI_PASSWORD")

        if not username or not password:
            raise ValueError(".env.local íŒŒì¼ì— KPI_USERNAMEê³¼ "
                             "KPI_PASSWORDë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.")

        # GitHub Actions í™˜ê²½ì—ì„œ ë” ì•ˆì •ì ì¸ ë¡œê·¸ì¸ ì²˜ë¦¬
        await self.page.wait_for_load_state('networkidle', timeout=45000)
        await asyncio.sleep(2)

        await self.page.locator("#user_id").fill(username)
        await asyncio.sleep(1)
        await self.page.locator("#user_pw").fill(password)
        await asyncio.sleep(1)
        await self.page.locator("#sendLogin").click()

        # ë¡œê·¸ì¸ ì™„ë£Œ ëŒ€ê¸°ì‹œê°„ ì¦ê°€
        await self.page.wait_for_load_state('networkidle', timeout=45000)
        await asyncio.sleep(3)
        log("ë¡œê·¸ì¸ ì™„ë£Œ", "SUCCESS")

    async def _navigate_to_category(self):
        """ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ë¡œ ì´ë™ ë° ì´ˆê¸° ì„¤ì • (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
        log("ì¢…í•©ë¬¼ê°€ì •ë³´ ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ë¡œ ì´ë™í•©ë‹ˆë‹¤.")

        max_retries = 3
        retry_count = 0

        while retry_count < max_retries:
            try:
                # íƒ€ì„ì•„ì›ƒì„ 60ì´ˆë¡œ ì¦ê°€
                await self.page.goto(
                    f"{self.base_url}/www/price/category.asp",
                    timeout=60000,  # 60ì´ˆ
                    wait_until="domcontentloaded"  # ë” ë¹ ë¥¸ ë¡œë”© ì™„ë£Œ ì¡°ê±´
                )

                # í˜ì´ì§€ ë¡œë”© ì™„ë£Œ ëŒ€ê¸°
                await self.page.wait_for_load_state('networkidle', timeout=60000)

                # íŒì—… ë‹«ê¸° (ìš°ì„  ì²˜ë¦¬)
                await self._close_popups()

                # Right Quick ë©”ë‰´ ìˆ¨ê¸°ê¸°
                try:
                    close_button = self.page.locator("#right_quick .q_cl")
                    if await close_button.is_visible():
                        await close_button.click()
                        log("Right Quick ë©”ë‰´ë¥¼ ìˆ¨ê²¼ìŠµë‹ˆë‹¤.")
                except Exception as e:
                    log(f"Right Quick ë©”ë‰´ ìˆ¨ê¸°ê¸° ì‹¤íŒ¨ "
                        f"(ì´ë¯¸ ìˆ¨ê²¨ì ¸ ìˆì„ ìˆ˜ ìˆìŒ): {e}")

                log("ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ ì´ë™ ì™„ë£Œ", "SUCCESS")
                return  # ì„±ê³µ ì‹œ í•¨ìˆ˜ ì¢…ë£Œ

            except Exception as e:
                retry_count += 1
                log(f"ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨ (ì‹œë„ {retry_count}/{max_retries}): {e}", "WARNING")

                if retry_count < max_retries:
                    wait_time = retry_count * 5  # 5ì´ˆ, 10ì´ˆ, 15ì´ˆ ëŒ€ê¸°
                    log(f"{wait_time}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...", "INFO")
                    await asyncio.sleep(wait_time)
                else:
                    log("ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ ì´ë™ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼", "ERROR")
                    raise e

    async def _close_popups(self):
        """í˜ì´ì§€ì˜ ëª¨ë“  íŒì—…ì„ ë‹«ëŠ” ë©”ì„œë“œ"""
        try:
            # 1. ì¼ë°˜ì ì¸ íŒì—… ë‹«ê¸° ë²„íŠ¼ë“¤ ì‹œë„
            popup_close_selectors = [
                ".pop-btn-close",  # ì¼ë°˜ì ì¸ íŒì—… ë‹«ê¸° ë²„íŠ¼
                ".btnClosepop",    # íŠ¹ì • íŒì—… ë‹«ê¸° ë²„íŠ¼
                "#popupNotice .pop-btn-close",  # ê³µì§€ì‚¬í•­ íŒì—… ë‹«ê¸°
                ".ui-popup .pop-btn-close",     # UI íŒì—… ë‹«ê¸°
                "button[class*='close']",       # closeê°€ í¬í•¨ëœ ë²„íŠ¼
                "a[class*='close']"             # closeê°€ í¬í•¨ëœ ë§í¬
            ]

            for selector in popup_close_selectors:
                try:
                    popup_close = self.page.locator(selector)
                    if await popup_close.count() > 0:
                        # ëª¨ë“  ë§¤ì¹­ë˜ëŠ” ìš”ì†Œì— ëŒ€í•´ ë‹«ê¸° ì‹œë„
                        for i in range(await popup_close.count()):
                            element = popup_close.nth(i)
                            if await element.is_visible():
                                await element.click(timeout=3000)
                                log(f"íŒì—… ë‹«ê¸° ì„±ê³µ: {selector}")
                                await self.page.wait_for_timeout(500)  # íŒì—…ì´ ë‹«í ì‹œê°„ ëŒ€ê¸°
                except Exception:
                    continue  # ë‹¤ìŒ ì…€ë ‰í„° ì‹œë„

            # 2. ESC í‚¤ë¡œ íŒì—… ë‹«ê¸° ì‹œë„
            await self.page.keyboard.press('Escape')
            await self.page.wait_for_timeout(500)

            log("íŒì—… ë‹«ê¸° ì²˜ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            log(f"íŒì—… ë‹«ê¸° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    async def _crawl_categories(self):
        """ëŒ€ë¶„ë¥˜ -> ì¤‘ë¶„ë¥˜ -> ì†Œë¶„ë¥˜ ìˆœì°¨ì ìœ¼ë¡œ í¬ë¡¤ë§"""
        major_selector = '#left_menu_kpi > ul.panel'
        major_categories = await self.page.locator(
            major_selector).first.locator('li.file-item > a').all()

        major_links = []
        for cat in major_categories:
            text = await cat.inner_text()
            href = await cat.get_attribute('href')
            major_links.append({'name': text, 'url': f"{self.base_url}{href}"})

        for major in major_links:
            # í¬ë¡¤ë§ ëª¨ë“œì— ë”°ë¥¸ ëŒ€ë¶„ë¥˜ í•„í„°ë§
            if self.target_major_category:
                # íƒ€ê²Ÿ ëŒ€ë¶„ë¥˜ê°€ ì§€ì •ëœ ê²½ìš°, í•´ë‹¹ ëŒ€ë¶„ë¥˜ë§Œ ì²˜ë¦¬
                if major['name'] != self.target_major_category:
                    continue  # íƒ€ê²Ÿ ëŒ€ë¶„ë¥˜ê°€ ì•„ë‹ˆë©´ ê±´ë„ˆë›°ê¸°

            log(f"ëŒ€ë¶„ë¥˜ '{major['name']}' í¬ë¡¤ë§ ì‹œì‘...")
            await self.page.goto(major['url'])

            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° - ê¸°ë³¸ ëŒ€ê¸° ì‹œê°„ë§Œ ì ìš© (ì„œë²„ í™˜ê²½ ê³ ë ¤í•˜ì—¬ 10ì´ˆ)
            log("í˜ì´ì§€ ë¡œë”©ì„ ìœ„í•œ ê¸°ë³¸ ëŒ€ê¸° ì‹œê°„ ì ìš© (10ì´ˆ)")
            await self.page.wait_for_timeout(10000)

            # ì£¼ì„ì²˜ë¦¬: ë¶ˆí•„ìš”í•œ ì„ íƒì ì‹œë„ ë¶€ë¶„ (ê³„ì† ì‹¤íŒ¨í•˜ì—¬ ì‹œê°„ë§Œ ì†Œëª¨)
            # try:
            #     # ì¹´í…Œê³ ë¦¬ ëª©ë¡ ì»¨í…Œì´ë„ˆ ëŒ€ê¸° (ì—¬ëŸ¬ ì„ íƒì ì‹œë„)
            #     selectors_to_try = [
            #         ".part-list",  # ê¸°ì¡´ ì„ íƒì
            #         "ul li a[href*='CATE_CD=']",  # ì¹´í…Œê³ ë¦¬ ë§í¬ë“¤
            #         "li a[href*='/www/price/category.asp']",  # ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ ë§í¬ë“¤
            #         ".category-list",  # ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤íŠ¸ í´ë˜ìŠ¤
            #     ]
            #
            #     page_loaded = False
            #     for selector in selectors_to_try:
            #         try:
            #             await self.page.wait_for_selector(selector, timeout=10000)
            #             log(f"í˜ì´ì§€ ë¡œë”© ì™„ë£Œ - ì„ íƒì: {selector}")
            #             page_loaded = True
            #             break
            #         except Exception as e:
            #             log(f"ì„ íƒì {selector} ëŒ€ê¸° ì‹¤íŒ¨: {str(e)}")
            #             continue
            #
            #     if not page_loaded:
            #         log("ëª¨ë“  ì„ íƒì ì‹œë„ ì‹¤íŒ¨, ê¸°ë³¸ ëŒ€ê¸° ì‹œê°„ ì ìš©")
            #         await self.page.wait_for_timeout(3000)
            #
            # except Exception as e:
            #     log(f"í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘ ì˜¤ë¥˜: {str(e)}")
            #     await self.page.wait_for_timeout(3000)

            # openSub() ë²„íŠ¼ í´ë¦­í•˜ì—¬ ëª¨ë“  ì¤‘ë¶„ë¥˜ì™€ ì†Œë¶„ë¥˜ë¥¼ í•œë²ˆì— í¼ì¹˜ê¸°
            open_sub_selector = 'a[href="javascript:openSub();"]'
            open_sub_button = self.page.locator(open_sub_selector)
            if await open_sub_button.count() > 0:
                log("openSub() ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ëª¨ë“  ë¶„ë¥˜ë¥¼ í¼ì¹©ë‹ˆë‹¤.")
                await open_sub_button.click()
                # ë¶„ë¥˜ê°€ í¼ì³ì§ˆ ì‹œê°„ì„ ê¸°ë‹¤ë¦¼ (2ì´ˆ -> 5ì´ˆë¡œ ì¦ê°€)
                await self.page.wait_for_timeout(5000) # ë” ë„‰ë„‰í•œ ëŒ€ê¸° ì‹œê°„
                # ë˜ëŠ”, íŠ¹ì • ì¤‘ë¶„ë¥˜ ëª©ë¡ì´ ì‹¤ì œë¡œ visible í•´ì§ˆ ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ê¸° (ë” ê²¬ê³ í•œ ë°©ë²•)
                try:
                    await self.page.wait_for_selector('.part-ttl > a', state='visible', timeout=15000)
                    log("ì¤‘ë¶„ë¥˜ ìš”ì†Œë“¤ì´ í™”ë©´ì— ì™„ì „íˆ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
                except Exception as e:
                    log(f"ì¤‘ë¶„ë¥˜ ìš”ì†Œ ê°€ì‹œì„± ëŒ€ê¸° ì‹¤íŒ¨: {e}", "WARNING")
                    # ì‹¤íŒ¨í•´ë„ ì¼ë‹¨ ì§„í–‰í•˜ë„ë¡ í•¨. ë‹¤ìŒ ë¡œì§ì—ì„œ ë‹¤ì‹œ ìš”ì†Œë¥¼ ì°¾ì„ ê²ƒì´ë¯€ë¡œ.

                # HTML êµ¬ì¡° í™•ì¸ì„ ìœ„í•´ í˜ì´ì§€ ë‚´ìš© ì¶œë ¥
                html_content = await self.page.content()
                # HTML êµ¬ì¡° í™•ì¸ì„ ìœ„í•´ í˜ì´ì§€ ë‚´ìš© ì¶œë ¥
                part_ttl_start = html_content.find('part-ttl')
                if 'part-ttl' in html_content:
                    sample_end = part_ttl_start + 1000
                    html_sample = html_content[part_ttl_start:sample_end]
                    log(f"í˜ì´ì§€ HTML ìƒ˜í”Œ (part-ttl ê´€ë ¨): {html_sample}")
                else:
                    log("í˜ì´ì§€ HTML ìƒ˜í”Œ (part-ttl ê´€ë ¨): "
                        "part-ttl ì—†ìŒ")
            else:
                # ëŒ€ë¶„ë¥˜ 'ê³µí†µìì¬' í´ë¦­í•˜ì—¬ ì¤‘ë¶„ë¥˜ ëª©ë¡ í¼ì¹˜ê¸°
                # ì´ ë¶€ë¶„ë„ ìœ„ì— openSub() ì²˜ëŸ¼ ëŒ€ê¸° ì‹œê°„ì„ ëŠ˜ë ¤ì£¼ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
                category_link = 'a[href="category.asp?CATE_CD=101"]'
                await self.page.click(category_link)
                await self.page.wait_for_timeout(3000) # 1ì´ˆ -> 3ì´ˆë¡œ ì¦ê°€
                log("ì¤‘ë¶„ë¥˜ ë° ì†Œë¶„ë¥˜ ëª©ë¡ì„ í¼ì³¤ìŠµë‹ˆë‹¤.")

            # ì†Œë¶„ë¥˜ ëª©ë¡ì´ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ëŒ€ê¸°
            # ì´ wait_for_selectorëŠ” ì‚¬ì‹¤ìƒ .part-ttl ì•„ë˜ì˜ ì¤‘ë¶„ë¥˜ë¥¼ ëŒ€ê¸°í•˜ëŠ” ê²ƒì´ë¯€ë¡œ,
            # .part-ttlì´ visible ìƒíƒœê°€ ë˜ëŠ” ê²ƒì„ ê¸°ë‹¤ë¦¬ëŠ” ê²ƒì´ ë” ì •í™•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            await self.page.wait_for_selector(".part-list", timeout=15000) # íƒ€ì„ì•„ì›ƒ ì¦ê°€

            # ì¤‘ë¶„ë¥˜ ì •ë³´ë¥¼ ë¯¸ë¦¬ ìˆ˜ì§‘
            middle_selector = '.part-ttl > a'
            middle_categories_elements = await self.page.locator(
                middle_selector).all()
            log(f"  ë°œê²¬ëœ ì¤‘ë¶„ë¥˜ ê°œìˆ˜: {len(middle_categories_elements)}")

            middle_categories_info = []
            for i, middle_element in enumerate(middle_categories_elements):
                try:
                    middle_name = await middle_element.inner_text()
                    middle_href = await middle_element.get_attribute('href')
                    if middle_href and 'CATE_CD=' in middle_href:
                        middle_categories_info.append({
                            'name': middle_name,
                            'href': middle_href
                        })
                        log(f"  ë°œê²¬ëœ ì¤‘ë¶„ë¥˜: '{middle_name}' "
                            f"(CATE_CD: {middle_href})")
                except Exception as e:
                    log(f"  ì¤‘ë¶„ë¥˜ {i + 1} ì •ë³´ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    continue

            # ê° ì¤‘ë¶„ë¥˜ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ë°©ë¬¸í•˜ì—¬ ì†Œë¶„ë¥˜ ìˆ˜ì§‘
            for middle_info in middle_categories_info:
                middle_name = middle_info['name']
                middle_href = middle_info['href']

                # í¬ë¡¤ë§ ëª¨ë“œì— ë”°ë¥¸ ì¤‘ë¶„ë¥˜ í•„í„°ë§
                if self.crawl_mode in ["middle_only", "sub_only"] and self.target_middle_category:
                    if middle_name != self.target_middle_category:
                        log(f"  [SKIP] íƒ€ê²Ÿ ì¤‘ë¶„ë¥˜ê°€ ì•„ë‹˜: '{middle_name}' ê±´ë„ˆëœë‹ˆë‹¤.")
                        continue
                elif self.crawl_mode == "major_only":
                    # major_only ëª¨ë“œì—ì„œëŠ” ëª¨ë“  ì¤‘ë¶„ë¥˜ ì²˜ë¦¬
                    pass

                # ê¸°ì¡´ INCLUSION_LIST ë¡œì§ (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)
                inclusions_for_major = INCLUSION_LIST.get(major['name'], {})

                # ëŒ€ë¶„ë¥˜ì— ì„¤ì •ì´ ì—†ìœ¼ë©´ ëª¨ë“  ì¤‘ë¶„ë¥˜ ì œì™¸ (ë‹¨, ìƒˆë¡œìš´ ëª¨ë“œì—ì„œëŠ” ë¬´ì‹œ)
                if not inclusions_for_major and self.crawl_mode == "all":
                    log(f"  [SKIP] í¬í•¨ ëª©ë¡ ì—†ìŒ: ì¤‘ë¶„ë¥˜ '{middle_name}' ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue

                # ëŒ€ë¶„ë¥˜ê°€ "__ALL__"ë¡œ ì„¤ì •ëœ ê²½ìš° ëª¨ë“  ì¤‘ë¶„ë¥˜ í¬í•¨
                if inclusions_for_major == "__ALL__":
                    log(f"  ì¤‘ë¶„ë¥˜ '{middle_name}' í¬í•¨ (ëŒ€ë¶„ë¥˜ ì „ì²´ í¬í•¨ ì„¤ì •)")
                else:
                    # ì¤‘ë¶„ë¥˜ê°€ í¬í•¨ ëª©ë¡ì— ì—†ìœ¼ë©´ ì œì™¸
                    if middle_name not in inclusions_for_major:
                        log(f"  [SKIP] í¬í•¨ ëª©ë¡ì— ì—†ìŒ: ì¤‘ë¶„ë¥˜ '{middle_name}' ê±´ë„ˆëœë‹ˆë‹¤.")
                        continue

                try:
                    # ì¤‘ë¶„ë¥˜ í˜ì´ì§€ë¡œ ì´ë™
                    middle_url = f"{self.base_url}/www/price/{middle_href}"
                    log(f"  ì¤‘ë¶„ë¥˜ '{middle_name}' "
                        f"í˜ì´ì§€ë¡œ ì´ë™: {middle_url}")
                    await self.page.goto(middle_url)
                    await self.page.wait_for_load_state(
                        'networkidle')

                    # ì¤‘ë¶„ë¥˜ CATE_CD ì¶”ì¶œ
                    middle_cate_cd_match = re.search(r'CATE_CD=(\d+)', middle_href)
                    middle_cate_cd = middle_cate_cd_match.group(1) if middle_cate_cd_match else None

                    if not middle_cate_cd:
                        log(f"  [SKIP] ì¤‘ë¶„ë¥˜ '{middle_name}'ì˜ CATE_CDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                        continue

                    # ì†Œë¶„ë¥˜ê°€ ìˆ¨ê²¨ì ¸ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì§ì ‘ ì°¾ê¸°
                    await self.page.wait_for_timeout(2000)

                    # ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ì†Œë¶„ë¥˜ ì°¾ê¸°
                    sub_categories_info = []

                    # ë°©ë²• 1: ul.part-list ë‚´ì˜ ë§í¬ë“¤
                    part_list_selector = 'ul.part-list'
                    part_lists = await self.page.locator(
                        part_list_selector).all()
                    for part_list in part_lists:
                        if await part_list.count() > 0:
                            sub_selector = 'li a'
                            sub_elements = await part_list.locator(
                                sub_selector).all()
                            for sub_element in sub_elements:
                                try:
                                    sub_name = await sub_element.inner_text()
                                    sub_href = await sub_element.get_attribute(
                                        'href')
                                    has_cate_cd = (
                                        sub_href and
                                        'CATE_CD=' in sub_href)
                                    if has_cate_cd:
                                        sub_cate_cd_match = re.search(r'CATE_CD=(\d+)', sub_href)
                                        sub_cate_cd = sub_cate_cd_match.group(1) if sub_cate_cd_match else None

                                        # ì¤‘ë¶„ë¥˜ CATE_CDì™€ ì†Œë¶„ë¥˜ CATE_CDê°€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
                                        if sub_cate_cd and sub_cate_cd.startswith(middle_cate_cd):
                                            sub_categories_info.append({
                                                'name': sub_name,
                                                'href': sub_href
                                            })
                                            log(f"    ë°œê²¬ëœ ì†Œë¶„ë¥˜: "
                                                f"'{sub_name}' (CATE_CD: {sub_cate_cd})")
                                        else:
                                            log(f"    [SKIP] ì¤‘ë¶„ë¥˜ '{middle_name}'ê³¼ "
                                                f"ì—°ê´€ ì—†ëŠ” ì†Œë¶„ë¥˜ '{sub_name}' (CATE_CD: {sub_cate_cd}) ê±´ë„ˆëœœ")
                                except Exception as e:
                                    log(f"    ì†Œë¶„ë¥˜ ì •ë³´ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: "
                                        f"{str(e)}")
                                    continue

                    # ë°©ë²• 2: ë§Œì•½ ìœ„ì—ì„œ ì°¾ì§€ ëª»í–ˆë‹¤ë©´ ë‹¤ë¥¸ ì„ íƒì ì‹œë„
                    if not sub_categories_info:
                        try:
                            # ì†Œë¶„ë¥˜ ë§í¬ ì°¾ê¸°
                            detail_selector = (
                                'a[href*="detail.asp?CATE_CD="]')
                            all_links = await self.page.locator(
                                detail_selector).all()
                            for link in all_links:
                                try:
                                    sub_name = await link.inner_text()
                                    sub_href = await link.get_attribute(
                                        'href')
                                    if sub_href and sub_name.strip():
                                        sub_cate_cd_match = re.search(r'CATE_CD=(\d+)', sub_href)
                                        sub_cate_cd = sub_cate_cd_match.group(1) if sub_cate_cd_match else None

                                        # ì¤‘ë¶„ë¥˜ CATE_CDì™€ ì†Œë¶„ë¥˜ CATE_CDê°€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
                                        if sub_cate_cd and sub_cate_cd.startswith(middle_cate_cd):
                                            sub_categories_info.append({
                                                'name': sub_name.strip(),
                                                'href': sub_href
                                            })
                                            log(f"    ë°œê²¬ëœ ì†Œë¶„ë¥˜: "
                                                f"'{sub_name}' (CATE_CD: {sub_cate_cd})")
                                        else:
                                            log(f"    [SKIP] ì¤‘ë¶„ë¥˜ '{middle_name}'ê³¼ "
                                                f"ì—°ê´€ ì—†ëŠ” ì†Œë¶„ë¥˜ '{sub_name}' (CATE_CD: {sub_cate_cd}) ê±´ë„ˆëœœ")
                                except Exception:
                                    continue
                        except Exception as e:
                            # ë°©ë²•2 ì†Œë¶„ë¥˜ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ë¡œê·¸ ìƒëµ)
                            pass

                    if not sub_categories_info:
                        log(f"    ì¤‘ë¶„ë¥˜ '{middle_name}'ì˜ "
                            f"ì†Œë¶„ë¥˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        continue

                    sub_count = len(sub_categories_info)
                    log(f"    ì¤‘ë¶„ë¥˜ '{middle_name}' - "
                        f"ë°œê²¬ëœ ì†Œë¶„ë¥˜ ê°œìˆ˜: {sub_count}")

                    # ìˆ˜ì§‘ëœ ì†Œë¶„ë¥˜ ì •ë³´ë¡œ ë³‘ë ¬ ë°ì´í„° í¬ë¡¤ë§
                    await self._crawl_subcategories_parallel(
                        major['name'], middle_name, sub_categories_info)

                except Exception as e:
                    log(f"  ì¤‘ë¶„ë¥˜ '{middle_name}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    continue

    async def _crawl_subcategories_parallel(self, major_name,
                                            middle_name,
                                            sub_categories_info):
        """ì†Œë¶„ë¥˜ë“¤ì„ ë³‘ë ¬ë¡œ í¬ë¡¤ë§"""
        
        # í¬ë¡¤ë§ ëª¨ë“œì— ë”°ë¥¸ ì†Œë¶„ë¥˜ í•„í„°ë§
        if self.crawl_mode == "sub_only" and self.target_sub_category:
            filtered_subs = []
            log(f"    [DEBUG] íƒ€ê²Ÿ ì†Œë¶„ë¥˜: '{self.target_sub_category}' (ê¸¸ì´: {len(self.target_sub_category)})")
            log(f"    [DEBUG] íƒ€ê²Ÿ ì†Œë¶„ë¥˜ ë°”ì´íŠ¸: {self.target_sub_category.encode('utf-8')}")
            
            # ìœ ë‹ˆì½”ë“œ ì •ê·œí™” import
            import unicodedata
            normalized_target = unicodedata.normalize('NFKC', self.target_sub_category)
            log(f"    [DEBUG] íƒ€ê²Ÿ ì†Œë¶„ë¥˜ ì •ê·œí™”: '{normalized_target}'")
            
            for sub_info in sub_categories_info:
                web_name = sub_info['name']
                normalized_web = unicodedata.normalize('NFKC', web_name)
                
                log(f"    [DEBUG] ì›¹ì‚¬ì´íŠ¸ ì†Œë¶„ë¥˜: '{web_name}' -> ì •ê·œí™”: '{normalized_web}'")
                log(f"    [DEBUG] ì •ê·œí™”ëœ ë¬¸ìì—´ ë¹„êµ ê²°ê³¼: {normalized_web == normalized_target}")
                
                if normalized_web == normalized_target:
                    log(f"    [MATCH] ì†Œë¶„ë¥˜ ë§¤ì¹­ ì„±ê³µ: '{web_name}'")
                    filtered_subs.append(sub_info)
                else:
                    log(f"    [SKIP] íƒ€ê²Ÿ ì†Œë¶„ë¥˜ê°€ ì•„ë‹˜: '{web_name}' ê±´ë„ˆëœë‹ˆë‹¤.")
                    sub_info['skip_reason'] = "íƒ€ê²Ÿ ì†Œë¶„ë¥˜ê°€ ì•„ë‹˜"
            sub_categories_info = filtered_subs
        elif self.crawl_mode in ["major_only", "middle_only"]:
            # major_only, middle_only ëª¨ë“œì—ì„œëŠ” ëª¨ë“  ì†Œë¶„ë¥˜ ì²˜ë¦¬
            pass
        elif self.crawl_mode == "all":
            # ê¸°ì¡´ INCLUSION_LIST ë¡œì§ (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)
            inclusions_for_major = INCLUSION_LIST.get(major_name, {})

            # ëŒ€ë¶„ë¥˜ê°€ "__ALL__"ë¡œ ì„¤ì •ëœ ê²½ìš° ëª¨ë“  ì¤‘ë¶„ë¥˜ì™€ ì†Œë¶„ë¥˜ í¬í•¨
            if inclusions_for_major == "__ALL__":
                log(f"    ëŒ€ë¶„ë¥˜ '{major_name}' ì „ì²´ í¬í•¨ ì„¤ì • - ì¤‘ë¶„ë¥˜ '{middle_name}' ëª¨ë“  ì†Œë¶„ë¥˜ í¬í•¨")
            else:
                sub_inclusion_rule = inclusions_for_major.get(middle_name, {})

                # ì¤‘ë¶„ë¥˜ê°€ "__ALL__"ì´ ì•„ë‹Œ ê²½ìš°, íŠ¹ì • ì†Œë¶„ë¥˜ë§Œ í¬í•¨
                if sub_inclusion_rule != "__ALL__":
                    if isinstance(sub_inclusion_rule, dict) and sub_inclusion_rule:
                        filtered_subs = []
                        for sub_info in sub_categories_info:
                            if sub_info['name'] in sub_inclusion_rule:
                                filtered_subs.append(sub_info)
                            else:
                                log(f"    [SKIP] í¬í•¨ ëª©ë¡ì— ì—†ìŒ: ì†Œë¶„ë¥˜ '{sub_info['name']}' ê±´ë„ˆëœë‹ˆë‹¤.")
                                sub_info['skip_reason'] = "í¬í•¨ ëª©ë¡ì— ì—†ìŒ"
                        sub_categories_info = filtered_subs  # í•„í„°ë§ëœ ëª©ë¡ìœ¼ë¡œ êµì²´
                    else:
                        # ë¹ˆ ë”•ì…”ë„ˆë¦¬ì´ê±°ë‚˜ ì˜ëª»ëœ í˜•ì‹ì¸ ê²½ìš° ëª¨ë“  ì†Œë¶„ë¥˜ ì œì™¸
                        log(f"    [SKIP] í¬í•¨í•  ì†Œë¶„ë¥˜ ì—†ìŒ: ì¤‘ë¶„ë¥˜ '{middle_name}' ëª¨ë“  ì†Œë¶„ë¥˜ ê±´ë„ˆëœë‹ˆë‹¤.")
                        return

        if not sub_categories_info:
            log(f"    ì¤‘ë¶„ë¥˜ '{middle_name}': "
                f"ì²˜ë¦¬í•  ì†Œë¶„ë¥˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        sub_count = len(sub_categories_info)
        log(f"    ì¤‘ë¶„ë¥˜ '{middle_name}': {sub_count}ê°œ "
            f"ì†Œë¶„ë¥˜ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

        # ë³‘ë ¬ ì‘ì—… ìƒì„±
        tasks = []
        for sub_info in sub_categories_info:
            task = self._crawl_single_subcategory(
                major_name, middle_name, sub_info)
            tasks.append(task)

        # ë³‘ë ¬ ì‹¤í–‰
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # ê²°ê³¼ ì²˜ë¦¬ ë° ë°°ì¹˜ ë°ì´í„° ìˆ˜ì§‘
        success_count = 0
        failed_count = 0
        
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                sub_name = sub_categories_info[i]['name']
                log(f"    âŒ ì†Œë¶„ë¥˜ '{sub_name}' ì²˜ë¦¬ ì‹¤íŒ¨: {str(result)}", "ERROR")
                failed_count += 1
            elif result is None:
                sub_name = sub_categories_info[i]['name']
                log(f"    âš ï¸ ì†Œë¶„ë¥˜ '{sub_name}' ì²˜ë¦¬ ê²°ê³¼ ì—†ìŒ", "WARNING")
                failed_count += 1
            else:
                success_count += 1
                # ì„±ê³µí•œ ì†Œë¶„ë¥˜ ë°ì´í„°ë¥¼ ë°°ì¹˜ì— ì¶”ê°€
                sub_info = sub_categories_info[i]
                self.batch_data.append({
                    'major': major_name,
                    'middle': middle_name,
                    'sub': sub_info['name'],
                    'result': result
                })

        log(f"    ì¤‘ë¶„ë¥˜ '{middle_name}' ì™„ë£Œ: {success_count}/{sub_count}ê°œ ì„±ê³µ, {failed_count}ê°œ ì‹¤íŒ¨")
        
        # ë°°ì¹˜ í¬ê¸°ì— ë„ë‹¬í•˜ë©´ ì²˜ë¦¬
        if len(self.batch_data) >= self.batch_size:
            await self._process_batch()

        total_count = len(sub_categories_info)
        log(f"    ì¤‘ë¶„ë¥˜ '{middle_name}' ì™„ë£Œ: "
            f"{success_count}/{total_count}ê°œ ì„±ê³µ")
    
    async def _process_batch(self):
        """ë°°ì¹˜ ë°ì´í„° ì²˜ë¦¬ (pandas ê°€ê³µ ë° supabase ì €ì¥)"""
        if not self.batch_data:
            return
            
        batch_count = len(self.batch_data)
        log(f"\n=== ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {batch_count}ê°œ ì†Œë¶„ë¥˜ ===\n")
        
        # ê° ì†Œë¶„ë¥˜ë³„ë¡œ ë°ì´í„° ì²˜ë¦¬
        total_processed = 0
        total_saved = 0
        
        for batch_item in self.batch_data:
            try:
                # pandas ê°€ê³µ
                processed_data = await self.processor.process_data(
                    batch_item['major'], 
                    batch_item['middle'], 
                    batch_item['sub']
                )
                
                if processed_data:
                    processed_count = len(processed_data)
                    total_processed += processed_count
                    
                    # supabase ì €ì¥
                    saved_count = await self.processor.save_to_supabase(processed_data)
                    total_saved += saved_count
                    
                    if saved_count == 0:
                        log(f"  - {batch_item['sub']}: pandas ê°€ê³µ {processed_count}ê°œ, "
                            f"supabase ì €ì¥ 0ê°œ (ëª¨ë‘ ì¤‘ë³µ ë°ì´í„°)")
                    else:
                        log(f"  - {batch_item['sub']}: pandas ê°€ê³µ {processed_count}ê°œ, "
                            f"supabase ì €ì¥ {saved_count}ê°œ")
                else:
                    # ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ëŠ” ì´ìœ ë¥¼ ëª…í™•íˆ êµ¬ë¶„
                    if batch_item.get('skip_reason') == 'not_found':
                        log(f"  - {batch_item['sub']}: ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì†Œë¶„ë¥˜ëª… ë¯¸ë°œê²¬")
                    elif batch_item.get('skip_reason') == 'no_target':
                        log(f"  - {batch_item['sub']}: íƒ€ê²Ÿ ì†Œë¶„ë¥˜ê°€ ì•„ë‹˜ (í•„í„°ë§ë¨)")
                    else:
                        log(f"  - {batch_item['sub']}: ì²˜ë¦¬í•  ë°ì´í„° ì—†ìŒ (ì›ì¸ ë¯¸ìƒ)")
                    
            except Exception as e:
                log(f"  - {batch_item['sub']} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        
        log(f"\n=== ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: pandas ê°€ê³µ {total_processed}ê°œ, "
            f"supabase ì €ì¥ {total_saved}ê°œ ===\n")
        
        # ë°°ì¹˜ ë°ì´í„° ì´ˆê¸°í™”
        self.batch_data = []
        self.processed_count += batch_count
    
    async def _process_final_batch(self):
        """ë§ˆì§€ë§‰ ë‚¨ì€ ë°°ì¹˜ ë°ì´í„° ì²˜ë¦¬"""
        if self.batch_data:
            log(f"\n=== ìµœì¢… ë°°ì¹˜ ì²˜ë¦¬: {len(self.batch_data)}ê°œ ì†Œë¶„ë¥˜ ===\n")
            await self._process_batch()

    async def _crawl_single_subcategory(self, major_name, middle_name, sub_info):
        """
        [ìµœì¢… ì™„ì„±ë³¸] ë‹¨ì¼ ì†Œë¶„ë¥˜ í˜ì´ì§€ì— ì ‘ì†í•˜ì—¬ 'ë¬¼ê°€ì¶”ì´ ë³´ê¸°' íƒ­ì˜ ëª¨ë“  specification ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ì²˜ë¦¬í•©ë‹ˆë‹¤.
        ë‹¤ì–‘í•œ í…Œì´ë¸” êµ¬ì¡°(ì§€ì—­, ìƒì„¸ê·œê²©, ê°€ê²©ëª…)ë¥¼ ìë™ìœ¼ë¡œ ê°ì§€í•©ë‹ˆë‹¤.
        """
        async with self.semaphore:
            sub_name = sub_info['name']
            sub_href = sub_info['href']
            sub_url = f"{self.base_url}/www/price/{sub_href}"

            log(f"  - [{major_name}>{middle_name}] '{sub_name}' ìˆ˜ì§‘ ì‹œì‘")

            new_page = None
            max_retries = 3
            
            for attempt in range(max_retries):
                try:
                    # 1. ìƒˆë¡œìš´ í˜ì´ì§€ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
                    new_page = await self.context.new_page()
                    
                    # 2. ì†Œë¶„ë¥˜ì˜ ì´ˆê¸° í˜ì´ì§€('ë¬¼ê°€ì •ë³´ ë³´ê¸°' íƒ­)ë¡œ ì´ë™
                    await new_page.goto(sub_url, timeout=60000)
                    await new_page.wait_for_load_state('networkidle', timeout=45000)
                    
                    # 3. 'ë¬¼ê°€ì¶”ì´ ë³´ê¸°' íƒ­ìœ¼ë¡œ ì´ë™
                    trend_view_tab_selector = 'a[href*="detail_change.asp"]'
                    await new_page.wait_for_selector(trend_view_tab_selector, timeout=15000)
                    await new_page.click(trend_view_tab_selector)
                    
                    # 4. 'ë¬¼ê°€ì¶”ì´ ë³´ê¸°' í˜ì´ì§€ê°€ ì™„ì „íˆ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
                    spec_dropdown_selector = 'select#ITEM_SPEC_CD'
                    await new_page.wait_for_selector(spec_dropdown_selector, timeout=30000)
                    log(f"    '{sub_name}': ë¬¼ê°€ì¶”ì´ ë³´ê¸° íƒ­ìœ¼ë¡œ ì´ë™ ì™„ë£Œ.")

                    # 5. ëª¨ë“  Specification ëª©ë¡ (option íƒœê·¸ë“¤) ì •ë³´ ìˆ˜ì§‘
                    options = await new_page.locator(f'{spec_dropdown_selector} option').all()
                    specs_to_crawl = []
                    for o in options:
                        value = await o.get_attribute('value')
                        name = (await o.inner_text()).strip()
                        if value and name:
                            specs_to_crawl.append({'name': name, 'value': value})
                    
                    if not specs_to_crawl:
                        log(f"    '{sub_name}': ìˆ˜ì§‘í•  Specificationì´ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.", "WARNING")
                        await new_page.close()
                        return None # ë°ì´í„°ê°€ ì—†ìœ¼ë¯€ë¡œ Noneì„ ë°˜í™˜
                        
                    log(f"    '{sub_name}': ì´ {len(specs_to_crawl)}ê°œì˜ Specification ë°œê²¬.")

                    # ì´ ì†Œë¶„ë¥˜ì—ì„œ ìµœì¢…ì ìœ¼ë¡œ ìˆ˜ì§‘ë  ëª¨ë“  ë°ì´í„°ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸
                    all_crawled_data = []

                    # 6. ê° Specificationì„ ìˆœíšŒí•˜ë©° ë°ì´í„° ìˆ˜ì§‘
                    for i, spec in enumerate(specs_to_crawl):
                        try:
                            current_spec_name = spec['name']
                            current_spec_value = spec['value']
                            log(f"      - [{i+1}/{len(specs_to_crawl)}] '{current_spec_name[:30]}...' ë°ì´í„° ìˆ˜ì§‘")

                            # a. ë“œë¡­ë‹¤ìš´ì—ì„œ í˜„ì¬ ê·œê²© ì„ íƒ
                            await new_page.select_option(spec_dropdown_selector, value=current_spec_value)

                            # b. ì²« ë²ˆì§¸ ê·œê²© ì¡°íšŒ ì‹œì—ë§Œ ê¸°ê°„ì„ ìµœëŒ€ë¡œ ì„¤ì • (íš¨ìœ¨ì„±)
                            if i == 0:
                                start_year_selector = 'select#DATA_YEAR_F'
                                all_year_options = await new_page.locator(f'{start_year_selector} option').all()
                                if all_year_options:
                                    # option ëª©ë¡ ì¤‘ ë§ˆì§€ë§‰ ìš”ì†Œ(ê°€ì¥ ì˜¤ë˜ëœ ì—°ë„)ë¥¼ ì„ íƒ
                                    oldest_year_value = await all_year_options[-1].get_attribute('value')
                                    await new_page.select_option(start_year_selector, value=oldest_year_value)
                                    await new_page.select_option('select#DATA_MONTH_F', value='01')
                                    log(f"      - ê¸°ê°„ì„ ìµœëŒ€ë¡œ ì„¤ì •: {oldest_year_value}ë…„ 1ì›”ë¶€í„°")
                            
                            # c. ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­ ë° í…Œì´ë¸” ì—…ë°ì´íŠ¸ ëŒ€ê¸°
                            search_button_selector = 'form[name="sForm"] input[type="image"]'
                            async with new_page.expect_response(lambda r: "detail_change.asp" in r.url, timeout=30000):
                                await new_page.click(search_button_selector)
                            
                            await new_page.wait_for_selector('table#priceTrendDataArea tr:nth-child(2)', timeout=15000)

                            # d. ë‹¨ìœ„(unit) ì •ë³´ ê°€ì ¸ì˜¤ê¸° (INCLUSION_LIST ì—ì„œë§Œ)
                            unit = self._get_unit_from_inclusion_list(major_name, middle_name, sub_name, current_spec_name)
                            if not unit:
                                log(f"      - ë‹¨ìœ„ ì •ë³´ë¥¼ INCLUSION_LISTì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŒ (null ì²˜ë¦¬).", "DEBUG")

                            # e. í…Œì´ë¸” êµ¬ì¡° ê°ì§€ ë° ë°ì´í„° íŒŒì‹±
                            price_table_selector = 'table#priceTrendDataArea'
                            headers = [th.strip() for th in await new_page.locator(f'{price_table_selector} th').all_inner_texts()]
                            
                            table_type = "region" # ê¸°ë³¸ê°’
                            if len(headers) > 1:
                                header_sample = headers[1]
                                if 'ê°€ê²©' in header_sample or re.match(r'ê°€[â‘ -â‘©]', header_sample):
                                    table_type = "price_name"
                                elif not self._is_region_header(header_sample):
                                    table_type = "detail_spec"
                            
                            log(f"      - í…Œì´ë¸” íƒ€ì… ê°ì§€: {table_type}")

                            rows = await new_page.locator(f'{price_table_selector} tr').all()
                            for row in rows[1:]:
                                cols_text = await row.locator('td').all_inner_texts()
                                if not cols_text: continue

                                date = cols_text[0].strip()
                                prices_text = [p.strip().replace(',', '') for p in cols_text[1:]]

                                data_headers = headers[1:] # 'êµ¬ë¶„' ì œì™¸
                                for idx, header in enumerate(data_headers):
                                    if idx < len(prices_text) and prices_text[idx].isdigit():
                                        region = "ì „êµ­" if table_type != "region" else header
                                        detail_spec = header if table_type != "region" else None
                                        
                                        all_crawled_data.append(self._create_data_entry(
                                            major_name, middle_name, sub_name, current_spec_name, 
                                            region, detail_spec, date, prices_text[idx], unit
                                        ))

                        except Exception as spec_e:
                            log(f"      - Specification '{spec.get('name', 'N/A')[:30]}...' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {spec_e}", "WARNING")
                            continue

                    # 7. ìˆ˜ì§‘ëœ ë°ì´í„° í›„ì²˜ë¦¬ ë° ì €ì¥
                    if all_crawled_data:
                        # processed_dataëŠ” ë°”ë¡œ all_crawled_dataê°€ ë©ë‹ˆë‹¤.
                        saved_count = await self.processor.save_to_supabase(all_crawled_data, 'kpi_price_data', check_duplicates=True)
                        log(f"  âœ… '{sub_name}' ì™„ë£Œ: {len(all_crawled_data)}ê°œ ë°ì´í„° ìˆ˜ì§‘ â†’ Supabase ì €ì¥ {saved_count}ê°œ ì„±ê³µ")
                        await self.clear_redis_cache(major_name, middle_name, sub_name)
                    else:
                        log(f"  âš ï¸ '{sub_name}' ì™„ë£Œ: ìµœì¢… ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    
                    await new_page.close()
                    # ì„±ê³µí–ˆìœ¼ë¯€ë¡œ ì¬ì‹œë„ ë£¨í”„ íƒˆì¶œ
                    return all_crawled_data # result ëŒ€ì‹  ì‹¤ì œ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜

                except Exception as e:
                    if new_page: await new_page.close()
                    
                    if attempt == max_retries - 1:
                        log(f"  âŒ ì†Œë¶„ë¥˜ '{sub_name}' ì²˜ë¦¬ ìµœì¢… ì‹¤íŒ¨: {str(e)}", "ERROR")
                        return None # ì‹¤íŒ¨ ë¦¬í„´
                    else:
                        log(f"  âš ï¸ ì†Œë¶„ë¥˜ '{sub_name}' ì²˜ë¦¬ ì¬ì‹œë„ {attempt + 1}/{max_retries}: {str(e)}", "WARNING")
                        await asyncio.sleep(5)

    def _is_region_header(self, header_text):
        """í—¤ë”ê°€ ì¼ë°˜ì ì¸ ì§€ì—­ëª…ì¸ì§€ íŒë³„í•˜ëŠ” ê°„ë‹¨í•œ í•¨ìˆ˜"""
        known_regions = ["ì„œìš¸", "ë¶€ì‚°", "ëŒ€êµ¬", "ì¸ì²œ", "ê´‘ì£¼", "ëŒ€ì „", "ìš¸ì‚°", "ì„¸ì¢…", "ê²½ê¸°", "ê°•ì›", "ì¶©ë¶", "ì¶©ë‚¨", "ì „ë¶", "ì „ë‚¨", "ê²½ë¶", "ê²½ë‚¨", "ì œì£¼", "ìˆ˜ì›"]
        for region in known_regions:
            if region in header_text:
                return True
        return False

    def _create_data_entry(self, major, middle, sub, spec, region, detail_spec, date, price, unit):
        """ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        # 'ìƒì„¸ê·œê²©'ì´ë‚˜ 'ê°€ê²©ëª…'ì´ ìˆëŠ” ê²½ìš°, ê¸°ì¡´ ê·œê²©ëª…ì— ë§ë¶™ì—¬ ìµœì¢… ê·œê²©ëª…ì„ ë§Œë“­ë‹ˆë‹¤.
        final_spec = f"{spec} - {detail_spec}" if detail_spec else spec
        
        return {
            'major_category': major,
            'middle_category': middle,
            'sub_category': sub,
            'specification': final_spec,
            'region': region,
            'date': f"{date.replace('.', '-')}-01", # ë‚ ì§œ í˜•ì‹ì„ 'YYYY-MM-DD'ë¡œ í‘œì¤€í™”
            'price': int(price),
            'unit': unit
        }

    def _get_unit_from_inclusion_list(self, major_name, middle_name, sub_name, spec_name=None):
        """INCLUSION_LISTì—ì„œ ë‹¨ìœ„ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            if major_name in INCLUSION_LIST:
                major_data = INCLUSION_LIST[major_name]
                if middle_name in major_data:
                    middle_data = major_data[middle_name]
                    if isinstance(middle_data, dict) and sub_name in middle_data:
                        unit_data = middle_data[sub_name]
                        if isinstance(unit_data, str): return unit_data
                        elif isinstance(unit_data, dict):
                            if spec_name and 'specifications' in unit_data:
                                specifications = unit_data['specifications']
                                if spec_name in specifications: return specifications[spec_name]
                                for spec_key, spec_unit in specifications.items():
                                    if spec_name in spec_key or spec_key in spec_name: return spec_unit
                            if 'default' in unit_data: return unit_data['default']
            return None
        except Exception as e:
            log(f"í•˜ë“œì½”ë”©ëœ ë‹¨ìœ„ ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}", "ERROR")
            return None

    def _load_unit_from_file_cache(self, cate_cd, item_cd):
        """íŒŒì¼ ìºì‹œì—ì„œ ë‹¨ìœ„ ì •ë³´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            import json
            import os
            
            cache_file = os.path.join(os.path.dirname(__file__), 'cache', 'unit_cache.json')
            
            if not os.path.exists(cache_file):
                return None
                
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            cache_key = f"{cate_cd}_{item_cd}"
            unit = cache_data.get(cache_key)
            
            if unit:
                log(f"íŒŒì¼ ìºì‹œì—ì„œ ë‹¨ìœ„ ì •ë³´ ë¡œë“œ: {cache_key} -> {unit}")
                return unit
                
        except Exception as e:
            log(f"íŒŒì¼ ìºì‹œ ë¡œë“œ ì˜¤ë¥˜: {str(e)}", "ERROR")
            
        return None

    async def _process_specs_parallel(self, spec_list, raw_item_data,
                                     major_name, middle_name, sub_name,
                                     sub_url):
        """ì—¬ëŸ¬ ê·œê²©ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ëŠ” ë©”ì„œë“œ"""
        semaphore = asyncio.Semaphore(2)  # ìµœëŒ€ 2ê°œ ë™ì‹œ ì²˜ë¦¬

        async def process_spec_with_new_page(spec):
            async with semaphore:
                try:
                    # ìƒˆë¡œìš´ í˜ì´ì§€ ìƒì„±
                    new_page = await self.context.new_page()
                    base_url = "https://www.kpi.or.kr/www/price/"
                    await new_page.goto(f"{base_url}{sub_url}")
                    await new_page.wait_for_load_state(
                        'networkidle', timeout=60000)
                    await new_page.wait_for_selector(
                        'body', timeout=5000)

                    # 'ë¬¼ê°€ì¶”ì´ ë³´ê¸°' íƒ­ìœ¼ë¡œ ì´ë™ (ì¬ì‹œë„ ë¡œì§)
                    for retry in range(3):
                        try:
                            # íƒ­ì´ ì¡´ì¬í•˜ëŠ”ì§€ ë¨¼ì € í™•ì¸
                            await new_page.wait_for_selector('a:has-text("ë¬¼ê°€ì¶”ì´ ë³´ê¸°")', timeout=15000)
                            
                            link_name = 'ë¬¼ê°€ì¶”ì´ ë³´ê¸°'
                            link_locator = new_page.get_by_role(
                                'link', name=link_name)
                            await link_locator.click(timeout=30000)
                            await new_page.wait_for_selector(
                                selector="#ITEM_SPEC_CD",
                                timeout=30000
                            )
                            break
                        except Exception as e:
                            if retry == 2:
                                raise e
                            log(f"ë¬¼ê°€ì¶”ì´ ë³´ê¸° íƒ­ í´ë¦­ ì¬ì‹œë„ "
                                f"{retry + 1}/3: {e}", "WARNING")
                            await new_page.reload()
                            await new_page.wait_for_load_state(
                                'domcontentloaded', timeout=10000)
                            await new_page.wait_for_load_state(
                                'networkidle', timeout=30000)

                    # ì„ì‹œ ë°ì´í„° êµ¬ì¡°
                    temp_data = {
                        'major_category_name': major_name,
                        'middle_category_name': middle_name,
                        'sub_category_name': sub_name,
                        'spec_data': []
                    }

                    # ê·œê²© ì²˜ë¦¬
                    await self._process_single_spec(new_page, spec, temp_data)

                    await new_page.close()
                    return temp_data['spec_data']

                except Exception as e:
                    error_msg = (f"    - ë³‘ë ¬ ì²˜ë¦¬ ì¤‘ ê·œê²© '{spec['name']}' ì˜¤ë¥˜ "
                                f"[ëŒ€ë¶„ë¥˜: {major_name}, ì¤‘ë¶„ë¥˜: {middle_name}, "
                                f"ì†Œë¶„ë¥˜: {sub_name}]: {str(e)}")
                    log(error_msg)
                    return []

        # ëª¨ë“  ê·œê²©ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬
        # ëª¨ë“  ê·œê²©ì— ëŒ€í•œ ë³‘ë ¬ ì²˜ë¦¬ íƒœìŠ¤í¬ ìƒì„±
        tasks = [process_spec_with_new_page(spec)
                 for spec in spec_list]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # ê²°ê³¼ ë³‘í•©
        for result in results:
            if isinstance(result, list) and result:
                raw_item_data['spec_data'].extend(result)

    async def _process_single_spec(self, page, spec, raw_item_data):
        """ë‹¨ì¼ ê·œê²©ì— ëŒ€í•œ ë°ì´í„° ì²˜ë¦¬"""
        spec_value = spec['value']
        spec_name = spec['name']
        log(f"    - ê·œê²©: '{spec_name}' ì¡°íšŒ ì¤‘...")

        max_retries = 3
        for attempt in range(max_retries):
            try:
                # ê·œê²© ì„ íƒ
                # ê·œê²© ì„ íƒ
                spec_locator = page.locator('#ITEM_SPEC_CD')
                await spec_locator.select_option(value=spec_value)
                await page.wait_for_load_state(
                    'networkidle', timeout=10000)

                # ê¸°ê°„ ì„ íƒ
                year_locator = page.locator('#DATA_YEAR_F')
                await year_locator.select_option(value=self.start_year)
                month_locator = page.locator('#DATA_MONTH_F')
                await month_locator.select_option(
                    value=self.start_month)
                await page.wait_for_load_state(
                    'networkidle', timeout=10000)

                # ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­ (ì¬ì‹œë„ ë¡œì§ ì¶”ê°€)
                search_selector = 'form[name="sForm"] input[type="image"]'
                search_button = page.locator(search_selector)
                
                # ì¬ì‹œë„ ë¡œì§ (ìµœëŒ€ 3íšŒ ì‹œë„)
                for search_attempt in range(3):
                    try:
                        await search_button.click(timeout=15000)
                        await page.wait_for_load_state(
                            'networkidle', timeout=20000)
                        break
                    except Exception as search_e:
                        if search_attempt == 2:  # ë§ˆì§€ë§‰ ì‹œë„
                            raise search_e
                        log(f"        - ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨ (ì‹œë„ {search_attempt + 1}/3), 2ì´ˆ í›„ ì¬ì‹œë„...")
                        await asyncio.sleep(2)
                break

            except Exception as e:
                if attempt < max_retries - 1:
                    attempt_num = attempt + 1
                    log(f"    - ê·œê²© '{spec_name}' ì‹œë„ {attempt_num} "
                        f"ì‹¤íŒ¨, ì¬ì‹œë„ ì¤‘...")
                    await page.wait_for_timeout(1000)
                    continue
                else:
                    log(f"    - ê·œê²© '{spec_name}' ìµœì¢… ì‹¤íŒ¨: "
                        f"{str(e)}")
                    return

        # ë°ì´í„° í…Œì´ë¸” íŒŒì‹±
        try:
            # ì¬ì‹œë„ ë¡œì§ìœ¼ë¡œ í…Œì´ë¸” ëŒ€ê¸° (ìµœëŒ€ 3íšŒ ì‹œë„)
            for table_attempt in range(3):
                try:
                    await page.wait_for_selector(
                        "#priceTrendDataArea tr", timeout=15000)
                    break
                except Exception as table_e:
                    if table_attempt == 2:  # ë§ˆì§€ë§‰ ì‹œë„
                        raise table_e
                    log(f"        - í…Œì´ë¸” ë¡œë”© ì‹¤íŒ¨ (ì‹œë„ {table_attempt + 1}/3), 2ì´ˆ í›„ ì¬ì‹œë„...")
                    await asyncio.sleep(2)

            # ì „ì²´ í…Œì´ë¸” HTMLì„ ë¡œê·¸ë¡œ ì¶œë ¥í•´ì„œ êµ¬ì¡° í™•ì¸
            table_html = await page.locator("#priceTrendDataArea").inner_html()
            log(f"    - ê·œê²© '{spec_name}': í…Œì´ë¸” HTML êµ¬ì¡°:\n{table_html[:500]}...")

            header_elements = await page.locator(
            "#priceTrendDataArea th").all()
            if not header_elements:
                log(f"    - ê·œê²© '{spec_name}': ë°ì´í„° í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return

            # ì²« ë²ˆì§¸ í—¤ë”ëŠ” 'êµ¬ë¶„'ì´ë¯€ë¡œ ì œì™¸
            dates = [await h.inner_text() for h in header_elements[1:]]
            log(f"    - ê·œê²© '{spec_name}': í—¤ë” ë°ì´í„° = {dates}")

            # ì²« ë²ˆì§¸ ì§€ì—­ ë°ì´í„° í–‰ë§Œ ì¶”ì¶œ (ì˜ˆ: 'ì„œâ‘ ìš¸')
            data_rows = await page.locator("#priceTrendDataArea tr").all()
            log(f"    - ê·œê²© '{spec_name}': ì´ {len(data_rows)}ê°œ í–‰ ë°œê²¬")
            if len(data_rows) < 2:
                log(f"    - ê·œê²© '{spec_name}': ë°ì´í„° í–‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return

            first_row_tds = await data_rows[1].locator("td").all()
            if not first_row_tds:
                log(f"    - ê·œê²© '{spec_name}': ë°ì´í„° ì…€ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return

            region = await first_row_tds[0].inner_text()
            prices = [await td.inner_text() for td in first_row_tds[1:]]
            log(f"    - ê·œê²© '{spec_name}': ì§€ì—­ = {region}, "
                f"ê°€ê²© ë°ì´í„° = {prices}")

            # í—¤ë” ì •ë³´ ì¶”ì¶œ (í…Œì´ë¸” êµ¬ì¡° íŒŒì•…ìš©)
            header_cells = await data_rows[0].locator("th").all()
            headers = []
            for i in range(1, len(header_cells)):  # ì²« ë²ˆì§¸ ì»¬ëŸ¼(êµ¬ë¶„) ì œì™¸
                header_text = await header_cells[i].inner_text()
                # ë™ê·¸ë¼ë¯¸ ìˆ«ìë¥¼ ì¼ë°˜ ìˆ«ìë¡œ ë³€í™˜ (â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘© ë“± -> 1,2,3)
                circle_to_num = {
                    'â‘ ': '1', 'â‘¡': '2', 'â‘¢': '3', 'â‘£': '4', 'â‘¤': '5',
                    'â‘¥': '6', 'â‘¦': '7', 'â‘§': '8', 'â‘¨': '9', 'â‘©': '10'
                }
                cleaned_header = header_text.strip()
                for circle, num in circle_to_num.items():
                    cleaned_header = cleaned_header.replace(circle, num)

                # ë‚ ì§œ í˜•ì‹ ê²€ì¦
                # (YYYY.M ë˜ëŠ” YYYY. M í˜•íƒœë§Œ í—ˆìš©)
                if self._is_valid_date_header(cleaned_header):
                    headers.append(cleaned_header)
                else:
                    log(f"    - ì˜ëª»ëœ ë‚ ì§œ í—¤ë” ì œì™¸: "
                        f"'{cleaned_header}' (ì›ë³¸: '{header_text}')")

            log(f"    - ê·œê²© '{spec_name}': "
                f"í—¤ë” ë°ì´í„° = {headers}")

            price_list = []
            # í…Œì´ë¸” êµ¬ì¡° ë¶„ì„: í—¤ë”ê°€ ë‚ ì§œì´ê³  í–‰ì´ ì§€ì—­ë³„ ë°ì´í„°ì¸ êµ¬ì¡°
            # ê° ì§€ì—­(í–‰)ì— ëŒ€í•´ ì²˜ë¦¬
            # í—¤ë” ì œì™¸í•˜ê³  ëª¨ë“  í–‰ ì²˜ë¦¬
            for i in range(1, len(data_rows)):
                try:
                    row_tds = await data_rows[i].locator("td").all()
                    if len(row_tds) >= 2:
                        # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì€ ì§€ì—­ëª…
                        region_str = await row_tds[0].inner_text()
                        # ë™ê·¸ë¼ë¯¸ ìˆ«ìë¥¼ ì¼ë°˜ ìˆ«ìë¡œ ë³€í™˜ (â‘ â‘¡â‘¢ ë“± -> 1,2,3)
                        circle_to_num = {
                            'â‘ ': '1', 'â‘¡': '2', 'â‘¢': '3', 'â‘£': '4',
                            'â‘¤': '5', 'â‘¥': '6', 'â‘¦': '7', 'â‘§': '8',
                            'â‘¨': '9', 'â‘©': '10'
                        }
                        clean_region = region_str.strip()
                        for circle, num in circle_to_num.items():
                            clean_region = clean_region.replace(
                                circle, num)

                        # ê° ë‚ ì§œë³„ ê°€ê²© ì²˜ë¦¬
                        for j, date_header in enumerate(headers):
                            if j + 1 < len(row_tds):
                                price_str = await row_tds[j + 1].inner_text()
                                # ê°€ê²© ë°ì´í„° ìœ íš¨ì„± ê²€ì¦ ê°•í™”
                                price_stripped = price_str.strip()
                                if price_stripped and price_stripped != '-':
                                    # ì‰¼í‘œê°€ í¬í•¨ëœ ìˆ«ìì¸ì§€ í™•ì¸
                                    clean_price = price_stripped.replace(
                                        ',', '')
                                    is_valid_price = (
                                        clean_price.isdigit() and
                                        int(clean_price) > 0)
                                    if is_valid_price:
                                        # ë‚ ì§œ íŒŒì‹± (YYYY. M í˜•íƒœ)
                                        try:
                                            is_valid_header = (
                                                self._is_valid_date_header(
                                                    date_header))
                                            if is_valid_header:
                                                year_month = (
                                                    date_header.strip()
                                                    .replace(' ', ''))
                                                if '.' in year_month:
                                                    year, month = year_month.split('.')
                                                    date_obj = datetime(
                                                        int(year),
                                                        int(month), 1)
                                                    price_data = {
                                                        'date': date_obj,
                                                        'region': clean_region,
                                                        'price': price_str.strip()
                                                    }
                                                    price_list.append(price_data)
                                                    # ìœ íš¨í•œ ê°€ê²© ë°ì´í„° ì¶”ê°€ ë¡œê·¸
                                                    log(
                                                        f"    - ìœ íš¨í•œ ê°€ê²© ë°ì´í„° ì¶”ê°€: "
                                                        f"{clean_region} "
                                                        f"({date_header}) = "
                                                        f"{price_str.strip()}")
                                            else:
                                                # ì˜ëª»ëœ ë‚ ì§œ í˜•ì‹ ì œì™¸ ë¡œê·¸
                                                log(
                                                    f"    - ì˜ëª»ëœ ë‚ ì§œ í˜•ì‹ ì œì™¸: "
                                                    f"{date_header}")
                                        except Exception as date_parse_error:
                                            # ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜ ë¡œê·¸
                                            log(
                                                f"    - ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: "
                                                f"{date_header} - "
                                                f"{str(date_parse_error)}")
                except Exception as row_error:
                    # í–‰ íŒŒì‹± ì˜¤ë¥˜ ë¡œê·¸
                    log(f"    - í–‰ {i} íŒŒì‹± ì˜¤ë¥˜: {str(row_error)}")
                    continue

            if price_list:
                raw_item_data['spec_data'].append({
                    'specification_name': spec_name,
                    'prices': price_list
                })
                log(f"    - ê·œê²© '{spec_name}': {len(price_list)}ê°œ ê°€ê²© ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
            else:
                log(f"    - ê·œê²© '{spec_name}': ìœ íš¨í•œ ê°€ê²© ë°ì´í„° ì—†ìŒ")

        except Exception as e:
            log(f"    - ê·œê²© '{spec_name}' ë°ì´í„° íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
            return


# --- 4. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ---
# <<< íŒŒì¼ ë§¨ ì•„ë˜ ë¶€ë¶„ì„ ì´ ì½”ë“œë¡œ ì „ì²´ êµì²´ (5/5) >>>

async def main():
    """ë©”ì¸ ì‹¤í–‰ ë¡œì§: ëª…ë ¹í–‰ ì¸ì íŒŒì‹± ë° í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
    # ëª…ë ¹í–‰ ì¸ì íŒŒì‹± - ë‘ ê°€ì§€ ë°©ì‹ ì§€ì›
    # ë°©ì‹ 1: --major="ê³µí†µìì¬" --middle="ë¹„ì² ê¸ˆì†" --sub="ì•Œë£¨ë¯¸ëŠ„" --mode="sub_only"
    # ë°©ì‹ 2: --major ê³µí†µìì¬ --middle ë¹„ì² ê¸ˆì† --sub ì•Œë£¨ë¯¸ëŠ„ --mode sub_only
    
    args = {}
    i = 1
    while i < len(sys.argv):
        arg = sys.argv[i]
        if arg.startswith('--'):
            key = arg.strip('-')
            if '=' in arg:
                # ë°©ì‹ 1: --key=value
                key, value = arg.split('=', 1)
                key = key.strip('-')
                value = value.strip('"\'')
                args[key] = value
            else:
                # ë°©ì‹ 2: --key value
                if i + 1 < len(sys.argv) and not sys.argv[i + 1].startswith('--'):
                    value = sys.argv[i + 1].strip('"\'')
                    args[key] = value
                    i += 1
                else:
                    args[key] = True
        i += 1
    
    target_major = args.get('major')
    target_middle = args.get('middle')
    target_sub = args.get('sub')
    crawl_mode = args.get('mode', 'all')
    start_year = args.get('start-year', '2020')
    start_month = args.get('start-month', '01').zfill(2)

    log(f"í¬ë¡¤ë§ ì„¤ì •:")
    log(f"  - ëª¨ë“œ: {crawl_mode}")
    log(f"  - ëŒ€ë¶„ë¥˜: {target_major}")
    log(f"  - ì¤‘ë¶„ë¥˜: {target_middle}")
    log(f"  - ì†Œë¶„ë¥˜: {target_sub}")
    log(f"  - ì‹œì‘ ì‹œì : {start_year}-{start_month}")

    # í¬ë¡¤ë§ ëª¨ë“œì— ë”°ë¥¸ ì‹¤í–‰
    if crawl_mode == "all" and not target_major:
        # ì „ì²´ ëŒ€ë¶„ë¥˜ í¬ë¡¤ë§ (ê¸°ì¡´ ë°©ì‹)
        log("ì „ì²´ ëŒ€ë¶„ë¥˜ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.", "INFO")
        all_major_categories = list(INCLUSION_LIST.keys())
        log(f"í¬ë¡¤ë§í•  ëŒ€ë¶„ë¥˜: {all_major_categories}", "INFO")
        
        for major in all_major_categories:
            log(f"=== {major} í¬ë¡¤ë§ ì‹œì‘ ===", "SUMMARY")
            crawler = KpiCrawler(target_major=major, crawl_mode="all", 
                               start_year=start_year, start_month=start_month)
            await crawler.run()
            log(f"ğŸŸ¢ {major} í¬ë¡¤ë§ ì™„ë£Œ", "SUCCESS")
        
        log("ğŸŸ¢ ì „ì²´ ëŒ€ë¶„ë¥˜ í¬ë¡¤ë§ ì™„ë£Œ", "SUCCESS")
    else:
        # ì„ íƒì  í¬ë¡¤ë§
        log(f"=== {crawl_mode} ëª¨ë“œ í¬ë¡¤ë§ ì‹œì‘ ===", "SUMMARY")
        crawler = KpiCrawler(
            target_major=target_major,
            target_middle=target_middle,
            target_sub=target_sub,
            crawl_mode=crawl_mode,
            start_year=start_year,
            start_month=start_month
        )
        await crawler.run()
        log(f"ğŸŸ¢ {crawl_mode} ëª¨ë“œ í¬ë¡¤ë§ ì™„ë£Œ", "SUCCESS")


async def test_unit_extraction():
    """ë‹¨ìœ„ ì¶”ì¶œ ë¡œì§ í…ŒìŠ¤íŠ¸"""
    log("=== ë‹¨ìœ„ ì¶”ì¶œ ë¡œì§ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===", "SUMMARY")
    
    # í…ŒìŠ¤íŠ¸í•  ë¹„ì² ê¸ˆì† ì†Œë¶„ë¥˜ ëª©ë¡
    test_categories = [
        ("ê³µí†µìì¬", "ë¹„ì² ê¸ˆì†", "ë™ì œí’ˆ(1)"),
        ("ê³µí†µìì¬", "ë¹„ì² ê¸ˆì†", "ë™ì œí’ˆ(2)"),
        ("ê³µí†µìì¬", "ë¹„ì² ê¸ˆì†", "ì•Œë£¨ë¯¸ëŠ„ì œí’ˆ(1)"),
        ("ê³µí†µìì¬", "ë¹„ì² ê¸ˆì†", "ì•Œë£¨ë¯¸ëŠ„ì œí’ˆ(2)"),
        ("ê³µí†µìì¬", "ë¹„ì² ê¸ˆì†", "ë¹„ì² ì§€ê¸ˆ(ééµåœ°é‡‘)"),
        ("ê³µí†µìì¬", "ë¹„ì² ê¸ˆì†", "ì—°(ë‚©)ì œí’ˆ(é‰›è£½å“)")
    ]
    
    crawler = KpiCrawler(target_major="ê³µí†µìì¬", crawl_mode="test")
    
    browser = None
    try:
        # ë¸Œë¼ìš°ì € ì‹œì‘ (run ë©”ì„œë“œì™€ ë™ì¼í•œ ë°©ì‹)
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=True,
                args=[
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-gpu',
                    '--disable-web-security',
                    '--disable-features=VizDisplayCompositor',
                    '--window-size=1920,1080'
                ]
            )
            crawler.context = await browser.new_context()
            crawler.page = await crawler.context.new_page()
            
            await crawler._login()
            
            for major, middle, sub in test_categories:
                log(f"\n--- {sub} ë‹¨ìœ„ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ---", "INFO")
                
                try:
                    # ì†Œë¶„ë¥˜ ì •ë³´ ìƒì„± (ì‹¤ì œ í¬ë¡¤ë§ì—ì„œ ì‚¬ìš©í•˜ëŠ” í˜•íƒœì™€ ë™ì¼)
                    sub_info = {'name': sub, 'href': f'price/price_list.asp?major={major}&middle={middle}&sub={sub}'}
                    sub_url = f"{crawler.base_url}/www/price/{sub_info['href']}"
                    
                    # í˜ì´ì§€ë¡œ ì´ë™
                    await crawler.page.goto(sub_url, timeout=60000)
                    await crawler.page.wait_for_load_state('networkidle', timeout=45000)
                    
                    # ë¬¼ê°€ì¶”ì´ í˜ì´ì§€ì—ì„œ ë‹¨ìœ„ ì¶”ì¶œ (ì˜¬ë°”ë¥¸ ë§¤ê°œë³€ìˆ˜ ê°œìˆ˜)
                    unit_from_trend = await crawler._extract_unit_from_price_trend_page(
                        crawler.page, sub
                    )
                    log(f"  ë¬¼ê°€ì¶”ì´ í˜ì´ì§€ ë‹¨ìœ„: {unit_from_trend}")
                    
                    # ë¬¼ê°€ì •ë³´ ë³´ê¸° íƒ­ í´ë¦­ (ë” ì•ˆì •ì ì¸ ì„ íƒì ì‚¬ìš©)
                    try:
                        # ì—¬ëŸ¬ ê°€ëŠ¥í•œ ì„ íƒìë¡œ ì‹œë„
                        tab_selectors = [
                            'a[href="#tab2"]',
                            'a[onclick*="tab2"]',
                            'a:has-text("ë¬¼ê°€ì •ë³´ ë³´ê¸°")',
                            'a:has-text("ë¬¼ê°€ì •ë³´")',
                            'li:nth-child(2) a',
                            '.tab-menu li:nth-child(2) a'
                        ]
                        
                        tab_clicked = False
                        for selector in tab_selectors:
                            try:
                                tab_element = await crawler.page.query_selector(selector)
                                if tab_element:
                                    await tab_element.click()
                                    await crawler.page.wait_for_timeout(2000)
                                    tab_clicked = True
                                    log(f"  íƒ­ í´ë¦­ ì„±ê³µ: {selector}")
                                    break
                            except Exception as tab_error:
                                log(f"  íƒ­ ì„ íƒì {selector} ì‹¤íŒ¨: {str(tab_error)}", "WARNING")
                                continue
                        
                        if not tab_clicked:
                            log("  âš ï¸ ë¬¼ê°€ì •ë³´ ë³´ê¸° íƒ­ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ. í˜„ì¬ í˜ì´ì§€ì—ì„œ ë‹¨ìœ„ ì¶”ì¶œ ì‹œë„", "WARNING")
                            
                    except Exception as tab_error:
                        log(f"  âš ï¸ íƒ­ í´ë¦­ ì‹¤íŒ¨: {str(tab_error)}", "WARNING")
                    
                    # ë¬¼ê°€ì •ë³´ ë³´ê¸° í˜ì´ì§€ì—ì„œ ë‹¨ìœ„ ì¶”ì¶œ (ì˜¬ë°”ë¥¸ ë§¤ê°œë³€ìˆ˜ ê°œìˆ˜)
                    unit_from_detail = await crawler._get_unit_from_detail_page(
                        crawler.page, sub
                    )
                    log(f"  ë¬¼ê°€ì •ë³´ ë³´ê¸° í˜ì´ì§€ ë‹¨ìœ„: {unit_from_detail}")
                    
                    # ìºì‹œì—ì„œ ë‹¨ìœ„ í™•ì¸ (redis_client ì´ˆê¸°í™” í™•ì¸)
                    cached_unit = None
                    try:
                        if hasattr(crawler, 'redis_client') and crawler.redis_client:
                            cache_key = f"unit_{major}_{middle}_{sub}"
                            cached_unit = await crawler.redis_client.get(cache_key)
                        else:
                            log("  Redis í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ", "WARNING")
                    except Exception as cache_error:
                        log(f"  ìºì‹œ í™•ì¸ ì‹¤íŒ¨: {str(cache_error)}", "WARNING")
                    
                    log(f"  ìºì‹œëœ ë‹¨ìœ„: {cached_unit}")
                    
                    # ê²°ê³¼ ë¹„êµ
                    if unit_from_trend and unit_from_detail:
                        if unit_from_trend == unit_from_detail:
                            log(f"  âœ… ë‹¨ìœ„ ì¼ì¹˜: {unit_from_trend}")
                        else:
                            log(f"  âš ï¸ ë‹¨ìœ„ ë¶ˆì¼ì¹˜ - ì¶”ì´: {unit_from_trend}, ìƒì„¸: {unit_from_detail}")
                    else:
                        log(f"  âŒ ë‹¨ìœ„ ì¶”ì¶œ ì‹¤íŒ¨ - ì¶”ì´: {unit_from_trend}, ìƒì„¸: {unit_from_detail}")
                        
                except Exception as e:
                    import traceback
                    log(f"  âŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "ERROR")
                    log(f"  ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}", "ERROR")
            
            log("\n=== ë‹¨ìœ„ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===", "SUMMARY")
            await browser.close()
        
    except Exception as e:
        log(f"âŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", "ERROR")
        import traceback
        log(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}", "ERROR")
        if browser:
            try:
                await browser.close()
            except:
                pass


if __name__ == "__main__":
    # ëª…ë ¹í–‰ ì¸ìˆ˜ í™•ì¸
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        # ë‹¨ìœ„ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        asyncio.run(test_unit_extraction())
    else:
        # ì¼ë°˜ í¬ë¡¤ë§ ì‹¤í–‰
        # running_crawlers = check_running_crawler()
        running_crawlers = []
        if running_crawlers:
            log(f"ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ í¬ë¡¤ëŸ¬ {len(running_crawlers)}ê°œ ë°œê²¬. ê¸°ì¡´ í¬ë¡¤ëŸ¬ ì™„ë£Œ í›„ ì¬ì‹¤í–‰í•˜ì„¸ìš”.", "ERROR")
            sys.exit(1)

        asyncio.run(main())

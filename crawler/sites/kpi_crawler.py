

import os
import asyncio
# json import removed as it is unused
import sys
import re
import psutil
from datetime import datetime
from dotenv import load_dotenv
from playwright.async_api import async_playwright
from upstash_redis import AsyncRedis
from jsonc_parser import parse_jsonc
from data_processor import create_data_processor, log
from supabase import create_client, Client

# ì ˆëŒ€ importë¥¼ ìœ„í•œ ê²½ë¡œ ì„¤ì •
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

# parse_jsonc is already imported at the top; remove duplicate import
# log is already imported at the top; remove duplicate import
# create_client already imported at line 15; remove duplicate

# --- 1. ì´ˆê¸° ì„¤ì • ë° í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ---
load_dotenv("../../.env.local")

# Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
SUPABASE_URL = os.environ.get("NEXT_PUBLIC_SUPABASE_URL")
SUPABASE_KEY = os.environ.get("NEXT_PUBLIC_SUPABASE_ANON_KEY")
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)


# --- 2. ì›¹ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤ ---


def check_running_crawler():
    """ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ í¬ë¡¤ëŸ¬ê°€ ìˆëŠ”ì§€ í™•ì¸"""
    current_pid = os.getpid()
    current_script = os.path.basename(__file__)

    running_crawlers = []
    for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
        try:
            if proc.info['pid'] == current_pid:
                continue

            cmdline = proc.info['cmdline']
            if cmdline and any(current_script in cmd for cmd in cmdline):
                running_crawlers.append({
                    'pid': proc.info['pid'],
                    'cmdline': ' '.join(cmdline)
                })
        except (psutil.NoSuchProcess, psutil.AccessDenied):
            continue

    return running_crawlers

# --- 3. í¬ë¡¤ë§ ëŒ€ìƒ ì¹´í…Œê³ ë¦¬ ë° ë‹¨ìœ„ ì„¤ì • ---
INCLUSION_LIST_PATH = os.path.join(current_dir, "kpi_inclusion_list_compact.jsonc")
with open(INCLUSION_LIST_PATH, "r", encoding="utf-8") as f:
    jsonc_content = f.read()
INCLUSION_LIST = parse_jsonc(jsonc_content)

# --- 4. Playwright ì›¹ í¬ë¡¤ëŸ¬ í´ë˜ ---
class KpiCrawler:
    def __init__(self, target_major: str = None, target_middle: str = None,
                 target_sub: str = None, crawl_mode: str = "all",
                 start_year: str = None, start_month: str = None, max_concurrent=3):
        """
        KPI í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”

        Args:
            target_major: í¬ë¡¤ë§í•  ëŒ€ë¶„ë¥˜ëª… (Noneì´ë©´ ì „ì²´)
            target_middle: í¬ë¡¤ë§í•  ì¤‘ë¶„ë¥˜ëª… (Noneì´ë©´ ì „ì²´)
            target_sub: í¬ë¡¤ë§í•  ì†Œë¶„ë¥˜ëª… (Noneì´ë©´ ì „ì²´)
            crawl_mode: í¬ë¡¤ë§ ëª¨ë“œ
                - "all": ì „ì²´ í¬ë¡¤ë§ (ê¸°ë³¸ê°’)
                - "major_only": ì§€ì •ëœ ëŒ€ë¶„ë¥˜ë§Œ í¬ë¡¤ë§
                - "middle_only": ì§€ì •ëœ ëŒ€ë¶„ë¥˜ì˜ íŠ¹ì • ì¤‘ë¶„ë¥˜ë§Œ í¬ë¡¤ë§
                - "sub_only": ì§€ì •ëœ ëŒ€ë¶„ë¥˜ì˜ íŠ¹ì • ì¤‘ë¶„ë¥˜ì˜ íŠ¹ì • ì†Œë¶„ë¥˜ë§Œ í¬ë¡¤ë§
            start_year: ì‹œì‘ ì—°ë„ (Noneì´ë©´ í˜„ì¬ ì—°ë„)
            start_month: ì‹œì‘ ì›” (Noneì´ë©´ í˜„ì¬ ì›”)
            max_concurrent: ìµœëŒ€ ë™ì‹œ ì‹¤í–‰ ìˆ˜
        """
        self.base_url = "https://www.kpi.or.kr"
        self.max_concurrent = max_concurrent
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.supabase = supabase  # ì „ì—­ supabase ê°ì²´ ì°¸ì¡°

        # ìƒˆë¡œ ì¶”ê°€ëœ ì†ì„±ë“¤
        self.target_major_category = target_major
        self.target_middle_category = target_middle
        self.target_sub_category = target_sub
        self.crawl_mode = crawl_mode
        self.start_year = start_year or str(datetime.now().year)
        self.start_month = start_month or str(datetime.now().month)

        self.processor = create_data_processor('kpi')

        # ë°°ì¹˜ ì²˜ë¦¬ìš© ë³€ìˆ˜
        self.batch_data = []
        self.batch_size = 5  # ì†Œë¶„ë¥˜ 5ê°œë§ˆë‹¤ ì²˜ë¦¬
        self.processed_count = 0

        self.redis = AsyncRedis(
            url=os.environ.get("UPSTASH_REDIS_REST_URL"),
            token=os.environ.get("UPSTASH_REDIS_REST_TOKEN")
        )

        log(f"í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” - í¬ë¡¤ë§ ëª¨ë“œ: {self.crawl_mode}")
        log(f"  íƒ€ê²Ÿ ëŒ€ë¶„ë¥˜: {self.target_major_category}")
        log(f"  íƒ€ê²Ÿ ì¤‘ë¶„ë¥˜: {self.target_middle_category}")
        log(f"  íƒ€ê²Ÿ ì†Œë¶„ë¥˜: {self.target_sub_category}")
        log(f"  ì‹œì‘ë‚ ì§œ: {self.start_year}-{self.start_month}")

    async def clear_redis_cache(self):
        try:
            # ì‹œì¥ì§€í‘œ ìºì‹œ ì‚­ì œ
            await self.redis.delete("market_indicators")
            # ìì¬ ê°€ê²© ìºì‹œ ì‚­ì œ
            keys = await self.redis.keys("material_prices:*")
            if keys:
                await self.redis.delete(*keys)
            log("âœ… Redis ìºì‹œê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.", "SUCCESS")
        except Exception as e:
            log(f"Redis ìºì‹œ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}", "ERROR")

    async def run(self):
        """í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        browser = None
        try:
            async with async_playwright() as p:
                # GitHub Actions í™˜ê²½ì—ì„œ ë” ì•ˆì •ì ì¸ ë¸Œë¼ìš°ì € ì„¤ì •
                browser = await p.chromium.launch(
                    headless=True,
                    args=[
                        '--no-sandbox',
                        '--disable-setuid-sandbox',
                        '--disable-dev-shm-usage',
                        '--disable-gpu',
                        '--disable-web-security',
                        '--disable-features=VizDisplayCompositor',
                        '--window-size=1920,1080'
                    ]
                )
                self.context = await browser.new_context()
                self.page = await self.context.new_page()

                await self._login()
                await self._navigate_to_category()
                await self._crawl_categories()

                # ë§ˆì§€ë§‰ ë‚¨ì€ ë°°ì¹˜ ë°ì´í„° ì²˜ë¦¬
                await self._process_final_batch()

                await self.clear_redis_cache()  # ìºì‹œ ì´ˆê¸°í™” ì¶”ê°€

                log(f"\nğŸŸ¢ === í¬ë¡¤ë§ ì™„ë£Œ: ì´ {self.processed_count}ê°œ ì†Œë¶„ë¥˜ ì²˜ë¦¬ë¨ === ğŸŸ¢\n")

                await browser.close()
                return self.processor
        except Exception as e:
            log(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "ERROR")
            if browser:
                try:
                    await browser.close()
                except:
                    pass
            raise

    async def _login(self):
        """ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ì´ë™í•˜ì—¬ ë¡œê·¸ì¸ ìˆ˜í–‰ (íƒ€ì„ì•„ì›ƒ ë° ì¬ì‹œë„ ë¡œì§ ì¶”ê°€)"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                # íƒ€ì„ì•„ì›ƒì„ 60ì´ˆë¡œ ëŠ˜ë¦¬ê³ , ë„¤íŠ¸ì›Œí¬ê°€ ì•ˆì •ë  ë•Œê¹Œì§€ ëŒ€ê¸°
                await self.page.goto(
                    f"{self.base_url}/www/member/login.asp",
                    timeout=60000,
                    wait_until="networkidle"
                )
                log("ë¡œê·¸ì¸ í˜ì´ì§€ ì´ë™ ì„±ê³µ")
                break  # ì„±ê³µ ì‹œ ì¬ì‹œë„ ì¤‘ë‹¨
            except Exception as e:
                log(f"ë¡œê·¸ì¸ í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_retries}): {e}", "WARNING")
                if attempt < max_retries - 1:
                    await asyncio.sleep(5 * (attempt + 1))  # 5ì´ˆ, 10ì´ˆ ê°„ê²©ìœ¼ë¡œ ëŒ€ê¸° í›„ ì¬ì‹œë„
                else:
                    log("ë¡œê·¸ì¸ í˜ì´ì§€ ì´ë™ì— ìµœì¢… ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.", "ERROR")
                    raise  # ìµœì¢… ì‹¤íŒ¨ ì‹œ ì˜¤ë¥˜ ë°œìƒ

        username = os.environ.get("KPI_USERNAME")
        password = os.environ.get("KPI_PASSWORD")

        if not username or not password:
            raise ValueError(".env.local íŒŒì¼ì— KPI_USERNAMEê³¼ "
                             "KPI_PASSWORDë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.")

        # GitHub Actions í™˜ê²½ì—ì„œ ë” ì•ˆì •ì ì¸ ë¡œê·¸ì¸ ì²˜ë¦¬
        await self.page.wait_for_load_state('networkidle', timeout=45000)
        await asyncio.sleep(2)

        await self.page.locator("#user_id").fill(username)
        await asyncio.sleep(1)
        await self.page.locator("#user_pw").fill(password)
        await asyncio.sleep(1)
        await self.page.locator("#sendLogin").click()

        # ë¡œê·¸ì¸ ì™„ë£Œ ëŒ€ê¸°ì‹œê°„ ì¦ê°€
        await self.page.wait_for_load_state('networkidle', timeout=45000)
        await asyncio.sleep(3)
        log("ë¡œê·¸ì¸ ì™„ë£Œ", "SUCCESS")

    async def _navigate_to_category(self):
        """ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ë¡œ ì´ë™ ë° ì´ˆê¸° ì„¤ì • (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
        log("ì¢…í•©ë¬¼ê°€ì •ë³´ ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ë¡œ ì´ë™í•©ë‹ˆë‹¤.")

        max_retries = 3
        retry_count = 0

        while retry_count < max_retries:
            try:
                # íƒ€ì„ì•„ì›ƒì„ 60ì´ˆë¡œ ì¦ê°€
                await self.page.goto(
                    f"{self.base_url}/www/price/category.asp",
                    timeout=60000,  # 60ì´ˆ
                    wait_until="domcontentloaded"  # ë” ë¹ ë¥¸ ë¡œë”© ì™„ë£Œ ì¡°ê±´
                )

                # í˜ì´ì§€ ë¡œë”© ì™„ë£Œ ëŒ€ê¸°
                await self.page.wait_for_load_state('networkidle', timeout=60000)

                # íŒì—… ë‹«ê¸° (ìš°ì„  ì²˜ë¦¬)
                await self._close_popups()

                # Right Quick ë©”ë‰´ ìˆ¨ê¸°ê¸°
                try:
                    close_button = self.page.locator("#right_quick .q_cl")
                    if await close_button.is_visible():
                        await close_button.click()
                        log("Right Quick ë©”ë‰´ë¥¼ ìˆ¨ê²¼ìŠµë‹ˆë‹¤.")
                except Exception as e:
                    log(f"Right Quick ë©”ë‰´ ìˆ¨ê¸°ê¸° ì‹¤íŒ¨ "
                        f"(ì´ë¯¸ ìˆ¨ê²¨ì ¸ ìˆì„ ìˆ˜ ìˆìŒ): {e}")

                log("ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ ì´ë™ ì™„ë£Œ", "SUCCESS")
                return  # ì„±ê³µ ì‹œ í•¨ìˆ˜ ì¢…ë£Œ

            except Exception as e:
                retry_count += 1
                log(f"ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨ (ì‹œë„ {retry_count}/{max_retries}): {e}", "WARNING")

                if retry_count < max_retries:
                    wait_time = retry_count * 5  # 5ì´ˆ, 10ì´ˆ, 15ì´ˆ ëŒ€ê¸°
                    log(f"{wait_time}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...", "INFO")
                    await asyncio.sleep(wait_time)
                else:
                    log("ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ ì´ë™ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼", "ERROR")
                    raise e

    async def _close_popups(self):
        """í˜ì´ì§€ì˜ ëª¨ë“  íŒì—…ì„ ë‹«ëŠ” ë©”ì„œë“œ"""
        try:
            # 1. ì¼ë°˜ì ì¸ íŒì—… ë‹«ê¸° ë²„íŠ¼ë“¤ ì‹œë„
            popup_close_selectors = [
                ".pop-btn-close",  # ì¼ë°˜ì ì¸ íŒì—… ë‹«ê¸° ë²„íŠ¼
                ".btnClosepop",    # íŠ¹ì • íŒì—… ë‹«ê¸° ë²„íŠ¼
                "#popupNotice .pop-btn-close",  # ê³µì§€ì‚¬í•­ íŒì—… ë‹«ê¸°
                ".ui-popup .pop-btn-close",     # UI íŒì—… ë‹«ê¸°
                "button[class*='close']",       # closeê°€ í¬í•¨ëœ ë²„íŠ¼
                "a[class*='close']"             # closeê°€ í¬í•¨ëœ ë§í¬
            ]

            for selector in popup_close_selectors:
                try:
                    popup_close = self.page.locator(selector)
                    if await popup_close.count() > 0:
                        # ëª¨ë“  ë§¤ì¹­ë˜ëŠ” ìš”ì†Œì— ëŒ€í•´ ë‹«ê¸° ì‹œë„
                        for i in range(await popup_close.count()):
                            element = popup_close.nth(i)
                            if await element.is_visible():
                                await element.click(timeout=3000)
                                log(f"íŒì—… ë‹«ê¸° ì„±ê³µ: {selector}")
                                await self.page.wait_for_timeout(500)  # íŒì—…ì´ ë‹«í ì‹œê°„ ëŒ€ê¸°
                except Exception:
                    continue  # ë‹¤ìŒ ì…€ë ‰í„° ì‹œë„

            # 2. ESC í‚¤ë¡œ íŒì—… ë‹«ê¸° ì‹œë„
            await self.page.keyboard.press('Escape')
            await self.page.wait_for_timeout(500)

            log("íŒì—… ë‹«ê¸° ì²˜ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            log(f"íŒì—… ë‹«ê¸° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    async def _crawl_categories(self):
        """ëŒ€ë¶„ë¥˜ -> ì¤‘ë¶„ë¥˜ -> ì†Œë¶„ë¥˜ ìˆœì°¨ì ìœ¼ë¡œ í¬ë¡¤ë§"""
        major_selector = '#left_menu_kpi > ul.panel'
        major_categories = await self.page.locator(
            major_selector).first.locator('li.file-item > a').all()

        major_links = []
        for cat in major_categories:
            text = await cat.inner_text()
            href = await cat.get_attribute('href')
            major_links.append({'name': text, 'url': f"{self.base_url}{href}"})

        for major in major_links:
            # í¬ë¡¤ë§ ëª¨ë“œì— ë”°ë¥¸ ëŒ€ë¶„ë¥˜ í•„í„°ë§
            if self.target_major_category:
                # íƒ€ê²Ÿ ëŒ€ë¶„ë¥˜ê°€ ì§€ì •ëœ ê²½ìš°, í•´ë‹¹ ëŒ€ë¶„ë¥˜ë§Œ ì²˜ë¦¬
                if major['name'] != self.target_major_category:
                    continue  # íƒ€ê²Ÿ ëŒ€ë¶„ë¥˜ê°€ ì•„ë‹ˆë©´ ê±´ë„ˆë›°ê¸°

            log(f"ëŒ€ë¶„ë¥˜ '{major['name']}' í¬ë¡¤ë§ ì‹œì‘...")
            await self.page.goto(major['url'])

            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° - ê¸°ë³¸ ëŒ€ê¸° ì‹œê°„ë§Œ ì ìš© (ì„œë²„ í™˜ê²½ ê³ ë ¤í•˜ì—¬ 10ì´ˆ)
            log("í˜ì´ì§€ ë¡œë”©ì„ ìœ„í•œ ê¸°ë³¸ ëŒ€ê¸° ì‹œê°„ ì ìš© (10ì´ˆ)")
            await self.page.wait_for_timeout(10000)

            # ì£¼ì„ì²˜ë¦¬: ë¶ˆí•„ìš”í•œ ì„ íƒì ì‹œë„ ë¶€ë¶„ (ê³„ì† ì‹¤íŒ¨í•˜ì—¬ ì‹œê°„ë§Œ ì†Œëª¨)
            # try:
            #     # ì¹´í…Œê³ ë¦¬ ëª©ë¡ ì»¨í…Œì´ë„ˆ ëŒ€ê¸° (ì—¬ëŸ¬ ì„ íƒì ì‹œë„)
            #     selectors_to_try = [
            #         ".part-list",  # ê¸°ì¡´ ì„ íƒì
            #         "ul li a[href*='CATE_CD=']",  # ì¹´í…Œê³ ë¦¬ ë§í¬ë“¤
            #         "li a[href*='/www/price/category.asp']",  # ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ ë§í¬ë“¤
            #         ".category-list",  # ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤íŠ¸ í´ë˜ìŠ¤
            #     ]
            #
            #     page_loaded = False
            #     for selector in selectors_to_try:
            #         try:
            #             await self.page.wait_for_selector(selector, timeout=10000)
            #             log(f"í˜ì´ì§€ ë¡œë”© ì™„ë£Œ - ì„ íƒì: {selector}")
            #             page_loaded = True
            #             break
            #         except Exception as e:
            #             log(f"ì„ íƒì {selector} ëŒ€ê¸° ì‹¤íŒ¨: {str(e)}")
            #             continue
            #
            #     if not page_loaded:
            #         log("ëª¨ë“  ì„ íƒì ì‹œë„ ì‹¤íŒ¨, ê¸°ë³¸ ëŒ€ê¸° ì‹œê°„ ì ìš©")
            #         await self.page.wait_for_timeout(3000)
            #
            # except Exception as e:
            #     log(f"í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘ ì˜¤ë¥˜: {str(e)}")
            #     await self.page.wait_for_timeout(3000)

            # openSub() ë²„íŠ¼ í´ë¦­í•˜ì—¬ ëª¨ë“  ì¤‘ë¶„ë¥˜ì™€ ì†Œë¶„ë¥˜ë¥¼ í•œë²ˆì— í¼ì¹˜ê¸°
            open_sub_selector = 'a[href="javascript:openSub();"]'
            open_sub_button = self.page.locator(open_sub_selector)
            if await open_sub_button.count() > 0:
                log("openSub() ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ëª¨ë“  ë¶„ë¥˜ë¥¼ í¼ì¹©ë‹ˆë‹¤.")
                await open_sub_button.click()
                # ë¶„ë¥˜ê°€ í¼ì³ì§ˆ ì‹œê°„ì„ ê¸°ë‹¤ë¦¼ (2ì´ˆ -> 5ì´ˆë¡œ ì¦ê°€)
                await self.page.wait_for_timeout(5000) # ë” ë„‰ë„‰í•œ ëŒ€ê¸° ì‹œê°„
                # ë˜ëŠ”, íŠ¹ì • ì¤‘ë¶„ë¥˜ ëª©ë¡ì´ ì‹¤ì œë¡œ visible í•´ì§ˆ ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ê¸° (ë” ê²¬ê³ í•œ ë°©ë²•)
                try:
                    await self.page.wait_for_selector('.part-ttl > a', state='visible', timeout=15000)
                    log("ì¤‘ë¶„ë¥˜ ìš”ì†Œë“¤ì´ í™”ë©´ì— ì™„ì „íˆ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
                except Exception as e:
                    log(f"ì¤‘ë¶„ë¥˜ ìš”ì†Œ ê°€ì‹œì„± ëŒ€ê¸° ì‹¤íŒ¨: {e}", "WARNING")
                    # ì‹¤íŒ¨í•´ë„ ì¼ë‹¨ ì§„í–‰í•˜ë„ë¡ í•¨. ë‹¤ìŒ ë¡œì§ì—ì„œ ë‹¤ì‹œ ìš”ì†Œë¥¼ ì°¾ì„ ê²ƒì´ë¯€ë¡œ.

                # HTML êµ¬ì¡° í™•ì¸ì„ ìœ„í•´ í˜ì´ì§€ ë‚´ìš© ì¶œë ¥
                html_content = await self.page.content()
                # HTML êµ¬ì¡° í™•ì¸ì„ ìœ„í•´ í˜ì´ì§€ ë‚´ìš© ì¶œë ¥
                part_ttl_start = html_content.find('part-ttl')
                if 'part-ttl' in html_content:
                    sample_end = part_ttl_start + 1000
                    html_sample = html_content[part_ttl_start:sample_end]
                    log(f"í˜ì´ì§€ HTML ìƒ˜í”Œ (part-ttl ê´€ë ¨): {html_sample}")
                else:
                    log("í˜ì´ì§€ HTML ìƒ˜í”Œ (part-ttl ê´€ë ¨): "
                        "part-ttl ì—†ìŒ")
            else:
                # ëŒ€ë¶„ë¥˜ 'ê³µí†µìì¬' í´ë¦­í•˜ì—¬ ì¤‘ë¶„ë¥˜ ëª©ë¡ í¼ì¹˜ê¸°
                # ì´ ë¶€ë¶„ë„ ìœ„ì— openSub() ì²˜ëŸ¼ ëŒ€ê¸° ì‹œê°„ì„ ëŠ˜ë ¤ì£¼ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
                category_link = 'a[href="category.asp?CATE_CD=101"]'
                await self.page.click(category_link)
                await self.page.wait_for_timeout(3000) # 1ì´ˆ -> 3ì´ˆë¡œ ì¦ê°€
                log("ì¤‘ë¶„ë¥˜ ë° ì†Œë¶„ë¥˜ ëª©ë¡ì„ í¼ì³¤ìŠµë‹ˆë‹¤.")

            # ì†Œë¶„ë¥˜ ëª©ë¡ì´ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ëŒ€ê¸°
            # ì´ wait_for_selectorëŠ” ì‚¬ì‹¤ìƒ .part-ttl ì•„ë˜ì˜ ì¤‘ë¶„ë¥˜ë¥¼ ëŒ€ê¸°í•˜ëŠ” ê²ƒì´ë¯€ë¡œ,
            # .part-ttlì´ visible ìƒíƒœê°€ ë˜ëŠ” ê²ƒì„ ê¸°ë‹¤ë¦¬ëŠ” ê²ƒì´ ë” ì •í™•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            await self.page.wait_for_selector(".part-list", timeout=15000) # íƒ€ì„ì•„ì›ƒ ì¦ê°€

            # ì¤‘ë¶„ë¥˜ ì •ë³´ë¥¼ ë¯¸ë¦¬ ìˆ˜ì§‘
            middle_selector = '.part-ttl > a'
            middle_categories_elements = await self.page.locator(
                middle_selector).all()
            log(f"  ë°œê²¬ëœ ì¤‘ë¶„ë¥˜ ê°œìˆ˜: {len(middle_categories_elements)}")

            middle_categories_info = []
            for i, middle_element in enumerate(middle_categories_elements):
                try:
                    middle_name = await middle_element.inner_text()
                    middle_href = await middle_element.get_attribute('href')
                    if middle_href and 'CATE_CD=' in middle_href:
                        middle_categories_info.append({
                            'name': middle_name,
                            'href': middle_href
                        })
                        log(f"  ë°œê²¬ëœ ì¤‘ë¶„ë¥˜: '{middle_name}' "
                            f"(CATE_CD: {middle_href})")
                except Exception as e:
                    log(f"  ì¤‘ë¶„ë¥˜ {i + 1} ì •ë³´ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    continue

            # ê° ì¤‘ë¶„ë¥˜ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ë°©ë¬¸í•˜ì—¬ ì†Œë¶„ë¥˜ ìˆ˜ì§‘
            for middle_info in middle_categories_info:
                middle_name = middle_info['name']
                middle_href = middle_info['href']

                # í¬ë¡¤ë§ ëª¨ë“œì— ë”°ë¥¸ ì¤‘ë¶„ë¥˜ í•„í„°ë§
                if self.crawl_mode in ["middle_only", "sub_only"] and self.target_middle_category:
                    if middle_name != self.target_middle_category:
                        log(f"  [SKIP] íƒ€ê²Ÿ ì¤‘ë¶„ë¥˜ê°€ ì•„ë‹˜: '{middle_name}' ê±´ë„ˆëœë‹ˆë‹¤.")
                        continue
                elif self.crawl_mode == "major_only":
                    # major_only ëª¨ë“œì—ì„œëŠ” ëª¨ë“  ì¤‘ë¶„ë¥˜ ì²˜ë¦¬
                    pass

                # ê¸°ì¡´ INCLUSION_LIST ë¡œì§ (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)
                inclusions_for_major = INCLUSION_LIST.get(major['name'], {})

                # ëŒ€ë¶„ë¥˜ì— ì„¤ì •ì´ ì—†ìœ¼ë©´ ëª¨ë“  ì¤‘ë¶„ë¥˜ ì œì™¸ (ë‹¨, ìƒˆë¡œìš´ ëª¨ë“œì—ì„œëŠ” ë¬´ì‹œ)
                if not inclusions_for_major and self.crawl_mode == "all":
                    log(f"  [SKIP] í¬í•¨ ëª©ë¡ ì—†ìŒ: ì¤‘ë¶„ë¥˜ '{middle_name}' ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue

                # ëŒ€ë¶„ë¥˜ê°€ "__ALL__"ë¡œ ì„¤ì •ëœ ê²½ìš° ëª¨ë“  ì¤‘ë¶„ë¥˜ í¬í•¨
                if inclusions_for_major == "__ALL__":
                    log(f"  ì¤‘ë¶„ë¥˜ '{middle_name}' í¬í•¨ (ëŒ€ë¶„ë¥˜ ì „ì²´ í¬í•¨ ì„¤ì •)")
                else:
                    # ì¤‘ë¶„ë¥˜ê°€ í¬í•¨ ëª©ë¡ì— ì—†ìœ¼ë©´ ì œì™¸
                    if middle_name not in inclusions_for_major:
                        log(f"  [SKIP] í¬í•¨ ëª©ë¡ì— ì—†ìŒ: ì¤‘ë¶„ë¥˜ '{middle_name}' ê±´ë„ˆëœë‹ˆë‹¤.")
                        continue

                try:
                    # ì¤‘ë¶„ë¥˜ í˜ì´ì§€ë¡œ ì´ë™
                    middle_url = f"{self.base_url}/www/price/{middle_href}"
                    log(f"  ì¤‘ë¶„ë¥˜ '{middle_name}' "
                        f"í˜ì´ì§€ë¡œ ì´ë™: {middle_url}")
                    await self.page.goto(middle_url)
                    await self.page.wait_for_load_state(
                        'networkidle')

                    # ì¤‘ë¶„ë¥˜ CATE_CD ì¶”ì¶œ
                    middle_cate_cd_match = re.search(r'CATE_CD=(\d+)', middle_href)
                    middle_cate_cd = middle_cate_cd_match.group(1) if middle_cate_cd_match else None

                    if not middle_cate_cd:
                        log(f"  [SKIP] ì¤‘ë¶„ë¥˜ '{middle_name}'ì˜ CATE_CDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                        continue

                    # ì†Œë¶„ë¥˜ê°€ ìˆ¨ê²¨ì ¸ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì§ì ‘ ì°¾ê¸°
                    await self.page.wait_for_timeout(2000)

                    # ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ì†Œë¶„ë¥˜ ì°¾ê¸°
                    sub_categories_info = []

                    # ë°©ë²• 1: ul.part-list ë‚´ì˜ ë§í¬ë“¤
                    part_list_selector = 'ul.part-list'
                    part_lists = await self.page.locator(
                        part_list_selector).all()
                    for part_list in part_lists:
                        if await part_list.count() > 0:
                            sub_selector = 'li a'
                            sub_elements = await part_list.locator(
                                sub_selector).all()
                            for sub_element in sub_elements:
                                try:
                                    sub_name = await sub_element.inner_text()
                                    sub_href = await sub_element.get_attribute(
                                        'href')
                                    has_cate_cd = (
                                        sub_href and
                                        'CATE_CD=' in sub_href)
                                    if has_cate_cd:
                                        sub_cate_cd_match = re.search(r'CATE_CD=(\d+)', sub_href)
                                        sub_cate_cd = sub_cate_cd_match.group(1) if sub_cate_cd_match else None

                                        # ì¤‘ë¶„ë¥˜ CATE_CDì™€ ì†Œë¶„ë¥˜ CATE_CDê°€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
                                        if sub_cate_cd and sub_cate_cd.startswith(middle_cate_cd):
                                            sub_categories_info.append({
                                                'name': sub_name,
                                                'href': sub_href
                                            })
                                            log(f"    ë°œê²¬ëœ ì†Œë¶„ë¥˜: "
                                                f"'{sub_name}' (CATE_CD: {sub_cate_cd})")
                                        else:
                                            log(f"    [SKIP] ì¤‘ë¶„ë¥˜ '{middle_name}'ê³¼ "
                                                f"ì—°ê´€ ì—†ëŠ” ì†Œë¶„ë¥˜ '{sub_name}' (CATE_CD: {sub_cate_cd}) ê±´ë„ˆëœœ")
                                except Exception as e:
                                    log(f"    ì†Œë¶„ë¥˜ ì •ë³´ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: "
                                        f"{str(e)}")
                                    continue

                    # ë°©ë²• 2: ë§Œì•½ ìœ„ì—ì„œ ì°¾ì§€ ëª»í–ˆë‹¤ë©´ ë‹¤ë¥¸ ì„ íƒì ì‹œë„
                    if not sub_categories_info:
                        try:
                            # ì†Œë¶„ë¥˜ ë§í¬ ì°¾ê¸°
                            detail_selector = (
                                'a[href*="detail.asp?CATE_CD="]')
                            all_links = await self.page.locator(
                                detail_selector).all()
                            for link in all_links:
                                try:
                                    sub_name = await link.inner_text()
                                    sub_href = await link.get_attribute(
                                        'href')
                                    if sub_href and sub_name.strip():
                                        sub_cate_cd_match = re.search(r'CATE_CD=(\d+)', sub_href)
                                        sub_cate_cd = sub_cate_cd_match.group(1) if sub_cate_cd_match else None

                                        # ì¤‘ë¶„ë¥˜ CATE_CDì™€ ì†Œë¶„ë¥˜ CATE_CDê°€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
                                        if sub_cate_cd and sub_cate_cd.startswith(middle_cate_cd):
                                            sub_categories_info.append({
                                                'name': sub_name.strip(),
                                                'href': sub_href
                                            })
                                            log(f"    ë°œê²¬ëœ ì†Œë¶„ë¥˜: "
                                                f"'{sub_name}' (CATE_CD: {sub_cate_cd})")
                                        else:
                                            log(f"    [SKIP] ì¤‘ë¶„ë¥˜ '{middle_name}'ê³¼ "
                                                f"ì—°ê´€ ì—†ëŠ” ì†Œë¶„ë¥˜ '{sub_name}' (CATE_CD: {sub_cate_cd}) ê±´ë„ˆëœœ")
                                except Exception:
                                    continue
                        except Exception as e:
                            # ë°©ë²•2 ì†Œë¶„ë¥˜ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ë¡œê·¸ ìƒëµ)
                            pass

                    if not sub_categories_info:
                        log(f"    ì¤‘ë¶„ë¥˜ '{middle_name}'ì˜ "
                            f"ì†Œë¶„ë¥˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        continue

                    sub_count = len(sub_categories_info)
                    log(f"    ì¤‘ë¶„ë¥˜ '{middle_name}' - "
                        f"ë°œê²¬ëœ ì†Œë¶„ë¥˜ ê°œìˆ˜: {sub_count}")

                    # ìˆ˜ì§‘ëœ ì†Œë¶„ë¥˜ ì •ë³´ë¡œ ë³‘ë ¬ ë°ì´í„° í¬ë¡¤ë§
                    await self._crawl_subcategories_parallel(
                        major['name'], middle_name, sub_categories_info)

                except Exception as e:
                    log(f"  ì¤‘ë¶„ë¥˜ '{middle_name}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    continue

    async def _crawl_subcategories_parallel(self, major_name,
                                            middle_name,
                                            sub_categories_info):
        """ì†Œë¶„ë¥˜ë“¤ì„ ë³‘ë ¬ë¡œ í¬ë¡¤ë§"""
        
        # í¬ë¡¤ë§ ëª¨ë“œì— ë”°ë¥¸ ì†Œë¶„ë¥˜ í•„í„°ë§
        if self.crawl_mode == "sub_only" and self.target_sub_category:
            filtered_subs = []
            log(f"    [DEBUG] íƒ€ê²Ÿ ì†Œë¶„ë¥˜: '{self.target_sub_category}' (ê¸¸ì´: {len(self.target_sub_category)})")
            log(f"    [DEBUG] íƒ€ê²Ÿ ì†Œë¶„ë¥˜ ë°”ì´íŠ¸: {self.target_sub_category.encode('utf-8')}")
            
            # ìœ ë‹ˆì½”ë“œ ì •ê·œí™” import
            import unicodedata
            normalized_target = unicodedata.normalize('NFKC', self.target_sub_category)
            log(f"    [DEBUG] íƒ€ê²Ÿ ì†Œë¶„ë¥˜ ì •ê·œí™”: '{normalized_target}'")
            
            for sub_info in sub_categories_info:
                web_name = sub_info['name']
                normalized_web = unicodedata.normalize('NFKC', web_name)
                
                log(f"    [DEBUG] ì›¹ì‚¬ì´íŠ¸ ì†Œë¶„ë¥˜: '{web_name}' -> ì •ê·œí™”: '{normalized_web}'")
                log(f"    [DEBUG] ì •ê·œí™”ëœ ë¬¸ìì—´ ë¹„êµ ê²°ê³¼: {normalized_web == normalized_target}")
                
                if normalized_web == normalized_target:
                    log(f"    [MATCH] ì†Œë¶„ë¥˜ ë§¤ì¹­ ì„±ê³µ: '{web_name}'")
                    filtered_subs.append(sub_info)
                else:
                    log(f"    [SKIP] íƒ€ê²Ÿ ì†Œë¶„ë¥˜ê°€ ì•„ë‹˜: '{web_name}' ê±´ë„ˆëœë‹ˆë‹¤.")
                    sub_info['skip_reason'] = "íƒ€ê²Ÿ ì†Œë¶„ë¥˜ê°€ ì•„ë‹˜"
            sub_categories_info = filtered_subs
        elif self.crawl_mode in ["major_only", "middle_only"]:
            # major_only, middle_only ëª¨ë“œì—ì„œëŠ” ëª¨ë“  ì†Œë¶„ë¥˜ ì²˜ë¦¬
            pass
        elif self.crawl_mode == "all":
            # ê¸°ì¡´ INCLUSION_LIST ë¡œì§ (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)
            inclusions_for_major = INCLUSION_LIST.get(major_name, {})

            # ëŒ€ë¶„ë¥˜ê°€ "__ALL__"ë¡œ ì„¤ì •ëœ ê²½ìš° ëª¨ë“  ì¤‘ë¶„ë¥˜ì™€ ì†Œë¶„ë¥˜ í¬í•¨
            if inclusions_for_major == "__ALL__":
                log(f"    ëŒ€ë¶„ë¥˜ '{major_name}' ì „ì²´ í¬í•¨ ì„¤ì • - ì¤‘ë¶„ë¥˜ '{middle_name}' ëª¨ë“  ì†Œë¶„ë¥˜ í¬í•¨")
            else:
                sub_inclusion_rule = inclusions_for_major.get(middle_name, {})

                # ì¤‘ë¶„ë¥˜ê°€ "__ALL__"ì´ ì•„ë‹Œ ê²½ìš°, íŠ¹ì • ì†Œë¶„ë¥˜ë§Œ í¬í•¨
                if sub_inclusion_rule != "__ALL__":
                    if isinstance(sub_inclusion_rule, dict) and sub_inclusion_rule:
                        filtered_subs = []
                        for sub_info in sub_categories_info:
                            if sub_info['name'] in sub_inclusion_rule:
                                filtered_subs.append(sub_info)
                            else:
                                log(f"    [SKIP] í¬í•¨ ëª©ë¡ì— ì—†ìŒ: ì†Œë¶„ë¥˜ '{sub_info['name']}' ê±´ë„ˆëœë‹ˆë‹¤.")
                                sub_info['skip_reason'] = "í¬í•¨ ëª©ë¡ì— ì—†ìŒ"
                        sub_categories_info = filtered_subs  # í•„í„°ë§ëœ ëª©ë¡ìœ¼ë¡œ êµì²´
                    else:
                        # ë¹ˆ ë”•ì…”ë„ˆë¦¬ì´ê±°ë‚˜ ì˜ëª»ëœ í˜•ì‹ì¸ ê²½ìš° ëª¨ë“  ì†Œë¶„ë¥˜ ì œì™¸
                        log(f"    [SKIP] í¬í•¨í•  ì†Œë¶„ë¥˜ ì—†ìŒ: ì¤‘ë¶„ë¥˜ '{middle_name}' ëª¨ë“  ì†Œë¶„ë¥˜ ê±´ë„ˆëœë‹ˆë‹¤.")
                        return

        if not sub_categories_info:
            log(f"    ì¤‘ë¶„ë¥˜ '{middle_name}': "
                f"ì²˜ë¦¬í•  ì†Œë¶„ë¥˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        sub_count = len(sub_categories_info)
        log(f"    ì¤‘ë¶„ë¥˜ '{middle_name}': {sub_count}ê°œ "
            f"ì†Œë¶„ë¥˜ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

        # ë³‘ë ¬ ì‘ì—… ìƒì„±
        tasks = []
        for sub_info in sub_categories_info:
            task = self._crawl_single_subcategory(
                major_name, middle_name, sub_info)
            tasks.append(task)

        # ë³‘ë ¬ ì‹¤í–‰
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # ê²°ê³¼ ì²˜ë¦¬ ë° ë°°ì¹˜ ë°ì´í„° ìˆ˜ì§‘
        success_count = 0
        failed_count = 0
        
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                sub_name = sub_categories_info[i]['name']
                log(f"    âŒ ì†Œë¶„ë¥˜ '{sub_name}' ì²˜ë¦¬ ì‹¤íŒ¨: {str(result)}", "ERROR")
                failed_count += 1
            elif result is None:
                sub_name = sub_categories_info[i]['name']
                log(f"    âš ï¸ ì†Œë¶„ë¥˜ '{sub_name}' ì²˜ë¦¬ ê²°ê³¼ ì—†ìŒ", "WARNING")
                failed_count += 1
            else:
                success_count += 1
                # ì„±ê³µí•œ ì†Œë¶„ë¥˜ ë°ì´í„°ë¥¼ ë°°ì¹˜ì— ì¶”ê°€
                sub_info = sub_categories_info[i]
                self.batch_data.append({
                    'major': major_name,
                    'middle': middle_name,
                    'sub': sub_info['name'],
                    'result': result
                })

        log(f"    ì¤‘ë¶„ë¥˜ '{middle_name}' ì™„ë£Œ: {success_count}/{sub_count}ê°œ ì„±ê³µ, {failed_count}ê°œ ì‹¤íŒ¨")
        
        # ë°°ì¹˜ í¬ê¸°ì— ë„ë‹¬í•˜ë©´ ì²˜ë¦¬
        if len(self.batch_data) >= self.batch_size:
            await self._process_batch()

        total_count = len(sub_categories_info)
        log(f"    ì¤‘ë¶„ë¥˜ '{middle_name}' ì™„ë£Œ: "
            f"{success_count}/{total_count}ê°œ ì„±ê³µ")
    
    async def _process_batch(self):
        """ë°°ì¹˜ ë°ì´í„° ì²˜ë¦¬ (pandas ê°€ê³µ ë° supabase ì €ì¥)"""
        if not self.batch_data:
            return
            
        batch_count = len(self.batch_data)
        log(f"\n=== ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {batch_count}ê°œ ì†Œë¶„ë¥˜ ===\n")
        
        # ê° ì†Œë¶„ë¥˜ë³„ë¡œ ë°ì´í„° ì²˜ë¦¬
        total_processed = 0
        total_saved = 0
        
        for batch_item in self.batch_data:
            try:
                # pandas ê°€ê³µ
                processed_data = await self.processor.process_data(
                    batch_item['major'], 
                    batch_item['middle'], 
                    batch_item['sub']
                )
                
                if processed_data:
                    processed_count = len(processed_data)
                    total_processed += processed_count
                    
                    # supabase ì €ì¥
                    saved_count = await self.processor.save_to_supabase(processed_data)
                    total_saved += saved_count
                    
                    if saved_count == 0:
                        log(f"  - {batch_item['sub']}: pandas ê°€ê³µ {processed_count}ê°œ, "
                            f"supabase ì €ì¥ 0ê°œ (ëª¨ë‘ ì¤‘ë³µ ë°ì´í„°)")
                    else:
                        log(f"  - {batch_item['sub']}: pandas ê°€ê³µ {processed_count}ê°œ, "
                            f"supabase ì €ì¥ {saved_count}ê°œ")
                else:
                    # ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ëŠ” ì´ìœ ë¥¼ ëª…í™•íˆ êµ¬ë¶„
                    if batch_item.get('skip_reason') == 'not_found':
                        log(f"  - {batch_item['sub']}: ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì†Œë¶„ë¥˜ëª… ë¯¸ë°œê²¬")
                    elif batch_item.get('skip_reason') == 'no_target':
                        log(f"  - {batch_item['sub']}: íƒ€ê²Ÿ ì†Œë¶„ë¥˜ê°€ ì•„ë‹˜ (í•„í„°ë§ë¨)")
                    else:
                        log(f"  - {batch_item['sub']}: ì²˜ë¦¬í•  ë°ì´í„° ì—†ìŒ (ì›ì¸ ë¯¸ìƒ)")
                    
            except Exception as e:
                log(f"  - {batch_item['sub']} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        
        log(f"\n=== ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: pandas ê°€ê³µ {total_processed}ê°œ, "
            f"supabase ì €ì¥ {total_saved}ê°œ ===\n")
        
        # ë°°ì¹˜ ë°ì´í„° ì´ˆê¸°í™”
        self.batch_data = []
        self.processed_count += batch_count
    
    async def _process_final_batch(self):
        """ë§ˆì§€ë§‰ ë‚¨ì€ ë°°ì¹˜ ë°ì´í„° ì²˜ë¦¬"""
        if self.batch_data:
            log(f"\n=== ìµœì¢… ë°°ì¹˜ ì²˜ë¦¬: {len(self.batch_data)}ê°œ ì†Œë¶„ë¥˜ ===\n")
            await self._process_batch()

    async def _crawl_single_subcategory(self, major_name,
                                        middle_name, sub_info):
        """ë‹¨ì¼ ì†Œë¶„ë¥˜ í¬ë¡¤ë§ (ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì œí•œ)"""
        async with self.semaphore:
            sub_name = sub_info['name']
            sub_href = sub_info['href']
            sub_url = f"{self.base_url}/www/price/{sub_href}"

            log(f"  - ì¤‘ë¶„ë¥˜ '{middle_name}' > "
                f"ì†Œë¶„ë¥˜ '{sub_name}' ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")

            new_page = None
            max_retries = 3
            
            for attempt in range(max_retries):
                try:
                    # ìƒˆë¡œìš´ í˜ì´ì§€ ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•´)
                    new_page = await self.context.new_page()
                    
                    # í˜ì´ì§€ ë¡œë“œ ì¬ì‹œë„ ë¡œì§
                    await new_page.goto(sub_url, timeout=60000)
                    await new_page.wait_for_load_state('networkidle', timeout=45000)
                    
                    # ê°€ê²© ë°ì´í„° ìˆ˜ì§‘
                    result = await self._get_price_data_with_page(
                        new_page, major_name, middle_name, sub_name, sub_url)

                    # í˜ì´ì§€ ì •ë¦¬
                    await new_page.close()
                    new_page = None

                    # ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì¦‰ì‹œ ì²˜ë¦¬í•˜ê³  ì €ì¥
                    has_data = (result and hasattr(result, 'raw_data_list')
                                and result.raw_data_list)
                    if has_data:
                        log(f"  - ì†Œë¶„ë¥˜ '{sub_name}' ë°ì´í„° ì²˜ë¦¬ ë° ì €ì¥ ì‹œì‘")

                        # DataFrameìœ¼ë¡œ ë³€í™˜
                        df = result.to_dataframe()

                        if not df.empty:
                            # DataFrameì„ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
                            processed_data = df.to_dict(orient='records')
                            # Supabaseì— ì €ì¥ (ì¤‘ë³µ ì²´í¬ í™œì„±í™”)
                            saved_count = await result.save_to_supabase(processed_data, 'kpi_price_data', check_duplicates=True)
                            log(f"  âœ… '{sub_name}' ì™„ë£Œ: "
                                f"{len(df)}ê°œ ë°ì´í„° â†’ Supabase ì €ì¥ {saved_count}ê°œ ì„±ê³µ")
                        else:
                            log(f"  âš ï¸ '{sub_name}' ì™„ë£Œ: ì €ì¥í•  ë°ì´í„° ì—†ìŒ")
                    else:
                        log(f"  âš ï¸ '{sub_name}' ì™„ë£Œ: ì²˜ë¦¬í•  ë°ì´í„° ì—†ìŒ")

                    return result

                except Exception as e:
                    if new_page:
                        try:
                            await new_page.close()
                        except:
                            pass
                        new_page = None
                    
                    if attempt == max_retries - 1:
                        error_msg = (f"  âŒ ì†Œë¶„ë¥˜ '{sub_name}' ì²˜ë¦¬ ì‹¤íŒ¨ "
                                     f"[ëŒ€ë¶„ë¥˜: {major_name}, ì¤‘ë¶„ë¥˜: {middle_name}] "
                                     f"(ìµœëŒ€ ì¬ì‹œë„ {max_retries}íšŒ ì´ˆê³¼): {str(e)}")
                        log(error_msg, "ERROR")
                        return None
                    else:
                        log(f"  âš ï¸ ì†Œë¶„ë¥˜ '{sub_name}' ì²˜ë¦¬ ì¬ì‹œë„ {attempt + 1}/{max_retries}: {str(e)}", "WARNING")
                        await asyncio.sleep(5)  # ì¬ì‹œë„ ì „ ëŒ€ê¸°

    async def _get_price_data(self, major_name, middle_name,
                             sub_name, sub_url):
        """ê¸°ì¡´ ë©”ì„œë“œ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼"""
        return await self._get_price_data_with_page(
            self.page, major_name, middle_name, sub_name, sub_url)



    async def _check_existing_data(self, major_name, middle_name, 
                                  sub_name, spec_name):
        """Supabaseì—ì„œ ê¸°ì¡´ ë°ì´í„° í™•ì¸í•˜ì—¬ ì¤‘ë³µ ì²´í¬"""
        try:
            response = self.supabase.table('kpi_price_data').select(
                'date, region, price, specification'
            ).eq(
                'major_category', major_name
            ).eq(
                'middle_category', middle_name
            ).eq(
                'sub_category', sub_name
            ).eq(
                'specification', spec_name
            ).execute()
            
            if response.data:
                # (ë‚ ì§œ, ì§€ì—­, ê°€ê²©, ê·œê²©) ì¡°í•©ìœ¼ë¡œ ì™„ì „ ì¤‘ë³µ ì²´í¬
                # pandas ê°€ê³µ í›„ ì»¬ëŸ¼ëª… ë³€ê²½ ê³ ë ¤ (region_name -> region)
                existing_data = set()
                for item in response.data:
                    existing_data.add((item['date'], item['region'], str(item['price']), item['specification']))
                log(f"        - ê¸°ì¡´ ë°ì´í„° ë°œê²¬: {len(existing_data)}ê°œ (ë‚ ì§œ-ì§€ì—­-ê°€ê²©-ê·œê²© ì¡°í•©)")
                return existing_data
            else:
                log("        - ê¸°ì¡´ ë°ì´í„° ì—†ìŒ: ì „ì²´ ì¶”ì¶œ í•„ìš”")
                return set()
                
        except Exception as e:
            log(f"        - ê¸°ì¡´ ë°ì´í„° í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return set()  # ì˜¤ë¥˜ ì‹œ ì „ì²´ ì¶”ì¶œ
    
    async def _get_available_date_range(self, page):
        """í˜ì´ì§€ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œ ë²”ìœ„ í™•ì¸"""
        try:
            # í—¤ë”ì—ì„œ ë‚ ì§œ ì •ë³´ ì¶”ì¶œ
            selector = "#priceTrendDataArea th"
            await page.wait_for_selector(selector, timeout=5000)
            header_elements = await page.locator(selector).all()

            if len(header_elements) > 1:
                dates = []
                for i in range(1, len(header_elements)):
                    header_text = await header_elements[i].inner_text()
                    dates.append(header_text.strip())
                return dates
            else:
                return []

        except Exception as e:
            log(f"        - ë‚ ì§œ ë²”ìœ„ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return []

    async def _get_price_data_with_page(self, page, major_name, 
                                        middle_name, sub_name, sub_url):
        """ì†Œë¶„ë¥˜ í˜ì´ì§€ì—ì„œ ì›”ë³„ ê°€ê²© ë°ì´í„°ë¥¼ ì¶”ì¶œ"""
        try:
            # í˜ì´ì§€ ìƒíƒœ í™•ì¸
            if page.is_closed():
                log(f"í˜ì´ì§€ê°€ ë‹«í˜€ìˆì–´ '{sub_name}' ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.", "ERROR")
                return None
                
            # í˜ì´ì§€ëŠ” ì´ë¯¸ ë¡œë“œëœ ìƒíƒœë¡œ ì „ë‹¬ë¨
            await page.wait_for_load_state('networkidle', timeout=60000)

            # 'ë¬¼ê°€ì¶”ì´ ë³´ê¸°' íƒ­ìœ¼ë¡œ ì´ë™
            try:
                # GitHub Actions í™˜ê²½ì„ ìœ„í•œ ë” ê¸´ ëŒ€ê¸°ì‹œê°„
                await page.wait_for_load_state('networkidle', timeout=60000)
                await asyncio.sleep(3)  # ì¶”ê°€ ì•ˆì •í™” ëŒ€ê¸°
                
                # ë‹¤ì–‘í•œ ì…€ë ‰í„°ë¡œ íƒ­ ì°¾ê¸° ì‹œë„
                selectors = [
                    'a:has-text("ë¬¼ê°€ì¶”ì´ ë³´ê¸°")',
                    'a[href*="price_trend"]',
                    'a:contains("ë¬¼ê°€ì¶”ì´")',
                    '.tab-menu a:has-text("ë¬¼ê°€ì¶”ì´")',
                    'ul.tab-list a:has-text("ë¬¼ê°€ì¶”ì´")'
                ]
                
                tab_found = False
                for selector in selectors:
                    try:
                        await page.wait_for_selector(selector, timeout=60000)
                        tab_found = True
                        break
                    except:
                        continue
                
                if not tab_found:
                    raise Exception("ë¬¼ê°€ì¶”ì´ ë³´ê¸° íƒ­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                
                # ì¬ì‹œë„ ë¡œì§ ê°œì„  (7íšŒë¡œ ì¦ê°€)
                for retry in range(7):
                    try:
                        # ë‹¤ì–‘í•œ ì…€ë ‰í„°ë¡œ í´ë¦­ ì‹œë„
                        clicked = False
                        for selector in selectors:
                            try:
                                tab_element = page.locator(selector)
                                if await tab_element.count() > 0:
                                    await tab_element.wait_for(state='visible', timeout=30000)
                                    await tab_element.click(timeout=60000)
                                    clicked = True
                                    break
                            except:
                                continue
                        
                        if not clicked:
                            raise Exception("íƒ­ í´ë¦­ ì‹¤íŒ¨")
                        
                        # í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ ëŒ€ê¸° (ë” ê¸´ íƒ€ì„ì•„ì›ƒ)
                        await page.wait_for_selector("#ITEM_SPEC_CD", timeout=60000)
                        await page.wait_for_load_state('networkidle', timeout=45000)
                        break
                    except Exception as e:
                        if retry == 6:
                            raise e
                        log(f"ë¬¼ê°€ì¶”ì´ ë³´ê¸° íƒ­ í´ë¦­ ì¬ì‹œë„ {retry + 1}/7: {e}", "WARNING")
                        await asyncio.sleep(5)  # ì¬ì‹œë„ ê°„ ëŒ€ê¸°ì‹œê°„ ì¦ê°€
                        await page.reload()
                        await page.wait_for_load_state('networkidle', timeout=60000)
                        await asyncio.sleep(3)
                        
            except Exception as e:
                log(f"ë¬¼ê°€ì¶”ì´ ë³´ê¸° íƒ­ í´ë¦­ ì™„ì „ ì‹¤íŒ¨: {str(e)}", "ERROR")
                # í˜ì´ì§€ ìƒíƒœ í™•ì¸ ë° ë³µêµ¬ ì‹œë„
                try:
                    # í˜ì´ì§€ ìƒíƒœ ê²€ì¦ ê°•í™”
                    current_url = page.url
                    log(f"í˜„ì¬ í˜ì´ì§€ URL: {current_url}", "INFO")
                    
                    # í˜ì´ì§€ê°€ ì˜¬ë°”ë¥¸ ìƒíƒœì¸ì§€ í™•ì¸
                    page_title = await page.title()
                    log(f"í˜„ì¬ í˜ì´ì§€ ì œëª©: {page_title}", "INFO")
                    
                    # í˜ì´ì§€ ë¦¬ë¡œë“œ ë° ìƒíƒœ ë³µêµ¬
                    await page.reload()
                    await page.wait_for_load_state('networkidle', timeout=60000)
                    await asyncio.sleep(5)  # ë” ê¸´ ì•ˆì •í™” ëŒ€ê¸°
                    
                    # í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ ê²€ì¦
                    await page.wait_for_load_state('domcontentloaded', timeout=60000)
                    
                    # ë§ˆì§€ë§‰ ì‹œë„: ë‹¤ì–‘í•œ ëŒ€ì²´ ì…€ë ‰í„°ë¡œ íƒ­ ì°¾ê¸°
                    alternative_selectors = [
                        'a[href*="price_trend"]',
                        'a:has-text("ë¬¼ê°€ì¶”ì´")',
                        'a:has-text("ì¶”ì´")',
                        'a[onclick*="price_trend"]',
                        'li:has-text("ë¬¼ê°€ì¶”ì´") a',
                        '.nav-tabs a:has-text("ë¬¼ê°€ì¶”ì´")',
                        'ul li a:has-text("ë¬¼ê°€ì¶”ì´")',
                        'a[title*="ë¬¼ê°€ì¶”ì´"]'
                    ]
                    
                    tab_clicked = False
                    for selector in alternative_selectors:
                        try:
                            # ìš”ì†Œ ì¡´ì¬ í™•ì¸
                            element_count = await page.locator(selector).count()
                            if element_count > 0:
                                log(f"ëŒ€ì²´ ì…€ë ‰í„° ë°œê²¬: {selector} (ê°œìˆ˜: {element_count})", "INFO")
                                
                                # ìš”ì†Œê°€ ë³´ì´ëŠ”ì§€ í™•ì¸
                                element = page.locator(selector).first
                                await element.wait_for(state='visible', timeout=15000)
                                
                                # í´ë¦­ ì‹œë„
                                await element.click(timeout=30000)
                                
                                # í´ë¦­ í›„ í˜ì´ì§€ ìƒíƒœ í™•ì¸
                                await page.wait_for_selector("#ITEM_SPEC_CD", timeout=45000)
                                await page.wait_for_load_state('networkidle', timeout=60000)
                                
                                log(f"ëŒ€ì²´ ì…€ë ‰í„°ë¡œ íƒ­ í´ë¦­ ì„±ê³µ: {selector}", "INFO")
                                tab_clicked = True
                                break
                        except Exception as selector_error:
                            log(f"ëŒ€ì²´ ì…€ë ‰í„° {selector} ì‹¤íŒ¨: {selector_error}", "DEBUG")
                            continue
                    
                    if not tab_clicked:
                        log(f"ëª¨ë“  ëŒ€ì²´ ë°©ë²• ì‹¤íŒ¨ - ì†Œë¶„ë¥˜ ê±´ë„ˆëœ€: {sub_name}", "WARNING")
                        return None
                        
                except Exception as recovery_error:
                    log(f"í˜ì´ì§€ ë³µêµ¬ ì‹œë„ ì‹¤íŒ¨: {recovery_error}", "ERROR")
                    log(f"ëª¨ë“  ëŒ€ì²´ ë°©ë²• ì‹¤íŒ¨ - ì†Œë¶„ë¥˜ ê±´ë„ˆëœ€: {sub_name}", "WARNING")
                    return None

            # ê·œê²© ì„ íƒ ì˜µì…˜ë“¤ ê°€ì ¸ì˜¤ê¸°
            spec_options = await page.locator('#ITEM_SPEC_CD option').all()

            raw_item_data = {
                'major_category_name': major_name,
                'middle_category_name': middle_name,
                'sub_category_name': sub_name,
                'spec_data': []
            }

            # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•´ ê·œê²© ë°ì´í„°ë¥¼ ë¨¼ì € ìˆ˜ì§‘
            spec_list = []
            for option in spec_options:
                spec_value = await option.get_attribute('value')
                spec_name = await option.inner_text()
                if spec_value and spec_name.strip():  # ë¹ˆ ê°’ ì œì™¸
                    spec_list.append({'value': spec_value, 'name': spec_name})

            # ëª¨ë“  ê·œê²©ì„ ìµœì í™”ëœ ìˆœì°¨ ì²˜ë¦¬ë¡œ ì§„í–‰
            log(f"    - {len(spec_list)}ê°œ ê·œê²©ì„ "
                f"ìµœì í™”ëœ ìˆœì°¨ ì²˜ë¦¬ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
            await self._process_specs_optimized(
                page, spec_list, raw_item_data,
                major_name, middle_name, sub_name)

        except Exception as e:
            log(f"  ì†Œë¶„ë¥˜ '{sub_name}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ "
                f"[ëŒ€ë¶„ë¥˜: {major_name}, ì¤‘ë¶„ë¥˜: {middle_name}]: {str(e)}", "ERROR")
            return None

        # ìˆ˜ì§‘ëœ ë°ì´í„° ì²˜ë¦¬ - ìƒˆë¡œìš´ DataProcessor ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        local_processor = create_data_processor('kpi')
        if raw_item_data['spec_data']:
            local_processor.add_raw_data(raw_item_data)
            spec_count = len(raw_item_data['spec_data'])
            log(f"  - '{sub_name}' ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ "
                f"(ì´ {spec_count}ê°œ ê·œê²©)")
        else:
            log(f"  - '{sub_name}': ìˆ˜ì§‘ëœ ê·œê²© ë°ì´í„° ì—†ìŒ")

        return local_processor

    async def _process_specs_optimized(
            self, page, spec_list, raw_item_data,
            major_name, middle_name, sub_name):
        """ìµœì í™”ëœ ìˆœì°¨ ì²˜ë¦¬ - í˜ì´ì§€ ë¦¬ë¡œë“œ ì—†ì´ ë¹ ë¥¸ ê·œê²© ë³€ê²½"""
        
        for i, spec in enumerate(spec_list):
            try:
                spec_value = spec['value']
                spec_name = spec['name']
                log(f"      - ê·œê²© {i + 1}/{len(spec_list)}: "
                    f"'{spec_name}' ì¡°íšŒ ì¤‘...")

                # í•˜ë“œì½”ë”©ëœ ë‹¨ìœ„ ì •ë³´ ì‚¬ìš©
                unit_info = self._get_unit_from_inclusion_list(major_name, middle_name, sub_name, spec_name)
                if unit_info:
                    log(f"      í•˜ë“œì½”ë”©ëœ ë‹¨ìœ„ ì •ë³´ ì‚¬ìš©: {unit_info}")
                    # raw_item_dataì— ë‹¨ìœ„ ì •ë³´ ì €ì¥
                    raw_item_data['unit'] = unit_info
                else:
                    log(f"      í•˜ë“œì½”ë”©ëœ ë‹¨ìœ„ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ - ë‹¨ìœ„ ì—†ì´ ì§„í–‰")

                # ê¸°ì¡´ ë°ì´í„° í™•ì¸
                existing_dates = await self._check_existing_data(
                    major_name, middle_name, sub_name, spec_name
                )

                # ê·œê²© ì„ íƒ (ëŒ€ê¸° ì‹œê°„ ìµœì†Œí™”)
                spec_selector = '#ITEM_SPEC_CD'
                await page.wait_for_selector(spec_selector, timeout=60000) # ìš”ì†Œê°€ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ 60ì´ˆ ëŒ€ê¸°
                await page.locator(spec_selector).wait_for(state='visible', timeout=60000) # ìš”ì†Œê°€ ë³´ì´ëŠ” ìƒíƒœê°€ ë  ë•Œê¹Œì§€ 60ì´ˆ ëŒ€ê¸°
                await page.locator(spec_selector).select_option(value=spec_value, timeout=60000) # select_option íƒ€ì„ì•„ì›ƒ 60ì´ˆë¡œ ì„¤ì •
                await page.wait_for_load_state('networkidle', timeout=45000)

                # ê¸°ê°„ ì„ íƒ (ì²« ë²ˆì§¸ ê·œê²©ì—ì„œë§Œ ì„¤ì •)
                if i == 0:
                    # ì‹œì‘ ê¸°ê°„: 2020ë…„ 1ì›”
                    year_from_selector = '#DATA_YEAR_F'
                    month_from_selector = '#DATA_MONTH_F'
                    await page.locator(year_from_selector).select_option(
                        value='2020')
                    await page.locator(month_from_selector).select_option(
                        value='01')
                    
                    # ì¢…ë£Œ ê¸°ê°„: í˜„ì¬ ë…„ì›”
                    current_date = datetime.now()
                    current_year = str(current_date.year)
                    current_month = str(current_date.month).zfill(2)
                    
                    year_to_selector = '#DATA_YEAR_T'
                    month_to_selector = '#DATA_MONTH_T'
                    await page.locator(year_to_selector).select_option(
                        value=current_year)
                    await page.locator(month_to_selector).select_option(
                        value=current_month)
                    
                    await page.wait_for_load_state(
                        'networkidle', timeout=10000)
                    
                    # ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­ (ê¸°ê°„ ì„¤ì • í›„ ë°˜ë“œì‹œ ì‹¤í–‰) - ì¬ì‹œë„ ë¡œì§ ì¶”ê°€
                    search_selector = 'form[name="sForm"] input[type="image"]'
                    search_button = page.locator(search_selector)
                    
                    # ì¬ì‹œë„ ë¡œì§ (ìµœëŒ€ 3íšŒ ì‹œë„)
                    for attempt in range(3):
                        try:
                            await search_button.click(timeout=10000)
                            break
                        except Exception as e:
                            if attempt == 2:  # ë§ˆì§€ë§‰ ì‹œë„
                                raise e
                            log(f"        - ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/3), 2ì´ˆ í›„ ì¬ì‹œë„...")
                            await asyncio.sleep(2)
                    
                    log(f"        - ê¸°ê°„ ì„¤ì • ì™„ë£Œ: 2020.01 ~ "
            f"{current_year}.{current_month}")
                else:
                    # ì²« ë²ˆì§¸ ê·œê²©ì´ ì•„ë‹Œ ê²½ìš°ì—ë„ ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­ - ì¬ì‹œë„ ë¡œì§ ì¶”ê°€
                    search_selector = 'form[name="sForm"] input[type="image"]'
                    search_button = page.locator(search_selector)
                    
                    # ì¬ì‹œë„ ë¡œì§ (ìµœëŒ€ 3íšŒ ì‹œë„)
                    for attempt in range(3):
                        try:
                            await search_button.click(timeout=10000)
                            break
                        except Exception as e:
                            if attempt == 2:  # ë§ˆì§€ë§‰ ì‹œë„
                                raise e
                            log(f"        - ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/3), 2ì´ˆ í›„ ì¬ì‹œë„...")
                            await asyncio.sleep(2)

                # í…Œì´ë¸” ë¡œë”© ëŒ€ê¸° (ë°ì´í„°ê°€ ë¡œë“œë  ë•Œê¹Œì§€) - ì¬ì‹œë„ ë¡œì§ ì¶”ê°€
                table_selector = "#priceTrendDataArea tr"
                
                # ì¬ì‹œë„ ë¡œì§ (ìµœëŒ€ 3íšŒ ì‹œë„)
                for attempt in range(3):
                    try:
                        await page.wait_for_selector(table_selector, timeout=15000)
                        await page.wait_for_load_state('networkidle', timeout=10000)
                        break
                    except Exception as e:
                        if attempt == 2:  # ë§ˆì§€ë§‰ ì‹œë„
                            raise e
                        log(f"        - í…Œì´ë¸” ë¡œë”© ëŒ€ê¸° ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/3), 2ì´ˆ í›„ ì¬ì‹œë„...")
                        await asyncio.sleep(2)

                # ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œ ë²”ìœ„ í™•ì¸
                available_dates = await self._get_available_date_range(page)
                if not available_dates:
                    log("        - ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œ ì—†ìŒ")
                    continue
                
                # ê¸°ì¡´ ë°ì´í„°ì—ì„œ ë‚ ì§œë§Œ ì¶”ì¶œí•˜ì—¬ ë¹„êµ
                existing_date_set = set()
                if existing_dates:
                    existing_date_set = {item[0] for item in existing_dates}  # íŠœí”Œì˜ ì²« ë²ˆì§¸ ìš”ì†Œ(ë‚ ì§œ)ë§Œ ì¶”ì¶œ
                
                # ëˆ„ë½ëœ ë‚ ì§œë§Œ ì¶”ì¶œ
                missing_dates = [date for date in available_dates 
                               if date not in existing_date_set]
                
                if not missing_dates:
                    continue
                
                # ëˆ„ë½ëœ ë‚ ì§œì— ëŒ€í•´ì„œë§Œ ê°€ê²© ë°ì´í„° ì¶”ì¶œ
                await self._extract_price_data_fast(
                    page, spec_name, raw_item_data, existing_dates, unit_info)

            except Exception as e:
                # ê·œê²© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜
                log(f"ê·œê²© '{spec['name']}' ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}", "ERROR")
                continue

    async def _extract_price_data_fast(self, page, spec_name,
                                       raw_item_data, existing_dates=None, unit_info=None):
        """ë¹ ë¥¸ ê°€ê²© ë°ì´í„° ì¶”ì¶œ - ëˆ„ë½ëœ ë°ì´í„°ë§Œ ì¶”ì¶œ"""
        try:
            # í•˜ë“œì½”ë”©ëœ ë‹¨ìœ„ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©, ì—†ëŠ” ê²½ìš°ì—ë§Œ ì›¹í˜ì´ì§€ì—ì„œ ì¶”ì¶œ
            if unit_info:
                log(f"      - INCLUSION_LIST ë‹¨ìœ„ ì •ë³´ ì‚¬ìš©: {unit_info}")
            else:
                cate_cd = raw_item_data.get('cate_cd')
                item_cd = raw_item_data.get('item_cd')
                
                if cate_cd and item_cd:
                    # ë¬¼ê°€ì •ë³´ ë³´ê¸° í˜ì´ì§€ì—ì„œ ë‹¨ìœ„ ì •ë³´ ì¶”ì¶œ ì‹œë„
                    unit_info = await self._get_unit_from_detail_page(cate_cd, item_cd)
                    if unit_info:
                        log(f"      - ë¬¼ê°€ì •ë³´ ë³´ê¸° í˜ì´ì§€ì—ì„œ ë‹¨ìœ„ ì •ë³´ ì¶”ì¶œ: {unit_info}")
                    else:
                        log(f"      - ë‹¨ìœ„ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            
            # í…Œì´ë¸” êµ¬ì¡° ê°ì§€ ë° ì²˜ë¦¬
            # 1. ì§€ì—­ í—¤ë”ê°€ ìˆëŠ” ë³µí•© í…Œì´ë¸” (ì²« ë²ˆì§¸ ì´ë¯¸ì§€ í˜•íƒœ)
            # 2. ë‚ ì§œì™€ ê°€ê²©ë§Œ ìˆëŠ” ë‹¨ìˆœ í…Œì´ë¸” (ë‘ ë²ˆì§¸ ì´ë¯¸ì§€ í˜•íƒœ)
            
            # í…Œì´ë¸”ì—ì„œ ì²« ë²ˆì§¸ í–‰ í™•ì¸
            all_table_rows = await page.locator("table").nth(1).locator("tr").all()
            if not all_table_rows:
                log(f"      - ê·œê²© '{spec_name}': í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return

            # ì²« ë²ˆì§¸ í–‰ ë¶„ì„í•˜ì—¬ í…Œì´ë¸” íƒ€ì… ê²°ì •
            first_row = all_table_rows[0]
            first_row_elements = await first_row.locator("td").all()
            if not first_row_elements:
                first_row_elements = await first_row.locator("th").all()
            
            if not first_row_elements:
                log(f"      - ê·œê²© '{spec_name}': ì²« ë²ˆì§¸ í–‰ì´ ë¹„ì–´ìˆìŒ")
                return

            # ì²« ë²ˆì§¸ í–‰ì˜ ì²« ë²ˆì§¸ ì…€ í…ìŠ¤íŠ¸ í™•ì¸
            first_cell_text = await first_row_elements[0].inner_text()
            first_cell_clean = first_cell_text.strip()
            
            # í…Œì´ë¸” íƒ€ì… ê²°ì •
            is_simple_table = (
                self._is_valid_date_value(first_cell_clean) or 
                self._is_valid_date_header(first_cell_clean)
            )
            
            if is_simple_table:
                # ë‹¨ìˆœ í…Œì´ë¸” ì²˜ë¦¬ (ë‚ ì§œ + ê°€ê²©)
                await self._extract_simple_table_data(
                    all_table_rows, spec_name, raw_item_data, existing_dates, unit_info
                )
            else:
                # ë³µí•© í…Œì´ë¸” ì²˜ë¦¬ (ì§€ì—­ í—¤ë” + ë‚ ì§œë³„ ë°ì´í„°)
                await self._extract_complex_table_data(
                    all_table_rows, spec_name, raw_item_data, existing_dates, unit_info
                )

        except Exception as e:
            log(f"'{spec_name}' ì˜¤ë¥˜: {str(e)}", "ERROR")

    async def _extract_simple_table_data(self, all_table_rows, spec_name, 
                                       raw_item_data, existing_dates, unit_info=None):
        """ë‹¨ìˆœ í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ (ë‚ ì§œ + ê°€ê²© í˜•íƒœ)"""
        try:
            extracted_count = 0
            default_region = "ì „êµ­"  # ì§€ì—­ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’
            
            for row_idx, row in enumerate(all_table_rows):
                try:
                    # í–‰ì˜ ëª¨ë“  ì…€ ì¶”ì¶œ
                    cells = await row.locator("td").all()
                    if not cells:
                        cells = await row.locator("th").all()
                    
                    if len(cells) < 2:  # ìµœì†Œ ë‚ ì§œì™€ ê°€ê²© í•„ìš”
                        continue
                    
                    # ì²« ë²ˆì§¸ ì…€ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
                    date_str = await cells[0].inner_text()
                    date_clean = date_str.strip()
                    
                    # ë‚ ì§œ ìœ íš¨ì„± ê²€ì¦
                    if not self._is_valid_date_value(date_clean):
                        continue
                    
                    formatted_date = self._format_date_header(date_clean)
                    if not formatted_date:
                        continue
                    
                    # ë‘ ë²ˆì§¸ ì…€ì—ì„œ ê°€ê²© ì¶”ì¶œ
                    price_str = await cells[1].inner_text()
                    
                    if self._is_valid_price(price_str):
                        clean_price = price_str.strip().replace(',', '')
                        try:
                            price_value = float(clean_price)
                            
                            # ì¤‘ë³µ ì²´í¬
                            if existing_dates and (formatted_date, default_region, str(price_value), spec_name) in existing_dates:
                                continue
                            
                            if not unit_info:
                                raise ValueError(f"ë‹¨ìœ„ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. spec_name: {spec_name}")
                            
                            price_data = {
                                'spec_name': spec_name,
                                'region': default_region,
                                'date': formatted_date,
                                'price': price_value,
                                'unit': unit_info
                            }
                            spec_data = raw_item_data['spec_data']
                            spec_data.append(price_data)
                            extracted_count += 1
                            
                            if extracted_count % 50 == 0:
                                log(f"ì§„í–‰: {extracted_count}ê°œ ì¶”ì¶œë¨")
                        except ValueError:
                            continue
                
                except Exception as e:
                    log(f"      - í–‰ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    continue
            
            if extracted_count > 0:
                log(f"'{spec_name}' (ë‹¨ìˆœí˜•): {extracted_count}ê°œ ì™„ë£Œ", "SUCCESS")
                
        except Exception as e:
            log(f"ë‹¨ìˆœ í…Œì´ë¸” ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}", "ERROR")

    async def _extract_complex_table_data(self, all_table_rows, spec_name, 
                                        raw_item_data, existing_dates, unit_info=None):
        """ë³µí•© í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ (ì§€ì—­ í—¤ë” + ë‚ ì§œë³„ ë°ì´í„°)"""
        try:
            # ì§€ì—­ í—¤ë” í–‰ ì¶”ì¶œ (ì²« ë²ˆì§¸ í–‰)
            if len(all_table_rows) < 1:
                return
                
            region_header_row = all_table_rows[0]
            region_header_elements = await region_header_row.locator("td").all()
            if not region_header_elements:
                region_header_elements = await region_header_row.locator("th").all()
            
            if not region_header_elements:
                log(f"      - ê·œê²© '{spec_name}': ì§€ì—­ í—¤ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return

            # ì§€ì—­ í—¤ë” ì¶”ì¶œ (ì²« ë²ˆì§¸ ì»¬ëŸ¼ 'êµ¬ë¶„' ì œì™¸, ìœ íš¨í•œ ì§€ì—­ë§Œ)
            regions = []
            valid_region_indices = []
            for i in range(1, len(region_header_elements)):
                header_text = await region_header_elements[i].inner_text()
                region_name = self._clean_region_name(header_text.strip())
                if self._is_valid_region_name(region_name):
                    regions.append(region_name)
                    valid_region_indices.append(i)

            if not regions:
                return

            # ë°ì´í„° í–‰ ì¶”ì¶œ (ë‘ ë²ˆì§¸ í–‰ë¶€í„°)
            data_rows = all_table_rows[1:] if len(all_table_rows) >= 2 else []
            if not data_rows:
                return

            extracted_count = 0
            # ê° ë‚ ì§œë³„ ë°ì´í„° ì²˜ë¦¬
            for row_idx, row in enumerate(data_rows):
                try:
                    # ì²« ë²ˆì§¸ ì…€ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
                    date_element = row.locator("td").first
                    if not await date_element.count():
                        date_element = row.locator("th").first

                    if not await date_element.count():
                        continue

                    date_str = await date_element.inner_text()
                    date_clean = date_str.strip()
                    
                    # ë‚ ì§œ í˜•ì‹ ë³€í™˜ ë° ì¤‘ë³µ ì²´í¬
                    if not self._is_valid_date_value(date_clean):
                        continue
                    
                    formatted_date = self._format_date_header(date_clean)
                    if not formatted_date:
                        continue
                    
                    # ë‚ ì§œ ìœ íš¨ì„± ì¬ê²€ì¦
                    if not self._is_valid_date_header(date_clean):
                        continue

                    # í•´ë‹¹ í–‰ì˜ ëª¨ë“  ê°€ê²© ì…€ ì¶”ì¶œ (ì²« ë²ˆì§¸ ì…€ ì œì™¸)
                    price_cells = await row.locator("td").all()
                    if not price_cells:
                        all_cells = await row.locator("th").all()
                        price_cells = all_cells[1:] if len(all_cells) > 1 else []

                    # ê° ì§€ì—­ë³„ ê°€ê²© ì²˜ë¦¬
                    for region_idx, region_name in enumerate(regions):
                        cell_idx = region_idx + 1
                        if cell_idx >= len(price_cells):
                            continue
                            
                        price_cell = price_cells[cell_idx]
                        price_str = await price_cell.inner_text()
                        
                        if self._is_valid_price(price_str):
                            clean_price = price_str.strip().replace(',', '')
                            try:
                                price_value = float(clean_price)
                                
                                # ì¤‘ë³µ ì²´í¬
                                if existing_dates and (formatted_date, region_name, str(price_value), spec_name) in existing_dates:
                                    continue
                                    
                                if not unit_info:
                                    raise ValueError(f"ë‹¨ìœ„ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. spec_name: {spec_name}")
                                
                                price_data = {
                                    'spec_name': spec_name,
                                    'region': region_name,
                                    'date': formatted_date,
                                    'price': price_value,
                                    'unit': unit_info
                                }
                                spec_data = raw_item_data['spec_data']
                                spec_data.append(price_data)
                                extracted_count += 1
                                
                                if extracted_count % 50 == 0:
                                    log(f"ì§„í–‰: {extracted_count}ê°œ ì¶”ì¶œë¨")
                            except ValueError:
                                continue
                        else:
                            continue
                except Exception as e:
                    log(f"      - í–‰ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    continue

            if extracted_count > 0:
                log(f"'{spec_name}' (ë³µí•©í˜•): {extracted_count}ê°œ ì™„ë£Œ", "SUCCESS")

        except Exception as e:
            log(f"ë³µí•© í…Œì´ë¸” ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}", "ERROR")

    def _clean_region_name(self, region_str):
        """ì§€ì—­ëª… ì •ë¦¬ í•¨ìˆ˜ - 'ì„œìš¸1', 'ë¶€ì‚°2' í˜•íƒœë¡œ ì •ê·œí™”"""
        import re
        
        # ë™ê·¸ë¼ë¯¸ ìˆ«ìë¥¼ ì¼ë°˜ ìˆ«ìë¡œ ë³€í™˜
        circle_to_num = {
            'â‘ ': '1', 'â‘¡': '2', 'â‘¢': '3', 'â‘£': '4', 'â‘¤': '5',
            'â‘¥': '6', 'â‘¦': '7', 'â‘§': '8', 'â‘¨': '9', 'â‘©': '10'
        }
        clean_region = region_str.strip()
        for circle, num in circle_to_num.items():
            clean_region = clean_region.replace(circle, num)
        
        # 'ì„œ1ìš¸' â†’ 'ì„œìš¸1' í˜•íƒœë¡œ ë³€í™˜
        pattern = r'^([ê°€-í£])(\d+)([ê°€-í£]+)$'
        match = re.match(pattern, clean_region)
        
        if match:
            first_char, number, rest = match.groups()
            clean_region = f"{first_char}{rest}{number}"
        
        return clean_region

    def _is_valid_date_header(self, header_text):
        """ë‚ ì§œ í—¤ë” ìœ íš¨ì„± ê²€ì¦ í•¨ìˆ˜"""
        if not header_text or not header_text.strip():
            return False

        header_str = header_text.strip()
        
        # ë¹ˆ ë¬¸ìì—´ì´ë‚˜ ê³µë°±ë§Œ ìˆëŠ” ê²½ìš°
        if not header_str or header_str.isspace():
            return False

        # ë‚ ì§œ íŒ¨í„´ í™•ì¸ (ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›)
        date_patterns = [
            r'^\d{4}\.\d{1,2}$',  # YYYY.M
            r'^\d{4}-\d{1,2}$',   # YYYY-M
            r'^\d{4}/\d{1,2}$',   # YYYY/M
            r'^\d{4}\.\s*\d{1,2}$',  # YYYY. M (ê³µë°± í¬í•¨)
            r'^\d{4}ë…„\s*\d{1,2}ì›”$',  # YYYYë…„ Mì›”
            r'^\d{4}\s+\d{1,2}$'  # YYYY M (ê³µë°±ìœ¼ë¡œ êµ¬ë¶„)
        ]
        
        for pattern in date_patterns:
            if re.match(pattern, header_str):
                return True

        # ê¸°íƒ€ ì˜ëª»ëœ ê°’ ì²´í¬ (ë” í¬ê´„ì ìœ¼ë¡œ)
        # ì£¼ì˜: 'êµ¬ë¶„'ì€ í…Œì´ë¸”ì˜ ì²« ë²ˆì§¸ ì»¬ëŸ¼ í—¤ë”ë¡œ ì‚¬ìš©ë˜ë¯€ë¡œ ì œì™¸í•˜ì§€ ì•ŠìŒ
        invalid_patterns = [
            'ì§€ì—­', 'í‰ê· ', 'ì „êµ­', 'ê¸°ì¤€', 'í•©ê³„', 'ì´ê³„', 'ì†Œê³„',
            'ë‹¨ìœ„', 'ì›', 'ì²œì›', 'ë§Œì›', 'ì–µì›',
            'ê·œê²©', 'í’ˆëª©', 'ìì¬', 'ì¬ë£Œ'
        ]
        
        # ê°€ê²© ê´€ë ¨ íŒ¨í„´ ì²´í¬ (ë™ê·¸ë¼ë¯¸ ìˆ«ì í¬í•¨)
        price_patterns = [
            'ê°€ê²©', 'ê°€â‘ ê²©', 'ê°€â‘¡ê²©', 'ê°€â‘¢ê²©', 'ê°€â‘£ê²©', 'ê°€â‘¤ê²©',
            'ê°€â‘¥ê²©', 'ê°€â‘¦ê²©', 'ê°€â‘§ê²©', 'ê°€â‘¨ê²©', 'ê°€â‘©ê²©'
        ]
        
        for pattern in invalid_patterns + price_patterns:
            if pattern in header_str:
                log(f"        - ì˜ëª»ëœ íŒ¨í„´ìœ¼ë¡œ ì¸ì‹ëœ ë‚ ì§œ í—¤ë” ì œì™¸: {header_str}")
                return False
        
        # ìˆ«ìë§Œìœ¼ë¡œ êµ¬ì„±ëœ ê²½ìš° (ì—°ë„ë‚˜ ì›”ë§Œ ìˆëŠ” ê²½ìš°)
        if header_str.isdigit():
            # 4ìë¦¬ ìˆ«ìëŠ” ì—°ë„ë¡œ ì¸ì •
            if len(header_str) == 4 and 1900 <= int(header_str) <= 2100:
                return True
            # 1-2ìë¦¬ ìˆ«ìëŠ” ì›”ë¡œ ì¸ì •
            elif len(header_str) <= 2 and 1 <= int(header_str) <= 12:
                return True
            else:
                return False

        return False

    def _format_date_header(self, header_text):
        """ë‚ ì§œ í—¤ë”ë¥¼ YYYY-MM-01 í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        if not header_text or not header_text.strip():
            return header_text
            
        header_str = header_text.strip()
        
        # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ íŒ¨í„´ ì²˜ë¦¬
        date_patterns = [
            (r'^(\d{4})\.(\d{1,2})$', '-'),  # YYYY.M
            (r'^(\d{4})-(\d{1,2})$', '-'),   # YYYY-M
            (r'^(\d{4})/(\d{1,2})$', '-'),   # YYYY/M
            (r'^(\d{4})\.\s*(\d{1,2})$', '-'),  # YYYY. M (ê³µë°± í¬í•¨)
            (r'^(\d{4})ë…„\s*(\d{1,2})ì›”$', '-'),  # YYYYë…„ Mì›”
            (r'^(\d{4})\s+(\d{1,2})$', '-')  # YYYY M (ê³µë°±ìœ¼ë¡œ êµ¬ë¶„)
        ]
        
        for pattern, separator in date_patterns:
            match = re.match(pattern, header_str)
            if match:
                year = match.group(1)
                month = match.group(2).zfill(2)  # ì›”ì„ 2ìë¦¬ë¡œ íŒ¨ë”©
                formatted_date = f"{year}-{month}-01"
                # ë¡œê·¸ ê°„ì†Œí™”: ì²« ë²ˆì§¸ ë³€í™˜ë§Œ ë¡œê·¸ ì¶œë ¥
                if not hasattr(self, '_date_conversion_logged'):
                    log(f"        - ë‚ ì§œ í—¤ë” ë³€í™˜ ì˜ˆì‹œ: {header_str} -> {formatted_date}")
                    self._date_conversion_logged = True
                return formatted_date
        
        # ìˆ«ìë§Œìœ¼ë¡œ êµ¬ì„±ëœ ê²½ìš° ì²˜ë¦¬
        if header_str.isdigit():
            # 4ìë¦¬ ìˆ«ìëŠ” ì—°ë„ë¡œ ì²˜ë¦¬ (1ì›”ë¡œ ì„¤ì •)
            if len(header_str) == 4 and 1900 <= int(header_str) <= 2100:
                formatted_date = f"{header_str}-01-01"
                return formatted_date
            # 1-2ìë¦¬ ìˆ«ìëŠ” ì›”ë¡œ ì²˜ë¦¬ (í˜„ì¬ ì—°ë„ ì‚¬ìš©)
            elif len(header_str) <= 2 and 1 <= int(header_str) <= 12:
                current_year = datetime.now().year
                month = header_str.zfill(2)
                formatted_date = f"{current_year}-{month}-01"
                return formatted_date
        
        # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜ (ë¡œê·¸ ìƒëµ)
        return header_text

    def _is_valid_region_name(self, region_name):
        """ì§€ì—­ëª… ìœ íš¨ì„± ê²€ì¦ í•¨ìˆ˜ - 'ì„œìš¸1', 'ë¶€ì‚°2' í˜•íƒœ í—ˆìš©"""
        if not region_name or not region_name.strip():
            return False

        region_str = region_name.strip()
        
        # ë¹ˆ ë¬¸ìì—´ì´ë‚˜ ê³µë°±ë§Œ ìˆëŠ” ê²½ìš°
        if not region_str or region_str.isspace():
            return False

        # í•œêµ­ ì§€ì—­ëª… íŒ¨í„´ í™•ì¸ (ë” í¬ê´„ì ìœ¼ë¡œ)
        valid_regions = [
            'ê°•ì›', 'ê²½ê¸°', 'ê²½ë‚¨', 'ê²½ë¶', 'ê´‘ì£¼', 'ëŒ€êµ¬', 'ëŒ€ì „', 'ë¶€ì‚°',
            'ì„œìš¸', 'ì„¸ì¢…', 'ìš¸ì‚°', 'ì¸ì²œ', 'ì „ë‚¨', 'ì „ë¶', 'ì œì£¼', 'ì¶©ë‚¨', 'ì¶©ë¶',
            'ìˆ˜ì›', 'ì„±ë‚¨', 'ì¶˜ì²œ'  # ì¶”ê°€ ì§€ì—­ëª…
        ]

        # ìˆ«ìê°€ í¬í•¨ëœ ì§€ì—­ëª…ë„ í—ˆìš© (ì˜ˆ: ì„œìš¸1, ë¶€ì‚°2)
        # ìƒˆë¡œìš´ íŒ¨í„´: ì§€ì—­ëª… + ìˆ«ì (ì„œìš¸1, ë¶€ì‚°2 ë“±)
        for region in valid_regions:
            if region in region_str:
                # ì§€ì—­ëª…ì´ í¬í•¨ë˜ì–´ ìˆê³ , ìˆ«ìê°€ ë’¤ì— ì˜¤ëŠ” íŒ¨í„´ í—ˆìš©
                pattern = f"{region}\\d*$"
                if re.search(pattern, region_str):
                    return True
                # ê¸°ì¡´ íŒ¨í„´ë„ í—ˆìš© (ì§€ì—­ëª…ë§Œ)
                if region_str == region:
                    return True

        # ë‚ ì§œ íŒ¨í„´ì´ í¬í•¨ëœ ê²½ìš° ì§€ì—­ëª…ì´ ì•„ë‹˜ (ë” ì—„ê²©í•˜ê²Œ)
        date_patterns = [
            r'\d{4}[./-]\d{1,2}',  # YYYY.M, YYYY/M, YYYY-M
            r'\d{4}\.\s*\d{1,2}',  # YYYY. M (ê³µë°± í¬í•¨)
            r'^\d{4}$',  # ì—°ë„ë§Œ
            r'^\d{1,2}$',  # ì›”ë§Œ
            r'^\d{4}ë…„',  # YYYYë…„
            r'^\d{1,2}ì›”'  # Mì›”
        ]
        
        for pattern in date_patterns:
            if re.search(pattern, region_str):
                # ë¡œê·¸ ìµœì í™”: ë‚ ì§œ íŒ¨í„´ ì œì™¸ ë¡œê·¸ ìƒëµ
                return False

        # ê¸°íƒ€ ì˜ëª»ëœ ê°’ ì²´í¬ (ë” í¬ê´„ì ìœ¼ë¡œ)
        invalid_patterns = [
            'êµ¬ë¶„', 'í‰ê· ', 'ì „êµ­', 'ê¸°ì¤€', 'í•©ê³„', 'ì´ê³„', 'ì†Œê³„',
            'ë‹¨ìœ„', 'ì›', 'ì²œì›', 'ë§Œì›', 'ì–µì›',
            'ë…„', 'ì›”', 'ì¼', 'ê¸°ê°„',
            '-', '/', '\\', '|', '+', '='
        ]
        
        for pattern in invalid_patterns:
            if pattern in region_str:
                # ë¡œê·¸ ìµœì í™”: ì˜ëª»ëœ íŒ¨í„´ ì œì™¸ ë¡œê·¸ ìƒëµ
                return False
        
        # ìˆ«ìë§Œìœ¼ë¡œ êµ¬ì„±ëœ ê²½ìš° ì œì™¸
        if region_str.isdigit():
            return False
        
        # íŠ¹ìˆ˜ë¬¸ìë§Œìœ¼ë¡œ êµ¬ì„±ëœ ê²½ìš° ì œì™¸
        if not re.search(r'[ê°€-í£a-zA-Z]', region_str):
            return False

        return True

    def _is_valid_date_value(self, date_value):
        """ë‚ ì§œ ê°’ì´ ìœ íš¨í•œì§€ í™•ì¸"""
        if date_value is None:
            return False

        # datetime ê°ì²´ì¸ ê²½ìš° ìœ íš¨
        if hasattr(date_value, 'strftime'):
            return True

        # ë¬¸ìì—´ì¸ ê²½ìš° ê²€ì¦
        if isinstance(date_value, str):
            # ë™ê·¸ë¼ë¯¸ ìˆ«ìë‚˜ íŠ¹ìˆ˜ë¬¸ìê°€ í¬í•¨ëœ ê²½ìš° ì œì™¸
            circle_chars = ['â‘ ', 'â‘¡', 'â‘¢', 'â‘£', 'â‘¤', 'â‘¥', 'â‘¦', 'â‘§', 'â‘¨', 'â‘©']
            if any(char in date_value for char in circle_chars):
                return False

            # 'ê°€ê²©' ë“±ì˜ í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ ê²½ìš° ì œì™¸
            if any(char in date_value for char in ['ê°€', 'ê²©', 'êµ¬', 'ë¶„']):
                return False

            # 'YYYY-MM-DD' í˜•íƒœ í™•ì¸
            if re.match(r'^\d{4}-\d{2}-\d{2}$', date_value.strip()):
                return True

            # 'YYYY. M' í˜•íƒœ í™•ì¸
            date_pattern = r'^\d{4}\.\s*\d{1,2}$'
            if re.match(date_pattern, date_value.strip()):
                return True

        return False

    def _is_valid_price(self, price_str):
        """ê°€ê²© ë°ì´í„° ìœ íš¨ì„± ê²€ì¦ - ë³€í˜•ëœ í˜•íƒœ í¬í•¨"""
        if not price_str:
            return False
        
        price_stripped = price_str.strip()
        if (not price_stripped or 
            price_stripped == '-' or 
            price_stripped == ''):
            return False
        
        # ë³€í˜•ëœ ê°€ê²© ì»¬ëŸ¼ëª… ì²˜ë¦¬ ('ê°€â‘ ê²©', 'ê°€â‘¡ê²©' ë“±)
        # ìˆ«ìì™€ íŠ¹ìˆ˜ë¬¸ìê°€ í¬í•¨ëœ ê°€ê²© í—¤ë”ëŠ” ì œì™¸
        if any(char in price_stripped for char in ['â‘ ', 'â‘¡', 'â‘¢', 'â‘£', 'â‘¤', 'â‘¥', 'â‘¦', 'â‘§', 'â‘¨', 'â‘©']):
            return False
        
        # í•œê¸€ì´ í¬í•¨ëœ ê²½ìš° ì œì™¸ (í—¤ë”ì¼ ê°€ëŠ¥ì„±)
        if any('\u3131' <= char <= '\u3163' or '\uac00' <= char <= '\ud7a3' for char in price_stripped):
            return False
        
        # ì˜ë¬¸ìê°€ í¬í•¨ëœ ê²½ìš° ì œì™¸ (í—¤ë”ì¼ ê°€ëŠ¥ì„±)
        if any(char.isalpha() for char in price_stripped):
            return False
            
        clean_price = price_stripped.replace(',', '')
        try:
            float(clean_price)
            return True
        except ValueError:
            return False

            # await page.close()
            
            # if unit:
            #     log(f"ë‹¨ìœ„ ì •ë³´ ì¶”ì¶œ ì„±ê³µ: {cate_cd}-{item_cd} -> {unit}")
            #     return unit
            # else:
            #     log(f"ë‹¨ìœ„ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {cate_cd}-{item_cd}")
            #     return None
                
        # except Exception as e:
        #     log(f"ë‹¨ìœ„ ì •ë³´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {cate_cd}-{item_cd}, {str(e)}", "ERROR")
        #     if 'page' in locals():
        #         await page.close()
        #     return None

    # ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” unit ì¶”ì¶œ í•¨ìˆ˜ (í•˜ë“œì½”ë”©ëœ unit ì‚¬ìš©ìœ¼ë¡œ ëŒ€ì²´)
    # def _get_unit_with_caching(self, cate_cd, item_cd):
    #     """ìºì‹±ì„ ì‚¬ìš©í•˜ì—¬ ë‹¨ìœ„ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    #     # ìºì‹œ í‚¤ ìƒì„±
    #     cache_key = f"unit_{cate_cd}_{item_cd}"
    #     
    #     # ë©”ëª¨ë¦¬ ìºì‹œì—ì„œ í™•ì¸
    #     if not hasattr(self, '_unit_cache'):
    #         self._unit_cache = {}
    #     
    #     if cache_key in self._unit_cache:
    #         return self._unit_cache[cache_key]
    #     
    #     # íŒŒì¼ ìºì‹œì—ì„œ í™•ì¸
    #     unit = self._load_unit_from_file_cache(cate_cd, item_cd)
    #     if unit:
    #         # ë©”ëª¨ë¦¬ ìºì‹œì—ë„ ì €ì¥
    #         self._unit_cache[cache_key] = unit
    #         return unit
    #     
    #     # ìºì‹œì— ì—†ìœ¼ë©´ None ë°˜í™˜ (ë¹„ë™ê¸° í•¨ìˆ˜ì—ì„œ ì‹¤ì œ ì¶”ì¶œ)
    #     return None
    
    # ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” unit ì¶”ì¶œ í•¨ìˆ˜ (í•˜ë“œì½”ë”©ëœ unit ì‚¬ìš©ìœ¼ë¡œ ëŒ€ì²´)
    # def _cache_unit(self, cate_cd, item_cd, unit):
    #     """ë‹¨ìœ„ ì •ë³´ë¥¼ ìºì‹œì— ì €ì¥í•©ë‹ˆë‹¤."""
    #     if not hasattr(self, '_unit_cache'):
    #         self._unit_cache = {}
    #     
    #     cache_key = f"unit_{cate_cd}_{item_cd}"
    #     self._unit_cache[cache_key] = unit
    #     log(f"ë‹¨ìœ„ ì •ë³´ ìºì‹œ ì €ì¥: {cache_key} -> {unit}")
    #     
    #     # íŒŒì¼ ê¸°ë°˜ ìºì‹œì—ë„ ì €ì¥
    #     self._save_unit_to_file_cache(cate_cd, item_cd, unit)

    # ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” unit ì¶”ì¶œ í•¨ìˆ˜ (í•˜ë“œì½”ë”©ëœ unit ì‚¬ìš©ìœ¼ë¡œ ëŒ€ì²´)
    # def _save_unit_to_file_cache(self, cate_cd, item_cd, unit):
    #     """ë‹¨ìœ„ ì •ë³´ë¥¼ íŒŒì¼ ìºì‹œì— ì €ì¥í•©ë‹ˆë‹¤."""
    #     try:
    #         import json
    #         import os
    #         
    #         cache_dir = os.path.join(os.path.dirname(__file__), 'cache')
    #         os.makedirs(cache_dir, exist_ok=True)
    #         
    #         cache_file = os.path.join(cache_dir, 'unit_cache.json')
    #         
    #         # ê¸°ì¡´ ìºì‹œ ë¡œë“œ
    #         cache_data = {}
    #         if os.path.exists(cache_file):
    #             with open(cache_file, 'r', encoding='utf-8') as f:
    #                 cache_data = json.load(f)
    #         
    #         # ìƒˆ ë°ì´í„° ì¶”ê°€
    #         cache_key = f"{cate_cd}_{item_cd}"
    #         cache_data[cache_key] = unit
    #         
    #         # íŒŒì¼ì— ì €ì¥
    #         with open(cache_file, 'w', encoding='utf-8') as f:
    #             json.dump(cache_data, f, ensure_ascii=False, indent=2)
    #             
    #         log(f"íŒŒì¼ ìºì‹œ ì €ì¥: {cache_key} -> {unit}")
    #         
    #     except Exception as e:
    #         log(f"íŒŒì¼ ìºì‹œ ì €ì¥ ì˜¤ë¥˜: {str(e)}", "ERROR")

    def _get_unit_from_inclusion_list(self, major_name, middle_name, sub_name, spec_name=None):
        """INCLUSION_LISTì—ì„œ ë‹¨ìœ„ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        try:
            log(f"[DEBUG] ë‹¨ìœ„ ì¡°íšŒ ì‹œì‘: major='{major_name}', middle='{middle_name}', sub='{sub_name}'")
            
            # INCLUSION_LISTì—ì„œ í•´ë‹¹ ê²½ë¡œì˜ ë‹¨ìœ„ ì •ë³´ ì°¾ê¸°
            if major_name in INCLUSION_LIST:
                log(f"[DEBUG] ëŒ€ë¶„ë¥˜ '{major_name}' ë°œê²¬")
                major_data = INCLUSION_LIST[major_name]
                
                if middle_name in major_data:
                    log(f"[DEBUG] ì¤‘ë¶„ë¥˜ '{middle_name}' ë°œê²¬")
                    middle_data = major_data[middle_name]
                    
                    log(f"[DEBUG] ì¤‘ë¶„ë¥˜ ë°ì´í„° í‚¤ë“¤: {list(middle_data.keys())}")
                    
                    # ì •í™•í•œ ë§¤ì¹­ ì‹œë„
                    if isinstance(middle_data, dict) and sub_name in middle_data:
                        log(f"[DEBUG] ì†Œë¶„ë¥˜ '{sub_name}' ì •í™• ë§¤ì¹­ ì„±ê³µ")
                        unit_data = middle_data[sub_name]
                        
                        # ë‹¨ìˆœ ë¬¸ìì—´ì¸ ê²½ìš° (ê¸°ì¡´ ë°©ì‹)
                        if isinstance(unit_data, str):
                            log(f"í•˜ë“œì½”ë”©ëœ ë‹¨ìœ„ ì •ë³´ ë°œê²¬: {major_name} > {middle_name} > {sub_name} = {unit_data}")
                            return unit_data
                        
                        # ê°ì²´ì¸ ê²½ìš° (ê·œê²©ë³„ ë‹¨ìœ„ ì²˜ë¦¬)
                        elif isinstance(unit_data, dict):
                            # "unit" í‚¤ê°€ ì§ì ‘ ìˆëŠ” ê²½ìš° (ì˜ˆ: {"unit": "ton"})
                            if "unit" in unit_data and isinstance(unit_data["unit"], str):
                                log(f"í•˜ë“œì½”ë”©ëœ ë‹¨ìœ„ ì •ë³´ ë°œê²¬: {major_name} > {middle_name} > {sub_name} = {unit_data['unit']}")
                                return unit_data["unit"]
                            # ê·œê²©ëª…ì´ ì œê³µëœ ê²½ìš° specificationsì—ì„œ ì°¾ê¸°
                            if spec_name and 'specifications' in unit_data:
                                specifications = unit_data['specifications']
                                
                                # ì •í™•í•œ ê·œê²©ëª… ë§¤ì¹­
                                if spec_name in specifications:
                                    unit = specifications[spec_name]
                                    log(f"ê·œê²©ë³„ ë‹¨ìœ„ ì •ë³´ ë°œê²¬: {major_name} > {middle_name} > {sub_name} > {spec_name} = {unit}")
                                    return unit
                                
                                # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„
                                for spec_key, spec_unit in specifications.items():
                                    if spec_name in spec_key or spec_key in spec_name:
                                        log(f"ê·œê²©ë³„ ë‹¨ìœ„ ì •ë³´ ë¶€ë¶„ ë§¤ì¹­: {major_name} > {middle_name} > {sub_name} > {spec_name} â‰ˆ {spec_key} = {spec_unit}")
                                        return spec_unit
                            
                            # ê¸°ë³¸ ë‹¨ìœ„ ë°˜í™˜
                            if 'default' in unit_data:
                                default_unit = unit_data['default']
                                log(f"ê¸°ë³¸ ë‹¨ìœ„ ì •ë³´ ì‚¬ìš©: {major_name} > {middle_name} > {sub_name} = {default_unit}")
                                return default_unit
                    else:
                        # ì •í™•í•œ ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ìœ ì‚¬ ë§¤ì¹­ ì‹œë„
                        log(f"[DEBUG] ì •í™•í•œ ë§¤ì¹­ ì‹¤íŒ¨, ìœ ì‚¬ ë§¤ì¹­ ì‹œë„")
                        for key in middle_data.keys():
                            # í•œì ë¶€ë¶„ì„ ì œê±°í•˜ê³  ë¹„êµ
                            key_without_hanja = re.sub(r'\([^)]*\)', '', key).strip()
                            sub_without_hanja = re.sub(r'\([^)]*\)', '', sub_name).strip()
                            
                            log(f"[DEBUG] í‚¤ ë¹„êµ: '{key}' vs '{sub_name}'")
                            log(f"[DEBUG] í•œì ì œê±° í›„: '{key_without_hanja}' vs '{sub_without_hanja}'")
                            
                            # ì •ê·œí™”ëœ ë¬¸ìì—´ë¡œ ë¹„êµ
                            if key_without_hanja == sub_without_hanja or key == sub_name:
                                log(f"[DEBUG] ìœ ì‚¬ ë§¤ì¹­ ì„±ê³µ: '{key}' â‰ˆ '{sub_name}'")
                                unit_data = middle_data[key]
                                
                                if isinstance(unit_data, str):
                                    log(f"í•˜ë“œì½”ë”©ëœ ë‹¨ìœ„ ì •ë³´ ë°œê²¬ (ìœ ì‚¬ë§¤ì¹­): {major_name} > {middle_name} > {key} = {unit_data}")
                                    return unit_data
                                elif isinstance(unit_data, dict) and 'default' in unit_data:
                                    default_unit = unit_data['default']
                                    log(f"ê¸°ë³¸ ë‹¨ìœ„ ì •ë³´ ì‚¬ìš© (ìœ ì‚¬ë§¤ì¹­): {major_name} > {middle_name} > {key} = {default_unit}")
                                    return default_unit
                        
                        log(f"[DEBUG] ì†Œë¶„ë¥˜ '{sub_name}' ë§¤ì¹­ ì‹¤íŒ¨. ì‚¬ìš© ê°€ëŠ¥í•œ ì†Œë¶„ë¥˜: {list(middle_data.keys()) if isinstance(middle_data, dict) else 'dictê°€ ì•„ë‹˜'}")
                else:
                    log(f"[DEBUG] ì¤‘ë¶„ë¥˜ '{middle_name}' ì—†ìŒ. ì‚¬ìš© ê°€ëŠ¥í•œ ì¤‘ë¶„ë¥˜: {list(major_data.keys())}")
            else:
                log(f"[DEBUG] ëŒ€ë¶„ë¥˜ '{major_name}' ì—†ìŒ. ì‚¬ìš© ê°€ëŠ¥í•œ ëŒ€ë¶„ë¥˜: {list(INCLUSION_LIST.keys())}")
            
            log(f"í•˜ë“œì½”ë”©ëœ ë‹¨ìœ„ ì •ë³´ ì—†ìŒ: {major_name} > {middle_name} > {sub_name}")
            return None
            
        except Exception as e:
            log(f"í•˜ë“œì½”ë”©ëœ ë‹¨ìœ„ ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}", "ERROR")
            return None

    def _load_unit_from_file_cache(self, cate_cd, item_cd):
        """íŒŒì¼ ìºì‹œì—ì„œ ë‹¨ìœ„ ì •ë³´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            import json
            import os
            
            cache_file = os.path.join(os.path.dirname(__file__), 'cache', 'unit_cache.json')
            
            if not os.path.exists(cache_file):
                return None
                
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            cache_key = f"{cate_cd}_{item_cd}"
            unit = cache_data.get(cache_key)
            
            if unit:
                log(f"íŒŒì¼ ìºì‹œì—ì„œ ë‹¨ìœ„ ì •ë³´ ë¡œë“œ: {cache_key} -> {unit}")
                return unit
                
        except Exception as e:
            log(f"íŒŒì¼ ìºì‹œ ë¡œë“œ ì˜¤ë¥˜: {str(e)}", "ERROR")
            
        return None

    async def _process_specs_parallel(self, spec_list, raw_item_data,
                                     major_name, middle_name, sub_name,
                                     sub_url):
        """ì—¬ëŸ¬ ê·œê²©ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ëŠ” ë©”ì„œë“œ"""
        semaphore = asyncio.Semaphore(2)  # ìµœëŒ€ 2ê°œ ë™ì‹œ ì²˜ë¦¬

        async def process_spec_with_new_page(spec):
            async with semaphore:
                try:
                    # ìƒˆë¡œìš´ í˜ì´ì§€ ìƒì„±
                    new_page = await self.context.new_page()
                    base_url = "https://www.kpi.or.kr/www/price/"
                    await new_page.goto(f"{base_url}{sub_url}")
                    await new_page.wait_for_load_state(
                        'networkidle', timeout=60000)
                    await new_page.wait_for_selector(
                        'body', timeout=5000)

                    # 'ë¬¼ê°€ì¶”ì´ ë³´ê¸°' íƒ­ìœ¼ë¡œ ì´ë™ (ì¬ì‹œë„ ë¡œì§)
                    for retry in range(3):
                        try:
                            # íƒ­ì´ ì¡´ì¬í•˜ëŠ”ì§€ ë¨¼ì € í™•ì¸
                            await new_page.wait_for_selector('a:has-text("ë¬¼ê°€ì¶”ì´ ë³´ê¸°")', timeout=15000)
                            
                            link_name = 'ë¬¼ê°€ì¶”ì´ ë³´ê¸°'
                            link_locator = new_page.get_by_role(
                                'link', name=link_name)
                            await link_locator.click(timeout=30000)
                            await new_page.wait_for_selector(
                                selector="#ITEM_SPEC_CD",
                                timeout=30000
                            )
                            break
                        except Exception as e:
                            if retry == 2:
                                raise e
                            log(f"ë¬¼ê°€ì¶”ì´ ë³´ê¸° íƒ­ í´ë¦­ ì¬ì‹œë„ "
                                f"{retry + 1}/3: {e}", "WARNING")
                            await new_page.reload()
                            await new_page.wait_for_load_state(
                                'domcontentloaded', timeout=10000)
                            await new_page.wait_for_load_state(
                                'networkidle', timeout=30000)

                    # ì„ì‹œ ë°ì´í„° êµ¬ì¡°
                    temp_data = {
                        'major_category_name': major_name,
                        'middle_category_name': middle_name,
                        'sub_category_name': sub_name,
                        'spec_data': []
                    }

                    # ê·œê²© ì²˜ë¦¬
                    await self._process_single_spec(new_page, spec, temp_data)

                    await new_page.close()
                    return temp_data['spec_data']

                except Exception as e:
                    error_msg = (f"    - ë³‘ë ¬ ì²˜ë¦¬ ì¤‘ ê·œê²© '{spec['name']}' ì˜¤ë¥˜ "
                                f"[ëŒ€ë¶„ë¥˜: {major_name}, ì¤‘ë¶„ë¥˜: {middle_name}, "
                                f"ì†Œë¶„ë¥˜: {sub_name}]: {str(e)}")
                    log(error_msg)
                    return []

        # ëª¨ë“  ê·œê²©ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬
        # ëª¨ë“  ê·œê²©ì— ëŒ€í•œ ë³‘ë ¬ ì²˜ë¦¬ íƒœìŠ¤í¬ ìƒì„±
        tasks = [process_spec_with_new_page(spec)
                 for spec in spec_list]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # ê²°ê³¼ ë³‘í•©
        for result in results:
            if isinstance(result, list) and result:
                raw_item_data['spec_data'].extend(result)

    async def _process_single_spec(self, page, spec, raw_item_data):
        """ë‹¨ì¼ ê·œê²©ì— ëŒ€í•œ ë°ì´í„° ì²˜ë¦¬"""
        spec_value = spec['value']
        spec_name = spec['name']
        log(f"    - ê·œê²©: '{spec_name}' ì¡°íšŒ ì¤‘...")

        max_retries = 3
        for attempt in range(max_retries):
            try:
                # ê·œê²© ì„ íƒ
                # ê·œê²© ì„ íƒ
                spec_locator = page.locator('#ITEM_SPEC_CD')
                await spec_locator.select_option(value=spec_value)
                await page.wait_for_load_state(
                    'networkidle', timeout=10000)

                # ê¸°ê°„ ì„ íƒ
                year_locator = page.locator('#DATA_YEAR_F')
                await year_locator.select_option(value=self.start_year)
                month_locator = page.locator('#DATA_MONTH_F')
                await month_locator.select_option(
                    value=self.start_month)
                await page.wait_for_load_state(
                    'networkidle', timeout=10000)

                # ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­ (ì¬ì‹œë„ ë¡œì§ ì¶”ê°€)
                search_selector = 'form[name="sForm"] input[type="image"]'
                search_button = page.locator(search_selector)
                
                # ì¬ì‹œë„ ë¡œì§ (ìµœëŒ€ 3íšŒ ì‹œë„)
                for search_attempt in range(3):
                    try:
                        await search_button.click(timeout=15000)
                        await page.wait_for_load_state(
                            'networkidle', timeout=20000)
                        break
                    except Exception as search_e:
                        if search_attempt == 2:  # ë§ˆì§€ë§‰ ì‹œë„
                            raise search_e
                        log(f"        - ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨ (ì‹œë„ {search_attempt + 1}/3), 2ì´ˆ í›„ ì¬ì‹œë„...")
                        await asyncio.sleep(2)
                break

            except Exception as e:
                if attempt < max_retries - 1:
                    attempt_num = attempt + 1
                    log(f"    - ê·œê²© '{spec_name}' ì‹œë„ {attempt_num} "
                        f"ì‹¤íŒ¨, ì¬ì‹œë„ ì¤‘...")
                    await page.wait_for_timeout(1000)
                    continue
                else:
                    log(f"    - ê·œê²© '{spec_name}' ìµœì¢… ì‹¤íŒ¨: "
                        f"{str(e)}")
                    return

        # ë°ì´í„° í…Œì´ë¸” íŒŒì‹±
        try:
            # ì¬ì‹œë„ ë¡œì§ìœ¼ë¡œ í…Œì´ë¸” ëŒ€ê¸° (ìµœëŒ€ 3íšŒ ì‹œë„)
            for table_attempt in range(3):
                try:
                    await page.wait_for_selector(
                        "#priceTrendDataArea tr", timeout=15000)
                    break
                except Exception as table_e:
                    if table_attempt == 2:  # ë§ˆì§€ë§‰ ì‹œë„
                        raise table_e
                    log(f"        - í…Œì´ë¸” ë¡œë”© ì‹¤íŒ¨ (ì‹œë„ {table_attempt + 1}/3), 2ì´ˆ í›„ ì¬ì‹œë„...")
                    await asyncio.sleep(2)

            # ì „ì²´ í…Œì´ë¸” HTMLì„ ë¡œê·¸ë¡œ ì¶œë ¥í•´ì„œ êµ¬ì¡° í™•ì¸
            table_html = await page.locator("#priceTrendDataArea").inner_html()
            log(f"    - ê·œê²© '{spec_name}': í…Œì´ë¸” HTML êµ¬ì¡°:\n{table_html[:500]}...")

            header_elements = await page.locator(
            "#priceTrendDataArea th").all()
            if not header_elements:
                log(f"    - ê·œê²© '{spec_name}': ë°ì´í„° í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return

            # ì²« ë²ˆì§¸ í—¤ë”ëŠ” 'êµ¬ë¶„'ì´ë¯€ë¡œ ì œì™¸
            dates = [await h.inner_text() for h in header_elements[1:]]
            log(f"    - ê·œê²© '{spec_name}': í—¤ë” ë°ì´í„° = {dates}")

            # ì²« ë²ˆì§¸ ì§€ì—­ ë°ì´í„° í–‰ë§Œ ì¶”ì¶œ (ì˜ˆ: 'ì„œâ‘ ìš¸')
            data_rows = await page.locator("#priceTrendDataArea tr").all()
            log(f"    - ê·œê²© '{spec_name}': ì´ {len(data_rows)}ê°œ í–‰ ë°œê²¬")
            if len(data_rows) < 2:
                log(f"    - ê·œê²© '{spec_name}': ë°ì´í„° í–‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return

            first_row_tds = await data_rows[1].locator("td").all()
            if not first_row_tds:
                log(f"    - ê·œê²© '{spec_name}': ë°ì´í„° ì…€ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return

            region = await first_row_tds[0].inner_text()
            prices = [await td.inner_text() for td in first_row_tds[1:]]
            log(f"    - ê·œê²© '{spec_name}': ì§€ì—­ = {region}, "
                f"ê°€ê²© ë°ì´í„° = {prices}")

            # í—¤ë” ì •ë³´ ì¶”ì¶œ (í…Œì´ë¸” êµ¬ì¡° íŒŒì•…ìš©)
            header_cells = await data_rows[0].locator("th").all()
            headers = []
            for i in range(1, len(header_cells)):  # ì²« ë²ˆì§¸ ì»¬ëŸ¼(êµ¬ë¶„) ì œì™¸
                header_text = await header_cells[i].inner_text()
                # ë™ê·¸ë¼ë¯¸ ìˆ«ìë¥¼ ì¼ë°˜ ìˆ«ìë¡œ ë³€í™˜ (â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘© ë“± -> 1,2,3)
                circle_to_num = {
                    'â‘ ': '1', 'â‘¡': '2', 'â‘¢': '3', 'â‘£': '4', 'â‘¤': '5',
                    'â‘¥': '6', 'â‘¦': '7', 'â‘§': '8', 'â‘¨': '9', 'â‘©': '10'
                }
                cleaned_header = header_text.strip()
                for circle, num in circle_to_num.items():
                    cleaned_header = cleaned_header.replace(circle, num)

                # ë‚ ì§œ í˜•ì‹ ê²€ì¦
                # (YYYY.M ë˜ëŠ” YYYY. M í˜•íƒœë§Œ í—ˆìš©)
                if self._is_valid_date_header(cleaned_header):
                    headers.append(cleaned_header)
                else:
                    log(f"    - ì˜ëª»ëœ ë‚ ì§œ í—¤ë” ì œì™¸: "
                        f"'{cleaned_header}' (ì›ë³¸: '{header_text}')")

            log(f"    - ê·œê²© '{spec_name}': "
                f"í—¤ë” ë°ì´í„° = {headers}")

            price_list = []
            # í…Œì´ë¸” êµ¬ì¡° ë¶„ì„: í—¤ë”ê°€ ë‚ ì§œì´ê³  í–‰ì´ ì§€ì—­ë³„ ë°ì´í„°ì¸ êµ¬ì¡°
            # ê° ì§€ì—­(í–‰)ì— ëŒ€í•´ ì²˜ë¦¬
            # í—¤ë” ì œì™¸í•˜ê³  ëª¨ë“  í–‰ ì²˜ë¦¬
            for i in range(1, len(data_rows)):
                try:
                    row_tds = await data_rows[i].locator("td").all()
                    if len(row_tds) >= 2:
                        # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì€ ì§€ì—­ëª…
                        region_str = await row_tds[0].inner_text()
                        # ë™ê·¸ë¼ë¯¸ ìˆ«ìë¥¼ ì¼ë°˜ ìˆ«ìë¡œ ë³€í™˜ (â‘ â‘¡â‘¢ ë“± -> 1,2,3)
                        circle_to_num = {
                            'â‘ ': '1', 'â‘¡': '2', 'â‘¢': '3', 'â‘£': '4',
                            'â‘¤': '5', 'â‘¥': '6', 'â‘¦': '7', 'â‘§': '8',
                            'â‘¨': '9', 'â‘©': '10'
                        }
                        clean_region = region_str.strip()
                        for circle, num in circle_to_num.items():
                            clean_region = clean_region.replace(
                                circle, num)

                        # ê° ë‚ ì§œë³„ ê°€ê²© ì²˜ë¦¬
                        for j, date_header in enumerate(headers):
                            if j + 1 < len(row_tds):
                                price_str = await row_tds[j + 1].inner_text()
                                # ê°€ê²© ë°ì´í„° ìœ íš¨ì„± ê²€ì¦ ê°•í™”
                                price_stripped = price_str.strip()
                                if price_stripped and price_stripped != '-':
                                    # ì‰¼í‘œê°€ í¬í•¨ëœ ìˆ«ìì¸ì§€ í™•ì¸
                                    clean_price = price_stripped.replace(
                                        ',', '')
                                    is_valid_price = (
                                        clean_price.isdigit() and
                                        int(clean_price) > 0)
                                    if is_valid_price:
                                        # ë‚ ì§œ íŒŒì‹± (YYYY. M í˜•íƒœ)
                                        try:
                                            is_valid_header = (
                                                self._is_valid_date_header(
                                                    date_header))
                                            if is_valid_header:
                                                year_month = (
                                                    date_header.strip()
                                                    .replace(' ', ''))
                                                if '.' in year_month:
                                                    year, month = year_month.split('.')
                                                    date_obj = datetime(
                                                        int(year),
                                                        int(month), 1)
                                                    price_data = {
                                                        'date': date_obj,
                                                        'region': clean_region,
                                                        'price': price_str.strip()
                                                    }
                                                    price_list.append(price_data)
                                                    # ìœ íš¨í•œ ê°€ê²© ë°ì´í„° ì¶”ê°€ ë¡œê·¸
                                                    log(
                                                        f"    - ìœ íš¨í•œ ê°€ê²© ë°ì´í„° ì¶”ê°€: "
                                                        f"{clean_region} "
                                                        f"({date_header}) = "
                                                        f"{price_str.strip()}")
                                            else:
                                                # ì˜ëª»ëœ ë‚ ì§œ í˜•ì‹ ì œì™¸ ë¡œê·¸
                                                log(
                                                    f"    - ì˜ëª»ëœ ë‚ ì§œ í˜•ì‹ ì œì™¸: "
                                                    f"{date_header}")
                                        except Exception as date_parse_error:
                                            # ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜ ë¡œê·¸
                                            log(
                                                f"    - ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: "
                                                f"{date_header} - "
                                                f"{str(date_parse_error)}")
                except Exception as row_error:
                    # í–‰ íŒŒì‹± ì˜¤ë¥˜ ë¡œê·¸
                    log(f"    - í–‰ {i} íŒŒì‹± ì˜¤ë¥˜: {str(row_error)}")
                    continue

            if price_list:
                raw_item_data['spec_data'].append({
                    'specification_name': spec_name,
                    'prices': price_list
                })
                log(f"    - ê·œê²© '{spec_name}': {len(price_list)}ê°œ ê°€ê²© ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
            else:
                log(f"    - ê·œê²© '{spec_name}': ìœ íš¨í•œ ê°€ê²© ë°ì´í„° ì—†ìŒ")

        except Exception as e:
            log(f"    - ê·œê²© '{spec_name}' ë°ì´í„° íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
            return


# --- 4. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ---
# <<< íŒŒì¼ ë§¨ ì•„ë˜ ë¶€ë¶„ì„ ì´ ì½”ë“œë¡œ ì „ì²´ êµì²´ (5/5) >>>

async def main():
    """ë©”ì¸ ì‹¤í–‰ ë¡œì§: ëª…ë ¹í–‰ ì¸ì íŒŒì‹± ë° í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
    # ëª…ë ¹í–‰ ì¸ì íŒŒì‹± - ë‘ ê°€ì§€ ë°©ì‹ ì§€ì›
    # ë°©ì‹ 1: --major="ê³µí†µìì¬" --middle="ë¹„ì² ê¸ˆì†" --sub="ì•Œë£¨ë¯¸ëŠ„" --mode="sub_only"
    # ë°©ì‹ 2: --major ê³µí†µìì¬ --middle ë¹„ì² ê¸ˆì† --sub ì•Œë£¨ë¯¸ëŠ„ --mode sub_only
    
    args = {}
    i = 1
    while i < len(sys.argv):
        arg = sys.argv[i]
        if arg.startswith('--'):
            key = arg.strip('-')
            if '=' in arg:
                # ë°©ì‹ 1: --key=value
                key, value = arg.split('=', 1)
                key = key.strip('-')
                value = value.strip('"\'')
                args[key] = value
            else:
                # ë°©ì‹ 2: --key value
                if i + 1 < len(sys.argv) and not sys.argv[i + 1].startswith('--'):
                    value = sys.argv[i + 1].strip('"\'')
                    args[key] = value
                    i += 1
                else:
                    args[key] = True
        i += 1
    
    target_major = args.get('major')
    target_middle = args.get('middle')
    target_sub = args.get('sub')
    crawl_mode = args.get('mode', 'all')
    start_year = args.get('start-year', '2020')
    start_month = args.get('start-month', '01').zfill(2)

    log(f"í¬ë¡¤ë§ ì„¤ì •:")
    log(f"  - ëª¨ë“œ: {crawl_mode}")
    log(f"  - ëŒ€ë¶„ë¥˜: {target_major}")
    log(f"  - ì¤‘ë¶„ë¥˜: {target_middle}")
    log(f"  - ì†Œë¶„ë¥˜: {target_sub}")
    log(f"  - ì‹œì‘ ì‹œì : {start_year}-{start_month}")

    # í¬ë¡¤ë§ ëª¨ë“œì— ë”°ë¥¸ ì‹¤í–‰
    if crawl_mode == "all" and not target_major:
        # ì „ì²´ ëŒ€ë¶„ë¥˜ í¬ë¡¤ë§ (ê¸°ì¡´ ë°©ì‹)
        log("ì „ì²´ ëŒ€ë¶„ë¥˜ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.", "INFO")
        all_major_categories = list(INCLUSION_LIST.keys())
        log(f"í¬ë¡¤ë§í•  ëŒ€ë¶„ë¥˜: {all_major_categories}", "INFO")
        
        for major in all_major_categories:
            log(f"=== {major} í¬ë¡¤ë§ ì‹œì‘ ===", "SUMMARY")
            crawler = KpiCrawler(target_major=major, crawl_mode="all", 
                               start_year=start_year, start_month=start_month)
            await crawler.run()
            log(f"ğŸŸ¢ {major} í¬ë¡¤ë§ ì™„ë£Œ", "SUCCESS")
        
        log("ğŸŸ¢ ì „ì²´ ëŒ€ë¶„ë¥˜ í¬ë¡¤ë§ ì™„ë£Œ", "SUCCESS")
    else:
        # ì„ íƒì  í¬ë¡¤ë§
        log(f"=== {crawl_mode} ëª¨ë“œ í¬ë¡¤ë§ ì‹œì‘ ===", "SUMMARY")
        crawler = KpiCrawler(
            target_major=target_major,
            target_middle=target_middle,
            target_sub=target_sub,
            crawl_mode=crawl_mode,
            start_year=start_year,
            start_month=start_month
        )
        await crawler.run()
        log(f"ğŸŸ¢ {crawl_mode} ëª¨ë“œ í¬ë¡¤ë§ ì™„ë£Œ", "SUCCESS")


async def test_unit_extraction():
    """ë‹¨ìœ„ ì¶”ì¶œ ë¡œì§ í…ŒìŠ¤íŠ¸"""
    log("=== ë‹¨ìœ„ ì¶”ì¶œ ë¡œì§ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===", "SUMMARY")
    
    # í…ŒìŠ¤íŠ¸í•  ë¹„ì² ê¸ˆì† ì†Œë¶„ë¥˜ ëª©ë¡
    test_categories = [
        ("ê³µí†µìì¬", "ë¹„ì² ê¸ˆì†", "ë™ì œí’ˆ(1)"),
        ("ê³µí†µìì¬", "ë¹„ì² ê¸ˆì†", "ë™ì œí’ˆ(2)"),
        ("ê³µí†µìì¬", "ë¹„ì² ê¸ˆì†", "ì•Œë£¨ë¯¸ëŠ„ì œí’ˆ(1)"),
        ("ê³µí†µìì¬", "ë¹„ì² ê¸ˆì†", "ì•Œë£¨ë¯¸ëŠ„ì œí’ˆ(2)"),
        ("ê³µí†µìì¬", "ë¹„ì² ê¸ˆì†", "ë¹„ì² ì§€ê¸ˆ(ééµåœ°é‡‘)"),
        ("ê³µí†µìì¬", "ë¹„ì² ê¸ˆì†", "ì—°(ë‚©)ì œí’ˆ(é‰›è£½å“)")
    ]
    
    crawler = KpiCrawler(target_major="ê³µí†µìì¬", crawl_mode="test")
    
    browser = None
    try:
        # ë¸Œë¼ìš°ì € ì‹œì‘ (run ë©”ì„œë“œì™€ ë™ì¼í•œ ë°©ì‹)
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=True,
                args=[
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-gpu',
                    '--disable-web-security',
                    '--disable-features=VizDisplayCompositor',
                    '--window-size=1920,1080'
                ]
            )
            crawler.context = await browser.new_context()
            crawler.page = await crawler.context.new_page()
            
            await crawler._login()
            
            for major, middle, sub in test_categories:
                log(f"\n--- {sub} ë‹¨ìœ„ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ---", "INFO")
                
                try:
                    # ì†Œë¶„ë¥˜ ì •ë³´ ìƒì„± (ì‹¤ì œ í¬ë¡¤ë§ì—ì„œ ì‚¬ìš©í•˜ëŠ” í˜•íƒœì™€ ë™ì¼)
                    sub_info = {'name': sub, 'href': f'price/price_list.asp?major={major}&middle={middle}&sub={sub}'}
                    sub_url = f"{crawler.base_url}/www/price/{sub_info['href']}"
                    
                    # í˜ì´ì§€ë¡œ ì´ë™
                    await crawler.page.goto(sub_url, timeout=60000)
                    await crawler.page.wait_for_load_state('networkidle', timeout=45000)
                    
                    # ë¬¼ê°€ì¶”ì´ í˜ì´ì§€ì—ì„œ ë‹¨ìœ„ ì¶”ì¶œ (ì˜¬ë°”ë¥¸ ë§¤ê°œë³€ìˆ˜ ê°œìˆ˜)
                    unit_from_trend = await crawler._extract_unit_from_price_trend_page(
                        crawler.page, sub
                    )
                    log(f"  ë¬¼ê°€ì¶”ì´ í˜ì´ì§€ ë‹¨ìœ„: {unit_from_trend}")
                    
                    # ë¬¼ê°€ì •ë³´ ë³´ê¸° íƒ­ í´ë¦­ (ë” ì•ˆì •ì ì¸ ì„ íƒì ì‚¬ìš©)
                    try:
                        # ì—¬ëŸ¬ ê°€ëŠ¥í•œ ì„ íƒìë¡œ ì‹œë„
                        tab_selectors = [
                            'a[href="#tab2"]',
                            'a[onclick*="tab2"]',
                            'a:has-text("ë¬¼ê°€ì •ë³´ ë³´ê¸°")',
                            'a:has-text("ë¬¼ê°€ì •ë³´")',
                            'li:nth-child(2) a',
                            '.tab-menu li:nth-child(2) a'
                        ]
                        
                        tab_clicked = False
                        for selector in tab_selectors:
                            try:
                                tab_element = await crawler.page.query_selector(selector)
                                if tab_element:
                                    await tab_element.click()
                                    await crawler.page.wait_for_timeout(2000)
                                    tab_clicked = True
                                    log(f"  íƒ­ í´ë¦­ ì„±ê³µ: {selector}")
                                    break
                            except Exception as tab_error:
                                log(f"  íƒ­ ì„ íƒì {selector} ì‹¤íŒ¨: {str(tab_error)}", "WARNING")
                                continue
                        
                        if not tab_clicked:
                            log("  âš ï¸ ë¬¼ê°€ì •ë³´ ë³´ê¸° íƒ­ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ. í˜„ì¬ í˜ì´ì§€ì—ì„œ ë‹¨ìœ„ ì¶”ì¶œ ì‹œë„", "WARNING")
                            
                    except Exception as tab_error:
                        log(f"  âš ï¸ íƒ­ í´ë¦­ ì‹¤íŒ¨: {str(tab_error)}", "WARNING")
                    
                    # ë¬¼ê°€ì •ë³´ ë³´ê¸° í˜ì´ì§€ì—ì„œ ë‹¨ìœ„ ì¶”ì¶œ (ì˜¬ë°”ë¥¸ ë§¤ê°œë³€ìˆ˜ ê°œìˆ˜)
                    unit_from_detail = await crawler._get_unit_from_detail_page(
                        crawler.page, sub
                    )
                    log(f"  ë¬¼ê°€ì •ë³´ ë³´ê¸° í˜ì´ì§€ ë‹¨ìœ„: {unit_from_detail}")
                    
                    # ìºì‹œì—ì„œ ë‹¨ìœ„ í™•ì¸ (redis_client ì´ˆê¸°í™” í™•ì¸)
                    cached_unit = None
                    try:
                        if hasattr(crawler, 'redis_client') and crawler.redis_client:
                            cache_key = f"unit_{major}_{middle}_{sub}"
                            cached_unit = await crawler.redis_client.get(cache_key)
                        else:
                            log("  Redis í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ", "WARNING")
                    except Exception as cache_error:
                        log(f"  ìºì‹œ í™•ì¸ ì‹¤íŒ¨: {str(cache_error)}", "WARNING")
                    
                    log(f"  ìºì‹œëœ ë‹¨ìœ„: {cached_unit}")
                    
                    # ê²°ê³¼ ë¹„êµ
                    if unit_from_trend and unit_from_detail:
                        if unit_from_trend == unit_from_detail:
                            log(f"  âœ… ë‹¨ìœ„ ì¼ì¹˜: {unit_from_trend}")
                        else:
                            log(f"  âš ï¸ ë‹¨ìœ„ ë¶ˆì¼ì¹˜ - ì¶”ì´: {unit_from_trend}, ìƒì„¸: {unit_from_detail}")
                    else:
                        log(f"  âŒ ë‹¨ìœ„ ì¶”ì¶œ ì‹¤íŒ¨ - ì¶”ì´: {unit_from_trend}, ìƒì„¸: {unit_from_detail}")
                        
                except Exception as e:
                    import traceback
                    log(f"  âŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "ERROR")
                    log(f"  ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}", "ERROR")
            
            log("\n=== ë‹¨ìœ„ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===", "SUMMARY")
            await browser.close()
        
    except Exception as e:
        log(f"âŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", "ERROR")
        import traceback
        log(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}", "ERROR")
        if browser:
            try:
                await browser.close()
            except:
                pass


if __name__ == "__main__":
    # ëª…ë ¹í–‰ ì¸ìˆ˜ í™•ì¸
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        # ë‹¨ìœ„ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        asyncio.run(test_unit_extraction())
    else:
        # ì¼ë°˜ í¬ë¡¤ë§ ì‹¤í–‰
        # running_crawlers = check_running_crawler()
        running_crawlers = []
        if running_crawlers:
            log(f"ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ í¬ë¡¤ëŸ¬ {len(running_crawlers)}ê°œ ë°œê²¬. ê¸°ì¡´ í¬ë¡¤ëŸ¬ ì™„ë£Œ í›„ ì¬ì‹¤í–‰í•˜ì„¸ìš”.", "ERROR")
            sys.exit(1)

        asyncio.run(main())

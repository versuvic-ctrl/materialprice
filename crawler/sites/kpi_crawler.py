

import logging
import os
import asyncio
import json
import sys
import re
import time
import psutil
from datetime import datetime
from dotenv import load_dotenv
import pandas as pd
from playwright.async_api import async_playwright, Page

# ì ˆëŒ€ importë¥¼ ìœ„í•œ ê²½ë¡œ ì„¤ì •
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

from data_processor import create_data_processor, log
from supabase import create_client, Client

# --- 1. ì´ˆê¸° ì„¤ì • ë° í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ---
load_dotenv("../../.env.local")

# Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
SUPABASE_URL = os.environ.get("NEXT_PUBLIC_SUPABASE_URL")
SUPABASE_KEY = os.environ.get("NEXT_PUBLIC_SUPABASE_ANON_KEY")
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)


# --- 2. ì›¹ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤ ---


def check_running_crawler():
    """ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ í¬ë¡¤ëŸ¬ê°€ ìˆëŠ”ì§€ í™•ì¸"""
    current_pid = os.getpid()
    current_script = os.path.basename(__file__)
    
    running_crawlers = []
    for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
        try:
            if proc.info['pid'] == current_pid:
                continue
                
            cmdline = proc.info['cmdline']
            if cmdline and any(current_script in cmd for cmd in cmdline):
                running_crawlers.append({
                    'pid': proc.info['pid'],
                    'cmdline': ' '.join(cmdline)
                })
        except (psutil.NoSuchProcess, psutil.AccessDenied):
            continue
    
    return running_crawlers

# JSONC íŒŒì¼ì—ì„œ ë‹¨ìœ„ ì •ë³´ë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
def load_unit_data_from_jsonc():
    """kpi_inclusion_list_compact.jsonc íŒŒì¼ì—ì„œ ë‹¨ìœ„ ì •ë³´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        jsonc_file_path = os.path.join(current_dir, "kpi_inclusion_list_compact.jsonc")
        if os.path.exists(jsonc_file_path):
            with open(jsonc_file_path, 'r', encoding='utf-8') as f:
                # JSONC ì£¼ì„ ì œê±°
                content = f.read()
                content = re.sub(r'//.*', '', content)
                content = re.sub(r'/\*.*?\*/', '', content, flags=re.DOTALL)
                data = json.loads(content)
                return data
        else:
            log(f"JSONC íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {jsonc_file_path}")
            return {}
    except Exception as e:
        log(f"JSONC íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {}

# JSONC íŒŒì¼ì—ì„œ ë‹¨ìœ„ ì •ë³´ ë¡œë“œ
UNIT_DATA_FROM_JSONC = load_unit_data_from_jsonc()

# ê¸°ì¡´ INCLUSION_LIST í˜¸í™˜ì„±ì„ ìœ„í•´ JSONC ë°ì´í„°ë¥¼ ë³€í™˜
INCLUSION_LIST = UNIT_DATA_FROM_JSONC

# --- 4. Playwright ì›¹ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤ ---
class KpiCrawler:
    def __init__(self, target_major: str = None, target_middle: str = None,
                 target_sub: str = None, crawl_mode: str = "all",
                 start_year: str = None, start_month: str = None, max_concurrent=5,
                 force_refresh: bool = False):
        """
        KPI í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        
        Args:
            target_major: í¬ë¡¤ë§í•  ëŒ€ë¶„ë¥˜ëª… (Noneì´ë©´ ì „ì²´)
            target_middle: í¬ë¡¤ë§í•  ì¤‘ë¶„ë¥˜ëª… (Noneì´ë©´ ì „ì²´)
            target_sub: í¬ë¡¤ë§í•  ì†Œë¶„ë¥˜ëª… (Noneì´ë©´ ì „ì²´)
            crawl_mode: í¬ë¡¤ë§ ëª¨ë“œ
                - "all": ì „ì²´ í¬ë¡¤ë§ (ê¸°ë³¸ê°’)
                - "major_only": ì§€ì •ëœ ëŒ€ë¶„ë¥˜ë§Œ í¬ë¡¤ë§
                - "middle_only": ì§€ì •ëœ ëŒ€ë¶„ë¥˜ì˜ íŠ¹ì • ì¤‘ë¶„ë¥˜ë§Œ í¬ë¡¤ë§
                - "sub_only": ì§€ì •ëœ ëŒ€ë¶„ë¥˜ì˜ íŠ¹ì • ì¤‘ë¶„ë¥˜ì˜ íŠ¹ì • ì†Œë¶„ë¥˜ë§Œ í¬ë¡¤ë§
            start_year: ì‹œì‘ ì—°ë„ (Noneì´ë©´ í˜„ì¬ ì—°ë„)
            start_month: ì‹œì‘ ì›” (Noneì´ë©´ í˜„ì¬ ì›”)
            max_concurrent: ìµœëŒ€ ë™ì‹œ ì‹¤í–‰ ìˆ˜
            force_refresh: ìºì‹œ ìš°íšŒ ì˜µì…˜ (Trueë©´ ì‹¤ì‹œê°„ ë°ì´í„° ì¡°íšŒ)
        """
        self.base_url = "https://www.kpi.or.kr"
        self.max_concurrent = 4  # 3ì—ì„œ 4ë¡œ ì¦ê°€ (ì„œë²„ ë¶€í•˜ ê³ ë ¤í•˜ì—¬ ì†Œí­ ì¦ê°€)
        self.semaphore = asyncio.Semaphore(self.max_concurrent)
        
        # í˜ì´ì§€ í’€ ê´€ë¦¬ - category_extractor_optimized.pyì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ê°„ì†Œí™”
        self.max_concurrent_pages = 4  # ë™ì‹œ í˜ì´ì§€ ìˆ˜ë¥¼ 3ì—ì„œ 4ê°œë¡œ ì¦ê°€
        self.page_semaphore = asyncio.Semaphore(self.max_concurrent_pages)
        # í˜ì´ì§€ í’€ ì œê±° - ê° ì‘ì—…ë§ˆë‹¤ ìƒˆ í˜ì´ì§€ ìƒì„±/ë‹«ê¸° ë°©ì‹ ì‚¬ìš©
        # self.page_pool = []
        # self.page_pool_semaphore = asyncio.Semaphore(1)  # í˜ì´ì§€ í’€ ì ‘ê·¼ ë™ê¸°í™”
        
        self.supabase = supabase  # ì „ì—­ supabase ê°ì²´ ì°¸ì¡°
        
        # auth.json íŒŒì¼ ê²½ë¡œ ì¶”ê°€
        self.auth_file = os.path.join(os.path.dirname(__file__), "auth.json")
        
        # í˜ì´ì§€ í’€ ê´€ë¦¬ ì œê±° - category_extractor_optimized.py ë°©ì‹ ì‚¬ìš©
        # self.page_pool = []
        # self.pool_size = 3
        
        # ìƒˆë¡œ ì¶”ê°€ëœ ì†ì„±ë“¤
        self.target_major_category = target_major
        self.target_middle_category = target_middle
        self.target_sub_category = target_sub
        self.crawl_mode = crawl_mode
        self.start_year = start_year or str(datetime.now().year)
        self.start_month = start_month or str(datetime.now().month)
        self.force_refresh = force_refresh  # ìºì‹œ ìš°íšŒ ì˜µì…˜ ì €ì¥
        
        self.processor = create_data_processor('kpi')
        
        # JSONC í¬í•¨ í•­ëª© ìºì‹±
        self.included_categories_cache = self._build_included_categories_cache()
        
        # íƒ€ì„ì•„ì›ƒ ê°ì§€ ë° ê°•ì œ ì¬ìƒì„± ì„¤ì •
        self.page_timeout_threshold = 30  # 30ì´ˆ ì´ìƒ ì‘ë‹µ ì—†ìœ¼ë©´ ê°•ì œ ì¬ìƒì„±
        self.page_last_activity = {}  # í˜ì´ì§€ë³„ ë§ˆì§€ë§‰ í™œë™ ì‹œê°„ ì¶”ì 
        
        # ë°°ì¹˜ ì²˜ë¦¬ìš© ë³€ìˆ˜
        self.batch_data = []
        self.batch_size = 6  # ì†Œë¶„ë¥˜ 5ì—ì„œ 6ìœ¼ë¡œ ì¦ê°€ (ì†Œí­ ì¦ê°€)
        self.processed_count = 0
        
        log(f"í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” - í¬ë¡¤ë§ ëª¨ë“œ: {self.crawl_mode}")
        log(f"  íƒ€ê²Ÿ ëŒ€ë¶„ë¥˜: {self.target_major_category}")
        log(f"  íƒ€ê²Ÿ ì¤‘ë¶„ë¥˜: {self.target_middle_category}")
        log(f"  íƒ€ê²Ÿ ì†Œë¶„ë¥˜: {self.target_sub_category}")
        log(f"  ì‹œì‘ë‚ ì§œ: {self.start_year}-{self.start_month}")
        log(f"  ìºì‹œ ìš°íšŒ ëª¨ë“œ: {self.force_refresh}")

    async def run(self):
        """í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        browser = None
        try:
            async with async_playwright() as p:
                # GitHub Actions í™˜ê²½ì—ì„œ ë” ì•ˆì •ì ì¸ ë¸Œë¼ìš°ì € ì„¤ì •
                browser = await p.chromium.launch(
                    headless=False,
                    args=[
                        '--no-sandbox',
                        '--disable-setuid-sandbox',
                        '--disable-dev-shm-usage',
                        '--disable-gpu',
                        '--disable-web-security',
                        '--disable-features=VizDisplayCompositor',
                        '--window-size=1920,1080'
                    ]
                )
                
                # ê¸°ì¡´ ì¸ì¦ ìƒíƒœ ë¡œë“œ ë˜ëŠ” ìƒˆ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
                if os.path.exists(self.auth_file):
                    log("ê¸°ì¡´ ì¸ì¦ ìƒíƒœ ë¡œë“œ")
                    self.context = await browser.new_context(
                        storage_state=self.auth_file)
                else:
                    log("ìƒˆ ë¸Œë¼ìš°ì € ì»¨í…ìŠ¤íŠ¸ ìƒì„±")
                    self.context = await browser.new_context()
                    
                self.page = await self.context.new_page()

                await self._login()
                await self._navigate_to_category()
                await self._crawl_categories()
                
                # ë§ˆì§€ë§‰ ë‚¨ì€ ë°°ì¹˜ ë°ì´í„° ì²˜ë¦¬
                await self._process_final_batch()
                
                log(f"\nğŸŸ¢ === í¬ë¡¤ë§ ì™„ë£Œ: ì´ {self.processed_count}ê°œ ì†Œë¶„ë¥˜ ì²˜ë¦¬ë¨ === ğŸŸ¢\n")

                # í˜ì´ì§€ í’€ ì •ë¦¬ ì œê±° - í˜ì´ì§€ í’€ ê´€ë¦¬ ë°©ì‹ ë³€ê²½
                
                await browser.close()
                return self.processor
        except Exception as e:
            log(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "ERROR")
            # í˜ì´ì§€ í’€ ì •ë¦¬ ì œê±° - í˜ì´ì§€ í’€ ê´€ë¦¬ ë°©ì‹ ë³€ê²½
            if browser:
                try:
                    await browser.close()
                except:
                    pass
            raise

    # í˜ì´ì§€ í’€ ê´€ë¦¬ í•¨ìˆ˜ë“¤ ì œê±° - category_extractor_optimized.py ë°©ì‹ ì‚¬ìš©
    # async def _get_page_from_pool(self):
    #     """í˜ì´ì§€ í’€ì—ì„œ í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜¤ê±°ë‚˜ ìƒˆë¡œ ìƒì„± (ê°„ì†Œí™”ëœ ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸)"""
    #     async with self.page_pool_semaphore:
    #         if self.page_pool:
    #             page = self.page_pool.pop()
    #             log(f"    ğŸ“„ í˜ì´ì§€ í’€ì—ì„œ í˜ì´ì§€ ê°€ì ¸ì˜´ (ë‚¨ì€ í’€ í¬ê¸°: {len(self.page_pool)})")
    #             
    #             # ê°„ì†Œí™”ëœ í˜ì´ì§€ ìƒíƒœ í™•ì¸ - í˜ì´ì§€ê°€ ë‹«í˜”ëŠ”ì§€ë§Œ í™•ì¸
    #             try:
    #                 # í˜ì´ì§€ê°€ ìœ íš¨í•œì§€ ê°„ë‹¨íˆ í™•ì¸
    #                 await page.evaluate('document.title')
    #                 log(f"    âœ… í˜ì´ì§€ í’€ í˜ì´ì§€ ìƒíƒœ ìœ íš¨")
    #                 return page
    #             except Exception as e:
    #                 log(f"    âš ï¸ í˜ì´ì§€ í’€ í˜ì´ì§€ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {str(e)} - ìƒˆ í˜ì´ì§€ ìƒì„±")
    #                 try:
    #                     await page.close()
    #                 except:
    #                     pass
    #                 new_page = await self.context.new_page()
    #                 log(f"    âœ… ìƒˆ í˜ì´ì§€ ìƒì„± ì™„ë£Œ")
    #                 return new_page
    #         else:
    #             log(f"    ğŸ“„ í˜ì´ì§€ í’€ì´ ë¹„ì–´ìˆìŒ - ìƒˆ í˜ì´ì§€ ìƒì„±")
    #             new_page = await self.context.new_page()
    #             log(f"    âœ… ìƒˆ í˜ì´ì§€ ìƒì„± ì™„ë£Œ")
    #             return new_page

    async def _return_page_to_pool(self, page):
        """í˜ì´ì§€ë¥¼ í’€ì— ë°˜í™˜ - category_extractor_optimized.pyì™€ ë™ì¼í•œ ë°©ì‹"""
        async with self.page_pool_semaphore:
            if len(self.page_pool) < self.max_concurrent_pages:  # ìµœëŒ€ ë™ì‹œ í˜ì´ì§€ ìˆ˜ë¡œ ì œí•œ
                # í˜ì´ì§€ ìƒíƒœ ì´ˆê¸°í™”
                try:
                    await page.goto("about:blank")
                    self.page_pool.append(page)
                    log(f"    ğŸ“„ í˜ì´ì§€ë¥¼ í’€ì— ë°˜í™˜ (í˜„ì¬ í’€ í¬ê¸°: {len(self.page_pool)})")
                except Exception as e:
                    # í˜ì´ì§€ê°€ ì†ìƒëœ ê²½ìš° ë‹«ê¸°
                    log(f"    âš ï¸ í˜ì´ì§€ ìƒíƒœ ì´ˆê¸°í™” ì‹¤íŒ¨, í˜ì´ì§€ ë‹«ê¸°: {str(e)}")
                    try:
                        await page.close()
                    except:
                        pass
            else:
                # í’€ì´ ê°€ë“ ì°¬ ê²½ìš° í˜ì´ì§€ ë‹«ê¸°
                log(f"    ğŸ“„ í˜ì´ì§€ í’€ì´ ê°€ë“ì°¸ ({len(self.page_pool)}/{self.max_concurrent_pages}), í˜ì´ì§€ ë‹«ê¸°")
                try:
                    await page.close()
                except:
                    pass

    async def _check_page_timeout(self, page, operation_name="operation"):
        """í˜ì´ì§€ íƒ€ì„ì•„ì›ƒ ê°ì§€ ë° ê°•ì œ ì¬ìƒì„±"""
        page_id = id(page)
        current_time = time.time()
        
        # ë§ˆì§€ë§‰ í™œë™ ì‹œê°„ í™•ì¸
        if page_id in self.page_last_activity:
            last_activity = self.page_last_activity[page_id]
            time_since_last_activity = current_time - last_activity
            
            if time_since_last_activity > self.page_timeout_threshold:
                log(f"    âš ï¸ í˜ì´ì§€ íƒ€ì„ì•„ì›ƒ ê°ì§€: {operation_name} - {time_since_last_activity:.1f}ì´ˆ ê²½ê³¼, ê°•ì œ ì¬ìƒì„±")
                
                # í˜ì´ì§€ ê°•ì œ ì¢…ë£Œ ë° ìƒˆë¡œ ìƒì„±
                try:
                    await page.close()
                except:
                    pass
                
                # ìƒˆ í˜ì´ì§€ ìƒì„±
                new_page = await self.context.new_page()
                self.page_last_activity[id(new_page)] = current_time
                log(f"    âœ… íƒ€ì„ì•„ì›ƒ í˜ì´ì§€ ê°•ì œ ì¬ìƒì„± ì™„ë£Œ")
                return new_page
        
        # í™œë™ ì‹œê°„ ì—…ë°ì´íŠ¸
        self.page_last_activity[page_id] = current_time
        return page

    async def _safe_page_operation(self, page, operation, *args, **kwargs):
        """ì•ˆì „í•œ í˜ì´ì§€ ì‘ì—… ì‹¤í–‰ (íƒ€ì„ì•„ì›ƒ ê°ì§€ í¬í•¨)"""
        operation_name = operation.__name__ if hasattr(operation, '__name__') else str(operation)
        
        # íƒ€ì„ì•„ì›ƒ ì²´í¬
        page = await self._check_page_timeout(page, operation_name)
        
        try:
            # ì‘ì—… ì‹¤í–‰
            result = await operation(page, *args, **kwargs)
            # í™œë™ ì‹œê°„ ì—…ë°ì´íŠ¸
            self.page_last_activity[id(page)] = time.time()
            return result
        except Exception as e:
            log(f"    âš ï¸ í˜ì´ì§€ ì‘ì—… ì‹¤íŒ¨: {operation_name} - {str(e)}")
            
            # íƒ€ì„ì•„ì›ƒ ì²´í¬ (ì‘ì—… ì‹¤íŒ¨ ì‹œì—ë„)
            page = await self._check_page_timeout(page, f"failed_{operation_name}")
            raise e
    #             try:
    #                 await page.close()
    #             except:
    #                 pass

    # async def _cleanup_page_pool(self):
    #     """í˜ì´ì§€ í’€ ì •ë¦¬"""
    #     while self.page_pool:
    #         page = self.page_pool.pop()
    #         try:
    #             await page.close()
    #         except:
    #             pass

    async def _login(self):
        """ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ì´ë™í•˜ì—¬ ë¡œê·¸ì¸ ìˆ˜í–‰ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                log(f"ë¡œê·¸ì¸ ì‹œë„ {attempt + 1}/{max_retries}")
                await self.page.goto(f"{self.base_url}/www/member/login.asp")
                await self.page.wait_for_load_state('networkidle', timeout=45000)
                await asyncio.sleep(2)

                # ì´ë¯¸ ë¡œê·¸ì¸ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ (ë” ì •í™•í•œ ê²€ì¦)
                if "login.asp" not in self.page.url:
                    # ë¡œê·¸ì¸ ìƒíƒœë¥¼ ë” ì •í™•íˆ í™•ì¸í•˜ê¸° ìœ„í•´ ë¡œê·¸ì•„ì›ƒ ë²„íŠ¼ì´ë‚˜ ì‚¬ìš©ì ì •ë³´ í™•ì¸
                    try:
                        logout_button = self.page.locator("text=ë¡œê·¸ì•„ì›ƒ")
                        if await logout_button.is_visible(timeout=5000):
                            log("ê¸°ì¡´ ë¡œê·¸ì¸ ì„¸ì…˜ ìœ íš¨ (ë¡œê·¸ì•„ì›ƒ ë²„íŠ¼ í™•ì¸)")
                            await self.context.storage_state(path=self.auth_file)
                            return True
                    except:
                        pass
                    
                    # ë¡œê·¸ì•„ì›ƒ ë²„íŠ¼ì´ ì—†ì–´ë„ URLë¡œ íŒë‹¨
                    log("ê¸°ì¡´ ë¡œê·¸ì¸ ì„¸ì…˜ ìœ íš¨ (URL ê¸°ì¤€)")
                    await self.context.storage_state(path=self.auth_file)
                    return True

                username = os.environ.get("KPI_USERNAME")
                password = os.environ.get("KPI_PASSWORD")

                if not username or not password:
                    raise ValueError(".env.local íŒŒì¼ì— KPI_USERNAMEê³¼ "
                                     "KPI_PASSWORDë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.")

                # GitHub Actions í™˜ê²½ì—ì„œ ë” ì•ˆì •ì ì¸ ë¡œê·¸ì¸ ì²˜ë¦¬
                await self.page.locator("#user_id").clear()
                await self.page.locator("#user_pw").clear()
                await asyncio.sleep(1)
                await self.page.locator("#user_id").fill(username)
                await asyncio.sleep(1)
                await self.page.locator("#user_pw").fill(password)
                await asyncio.sleep(1)
                await self.page.locator("#sendLogin").click()

                # ë¡œê·¸ì¸ ì™„ë£Œ ëŒ€ê¸°ì‹œê°„ ì¦ê°€
                await self.page.wait_for_load_state('networkidle', timeout=45000)
                await asyncio.sleep(3)  # ë¡œê·¸ì¸ í›„ ì¶”ê°€ ëŒ€ê¸°
                
                if "login.asp" not in self.page.url:
                    # ë¡œê·¸ì¸ ì„±ê³µ ì—¬ë¶€ë¥¼ ë” ì •í™•íˆ í™•ì¸
                    try:
                        logout_button = self.page.locator("text=ë¡œê·¸ì•„ì›ƒ")
                        if await logout_button.is_visible(timeout=5000):
                            log("ë¡œê·¸ì¸ ì„±ê³µ (ë¡œê·¸ì•„ì›ƒ ë²„íŠ¼ í™•ì¸)", "SUCCESS")
                            # ì„¸ì…˜ ìƒíƒœ ì €ì¥
                            await self.context.storage_state(path=self.auth_file)
                            return True
                    except:
                        pass
                    
                    # ë¡œê·¸ì•„ì›ƒ ë²„íŠ¼ì´ ì—†ì–´ë„ URLë¡œ íŒë‹¨
                    log("ë¡œê·¸ì¸ ì„±ê³µ (URL ê¸°ì¤€)", "SUCCESS")
                    # ì„¸ì…˜ ìƒíƒœ ì €ì¥
                    await self.context.storage_state(path=self.auth_file)
                    return True
                else:
                    log(f"ë¡œê·¸ì¸ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1})")
                    if attempt < max_retries - 1:
                        await asyncio.sleep(5)
                        if os.path.exists(self.auth_file):
                            os.remove(self.auth_file)

            except Exception as e:
                log(f"ë¡œê·¸ì¸ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}): {str(e)}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(5)

        raise ValueError("ë¡œê·¸ì¸ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

    async def _navigate_to_category(self):
        """ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ë¡œ ì´ë™ ë° ì´ˆê¸° ì„¤ì • (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
        log("ì¢…í•©ë¬¼ê°€ì •ë³´ ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ë¡œ ì´ë™í•©ë‹ˆë‹¤.")
        
        max_retries = 3
        retry_count = 0
        
        while retry_count < max_retries:
            try:
                # íƒ€ì„ì•„ì›ƒì„ 60ì´ˆë¡œ ì¦ê°€
                await self.page.goto(
                    f"{self.base_url}/www/price/category.asp",
                    timeout=60000,  # 60ì´ˆ
                    wait_until="domcontentloaded"  # ë” ë¹ ë¥¸ ë¡œë”© ì™„ë£Œ ì¡°ê±´
                )
                
                # í˜ì´ì§€ ë¡œë”© ì™„ë£Œ ëŒ€ê¸°
                await self.page.wait_for_load_state('networkidle', timeout=60000)
                
                # íŒì—… ë‹«ê¸° (ìš°ì„  ì²˜ë¦¬)
                await self._close_popups()

                # Right Quick ë©”ë‰´ ìˆ¨ê¸°ê¸°
                try:
                    close_button = self.page.locator("#right_quick .q_cl")
                    if await close_button.is_visible():
                        await close_button.click()
                        log("Right Quick ë©”ë‰´ë¥¼ ìˆ¨ê²¼ìŠµë‹ˆë‹¤.")
                except Exception as e:
                    log(f"Right Quick ë©”ë‰´ ìˆ¨ê¸°ê¸° ì‹¤íŒ¨ "
                        f"(ì´ë¯¸ ìˆ¨ê²¨ì ¸ ìˆì„ ìˆ˜ ìˆìŒ): {e}")
                
                log("ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ ì´ë™ ì™„ë£Œ", "SUCCESS")
                return  # ì„±ê³µ ì‹œ í•¨ìˆ˜ ì¢…ë£Œ
                
            except Exception as e:
                retry_count += 1
                log(f"ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ ì´ë™ ì‹¤íŒ¨ (ì‹œë„ {retry_count}/{max_retries}): {e}", "WARNING")
                
                if retry_count < max_retries:
                    wait_time = retry_count * 5  # 5ì´ˆ, 10ì´ˆ, 15ì´ˆ ëŒ€ê¸°
                    log(f"{wait_time}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...", "INFO")
                    await asyncio.sleep(wait_time)
                else:
                    log("ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ ì´ë™ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼", "ERROR")
                    raise e

    async def _close_popups(self):
        """í˜ì´ì§€ì˜ ëª¨ë“  íŒì—…ì„ ë‹«ëŠ” ë©”ì„œë“œ"""
        try:
            # 1. ì¼ë°˜ì ì¸ íŒì—… ë‹«ê¸° ë²„íŠ¼ë“¤ ì‹œë„
            popup_close_selectors = [
                ".pop-btn-close",  # ì¼ë°˜ì ì¸ íŒì—… ë‹«ê¸° ë²„íŠ¼
                ".btnClosepop",    # íŠ¹ì • íŒì—… ë‹«ê¸° ë²„íŠ¼
                "#popupNotice .pop-btn-close",  # ê³µì§€ì‚¬í•­ íŒì—… ë‹«ê¸°
                ".ui-popup .pop-btn-close",     # UI íŒì—… ë‹«ê¸°
                "button[class*='close']",       # closeê°€ í¬í•¨ëœ ë²„íŠ¼
                "a[class*='close']"             # closeê°€ í¬í•¨ëœ ë§í¬
            ]
            
            for selector in popup_close_selectors:
                try:
                    popup_close = self.page.locator(selector)
                    if await popup_close.count() > 0:
                        # ëª¨ë“  ë§¤ì¹­ë˜ëŠ” ìš”ì†Œì— ëŒ€í•´ ë‹«ê¸° ì‹œë„
                        for i in range(await popup_close.count()):
                            element = popup_close.nth(i)
                            if await element.is_visible():
                                await element.click(timeout=3000)
                                log(f"íŒì—… ë‹«ê¸° ì„±ê³µ: {selector}")
                                await self.page.wait_for_timeout(500)  # íŒì—…ì´ ë‹«í ì‹œê°„ ëŒ€ê¸°
                except Exception:
                    continue  # ë‹¤ìŒ ì…€ë ‰í„° ì‹œë„
            
            # 2. ESC í‚¤ë¡œ íŒì—… ë‹«ê¸° ì‹œë„
            await self.page.keyboard.press('Escape')
            await self.page.wait_for_timeout(500)
            
            log("íŒì—… ë‹«ê¸° ì²˜ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            log(f"íŒì—… ë‹«ê¸° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    async def _crawl_categories(self):
        """ëŒ€ë¶„ë¥˜ -> ì¤‘ë¶„ë¥˜ -> ì†Œë¶„ë¥˜ ìˆœì°¨ì ìœ¼ë¡œ í¬ë¡¤ë§"""
        major_selector = '#left_menu_kpi > ul.panel'
        major_categories = await self.page.locator(
            major_selector).first.locator('li.file-item > a').all()

        major_links = []
        for cat in major_categories:
            text = await cat.inner_text()
            href = await cat.get_attribute('href')
            major_links.append({'name': text, 'url': f"{self.base_url}{href}"})

        for major in major_links:
            # í¬ë¡¤ë§ ëª¨ë“œì— ë”°ë¥¸ ëŒ€ë¶„ë¥˜ í•„í„°ë§
            if self.target_major_category:
                # íƒ€ê²Ÿ ëŒ€ë¶„ë¥˜ê°€ ì§€ì •ëœ ê²½ìš°, í•´ë‹¹ ëŒ€ë¶„ë¥˜ë§Œ ì²˜ë¦¬
                if major['name'] != self.target_major_category:
                    continue  # íƒ€ê²Ÿ ëŒ€ë¶„ë¥˜ê°€ ì•„ë‹ˆë©´ ê±´ë„ˆë›°ê¸°

            log(f"ëŒ€ë¶„ë¥˜ '{major['name']}' í¬ë¡¤ë§ ì‹œì‘...")
            await self.page.goto(major['url'])
            
            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° - ê¸°ë³¸ ëŒ€ê¸° ì‹œê°„ë§Œ ì ìš© (ì„œë²„ í™˜ê²½ ê³ ë ¤í•˜ì—¬ 10ì´ˆ)
            log("í˜ì´ì§€ ë¡œë”©ì„ ìœ„í•œ ê¸°ë³¸ ëŒ€ê¸° ì‹œê°„ ì ìš© (10ì´ˆ)")
            await self.page.wait_for_timeout(10000)
            
            # ì£¼ì„ì²˜ë¦¬: ë¶ˆí•„ìš”í•œ ì„ íƒì ì‹œë„ ë¶€ë¶„ (ê³„ì† ì‹¤íŒ¨í•˜ì—¬ ì‹œê°„ë§Œ ì†Œëª¨)
            # try:
            #     # ì¹´í…Œê³ ë¦¬ ëª©ë¡ ì»¨í…Œì´ë„ˆ ëŒ€ê¸° (ì—¬ëŸ¬ ì„ íƒì ì‹œë„)
            #     selectors_to_try = [
            #         ".part-list",  # ê¸°ì¡´ ì„ íƒì
            #         "ul li a[href*='CATE_CD=']",  # ì¹´í…Œê³ ë¦¬ ë§í¬ë“¤
            #         "li a[href*='/www/price/category.asp']",  # ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ ë§í¬ë“¤
            #         ".category-list",  # ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬ ë¦¬ìŠ¤íŠ¸ í´ë˜ìŠ¤
            #     ]
            #     
            #     page_loaded = False
            #     for selector in selectors_to_try:
            #         try:
            #             await self.page.wait_for_selector(selector, timeout=10000)
            #             log(f"í˜ì´ì§€ ë¡œë”© ì™„ë£Œ - ì„ íƒì: {selector}")
            #             page_loaded = True
            #             break
            #         except Exception as e:
            #             log(f"ì„ íƒì {selector} ëŒ€ê¸° ì‹¤íŒ¨: {str(e)}")
            #             continue
            #     
            #     if not page_loaded:
            #         log("ëª¨ë“  ì„ íƒì ì‹œë„ ì‹¤íŒ¨, ê¸°ë³¸ ëŒ€ê¸° ì‹œê°„ ì ìš©")
            #         await self.page.wait_for_timeout(3000)
            #         
            # except Exception as e:
            #     log(f"í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘ ì˜¤ë¥˜: {str(e)}")
            #     await self.page.wait_for_timeout(3000)

            # openSub() ë²„íŠ¼ í´ë¦­í•˜ì—¬ ëª¨ë“  ì¤‘ë¶„ë¥˜ì™€ ì†Œë¶„ë¥˜ë¥¼ í•œë²ˆì— í¼ì¹˜ê¸°
            open_sub_selector = 'a[href="javascript:openSub();"]'
            open_sub_button = self.page.locator(open_sub_selector)
            if await open_sub_button.count() > 0:
                log("openSub() ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ëª¨ë“  ë¶„ë¥˜ë¥¼ í¼ì¹©ë‹ˆë‹¤.")
                await open_sub_button.click()
                # ë¶„ë¥˜ê°€ í¼ì³ì§ˆ ì‹œê°„ì„ ê¸°ë‹¤ë¦¼ (2ì´ˆ -> 5ì´ˆë¡œ ì¦ê°€)
                await self.page.wait_for_timeout(5000) # ë” ë„‰ë„‰í•œ ëŒ€ê¸° ì‹œê°„
                # ë˜ëŠ”, íŠ¹ì • ì¤‘ë¶„ë¥˜ ëª©ë¡ì´ ì‹¤ì œë¡œ visible í•´ì§ˆ ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ê¸° (ë” ê²¬ê³ í•œ ë°©ë²•)
                try:
                    await self.page.wait_for_selector('.part-ttl > a', state='visible', timeout=15000)
                    log("ì¤‘ë¶„ë¥˜ ìš”ì†Œë“¤ì´ í™”ë©´ì— ì™„ì „íˆ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
                except Exception as e:
                    log(f"ì¤‘ë¶„ë¥˜ ìš”ì†Œ ê°€ì‹œì„± ëŒ€ê¸° ì‹¤íŒ¨: {e}", "WARNING")
                    # ì‹¤íŒ¨í•´ë„ ì¼ë‹¨ ì§„í–‰í•˜ë„ë¡ í•¨. ë‹¤ìŒ ë¡œì§ì—ì„œ ë‹¤ì‹œ ìš”ì†Œë¥¼ ì°¾ì„ ê²ƒì´ë¯€ë¡œ.

                # HTML êµ¬ì¡° í™•ì¸ì„ ìœ„í•´ í˜ì´ì§€ ë‚´ìš© ì¶œë ¥
                html_content = await self.page.content()
                # HTML êµ¬ì¡° í™•ì¸ì„ ìœ„í•´ í˜ì´ì§€ ë‚´ìš© ì¶œë ¥
                part_ttl_start = html_content.find('part-ttl')
                if 'part-ttl' in html_content:
                    sample_end = part_ttl_start + 1000
                    html_sample = html_content[part_ttl_start:sample_end]
                    log(f"í˜ì´ì§€ HTML ìƒ˜í”Œ (part-ttl ê´€ë ¨): {html_sample}")
                else:
                    log("í˜ì´ì§€ HTML ìƒ˜í”Œ (part-ttl ê´€ë ¨): "
                        "part-ttl ì—†ìŒ")
            else:
                # ëŒ€ë¶„ë¥˜ 'ê³µí†µìì¬' í´ë¦­í•˜ì—¬ ì¤‘ë¶„ë¥˜ ëª©ë¡ í¼ì¹˜ê¸°
                # ì´ ë¶€ë¶„ë„ ìœ„ì— openSub() ì²˜ëŸ¼ ëŒ€ê¸° ì‹œê°„ì„ ëŠ˜ë ¤ì£¼ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
                category_link = 'a[href="category.asp?CATE_CD=101"]'
                await self.page.click(category_link)
                await self.page.wait_for_timeout(3000) # 1ì´ˆ -> 3ì´ˆë¡œ ì¦ê°€
                log("ì¤‘ë¶„ë¥˜ ë° ì†Œë¶„ë¥˜ ëª©ë¡ì„ í¼ì³¤ìŠµë‹ˆë‹¤.")

            # ì†Œë¶„ë¥˜ ëª©ë¡ì´ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ëŒ€ê¸°
            # ì´ wait_for_selectorëŠ” ì‚¬ì‹¤ìƒ .part-ttl ì•„ë˜ì˜ ì¤‘ë¶„ë¥˜ë¥¼ ëŒ€ê¸°í•˜ëŠ” ê²ƒì´ë¯€ë¡œ,
            # .part-ttlì´ visible ìƒíƒœê°€ ë˜ëŠ” ê²ƒì„ ê¸°ë‹¤ë¦¬ëŠ” ê²ƒì´ ë” ì •í™•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            await self.page.wait_for_selector(".part-list", timeout=15000) # íƒ€ì„ì•„ì›ƒ ì¦ê°€

            # ì¤‘ë¶„ë¥˜ ì •ë³´ë¥¼ ë¯¸ë¦¬ ìˆ˜ì§‘
            middle_selector = '.part-ttl > a'
            middle_categories_elements = await self.page.locator(
                middle_selector).all()
            log(f"  ë°œê²¬ëœ ì¤‘ë¶„ë¥˜ ê°œìˆ˜: {len(middle_categories_elements)}")

            middle_categories_info = []
            for i, middle_element in enumerate(middle_categories_elements):
                try:
                    parent_li = await middle_element.locator('..').get_attribute('title')
                    middle_name = parent_li if parent_li else await middle_element.inner_text()
                    middle_href = await middle_element.get_attribute('href')
                    if middle_href and 'CATE_CD=' in middle_href:
                        middle_categories_info.append({
                            'name': middle_name,
                            'href': middle_href
                        })
                        log(f"  ë°œê²¬ëœ ì¤‘ë¶„ë¥˜: '{middle_name}' "
                            f"(CATE_CD: {middle_href})")
                except Exception as e:
                    log(f"  ì¤‘ë¶„ë¥˜ {i + 1} ì •ë³´ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    continue

            # ê° ì¤‘ë¶„ë¥˜ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ë°©ë¬¸í•˜ì—¬ ì†Œë¶„ë¥˜ ìˆ˜ì§‘
            for middle_info in middle_categories_info:
                middle_name = middle_info['name']
                middle_href = middle_info['href']
                
                # í¬ë¡¤ë§ ëª¨ë“œì— ë”°ë¥¸ ì¤‘ë¶„ë¥˜ í•„í„°ë§
                if self.crawl_mode in ["middle_only", "sub_only"] and self.target_middle_category:
                    if middle_name != self.target_middle_category:
                        log(f"  [SKIP] íƒ€ê²Ÿ ì¤‘ë¶„ë¥˜ê°€ ì•„ë‹˜: '{middle_name}' ê±´ë„ˆëœë‹ˆë‹¤.")
                        continue
                    else:
                        log(f"  ì¤‘ë¶„ë¥˜ '{middle_name}' í¬í•¨ (íƒ€ê²Ÿ ì¤‘ë¶„ë¥˜ ì¼ì¹˜)")
                elif self.crawl_mode == "major_only":
                    # major_only ëª¨ë“œì—ì„œëŠ” ëª¨ë“  ì¤‘ë¶„ë¥˜ ì²˜ë¦¬
                    pass
                elif self.crawl_mode == "all":
                    # ê¸°ì¡´ INCLUSION_LIST ë¡œì§ (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)
                    inclusions_for_major = INCLUSION_LIST.get(major['name'], {})
                    
                    # ëŒ€ë¶„ë¥˜ì— ì„¤ì •ì´ ì—†ìœ¼ë©´ ëª¨ë“  ì¤‘ë¶„ë¥˜ ì œì™¸
                    if not inclusions_for_major:
                        log(f"  [SKIP] í¬í•¨ ëª©ë¡ ì—†ìŒ: ì¤‘ë¶„ë¥˜ '{middle_name}' ê±´ë„ˆëœë‹ˆë‹¤.")
                        continue
                    
                    # ëŒ€ë¶„ë¥˜ê°€ "__ALL__"ë¡œ ì„¤ì •ëœ ê²½ìš° ëª¨ë“  ì¤‘ë¶„ë¥˜ í¬í•¨
                    if inclusions_for_major == "__ALL__":
                        log(f"  ì¤‘ë¶„ë¥˜ '{middle_name}' í¬í•¨ (ëŒ€ë¶„ë¥˜ ì „ì²´ í¬í•¨ ì„¤ì •)")
                    else:
                        # ì¤‘ë¶„ë¥˜ê°€ í¬í•¨ ëª©ë¡ì— ì—†ìœ¼ë©´ ì œì™¸
                        if middle_name not in inclusions_for_major:
                            log(f"  [SKIP] í¬í•¨ ëª©ë¡ì— ì—†ìŒ: ì¤‘ë¶„ë¥˜ '{middle_name}' ê±´ë„ˆëœë‹ˆë‹¤.")
                            continue

                try:
                    # ì¤‘ë¶„ë¥˜ í˜ì´ì§€ë¡œ ì´ë™
                    middle_url = f"{self.base_url}/www/price/{middle_href}"
                    log(f"  ì¤‘ë¶„ë¥˜ '{middle_name}' "
                        f"í˜ì´ì§€ë¡œ ì´ë™: {middle_url}")
                    await self.page.goto(middle_url)
                    await self.page.wait_for_load_state('networkidle')
                    
                    # ì¤‘ë¶„ë¥˜ í˜ì´ì§€ ì•ˆì •í™”ë¥¼ ìœ„í•œ ì¶”ê°€ ëŒ€ê¸°
                    await asyncio.sleep(4)  # 3ì´ˆì—ì„œ 4ì´ˆë¡œ ì¦ê°€
                    
                    # í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ í™•ì¸
                    await self.page.wait_for_load_state('domcontentloaded')
                    await asyncio.sleep(3)  # 2ì´ˆì—ì„œ 3ì´ˆë¡œ ì¦ê°€

                    # ì†Œë¶„ë¥˜ê°€ ìˆ¨ê²¨ì ¸ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì§ì ‘ ì°¾ê¸°
                    await self.page.wait_for_timeout(5000)  # 3ì´ˆì—ì„œ 5ì´ˆë¡œ ì¦ê°€í•˜ì—¬ ì•ˆì •ì„± ê°œì„ 

                    # ì†Œë¶„ë¥˜ í¬ë¡¤ë§ (detail.asp?CATE_CD= ë§í¬ë§Œ ì‚¬ìš©)
                    sub_categories_info = []
                    
                    try:
                        detail_selector = 'a[href*="detail.asp?CATE_CD="]'
                        
                        # ì†Œë¶„ë¥˜ ë§í¬ê°€ ë¡œë“œë  ë•Œê¹Œì§€ ì¬ì‹œë„
                        for attempt in range(3):
                            try:
                                await self.page.wait_for_selector(detail_selector, timeout=10000)
                                break
                            except Exception as e:
                                if attempt == 2:
                                    log(f"  ì†Œë¶„ë¥˜ ë§í¬ ëŒ€ê¸° ì‹¤íŒ¨: {str(e)}")
                                    # ë””ë²„ê¹…ì„ ìœ„í•œ í˜ì´ì§€ ìŠ¤í¬ë¦°ìƒ· ìº¡ì²˜
                                    try:
                                        await self.page.screenshot(path=f"debug_middle_{middle_name}_attempt_{attempt}.png", full_page=True)
                                        log(f"  ë””ë²„ê¹… ìŠ¤í¬ë¦°ìƒ· ì €ì¥: debug_middle_{middle_name}_attempt_{attempt}.png")
                                    except:
                                        pass
                                    raise e
                                log(f"  ì†Œë¶„ë¥˜ ë§í¬ ëŒ€ê¸° ì¬ì‹œë„ {attempt + 1}/3")
                                await asyncio.sleep(2)
                        
                        # í˜ì´ì§€ HTMLì„ í™•ì¸í•˜ì—¬ ë””ë²„ê¹…
                        page_html = await self.page.content()
                        log(f"  í˜ì´ì§€ HTML ê¸¸ì´: {len(page_html)} characters")
                        
                        # subcate-up div ë‚´ìš© í™•ì¸
                        try:
                            subcate_up_div = await self.page.locator('div.subcate-up').inner_html()
                            log(f"  subcate-up div ë‚´ìš© ê¸¸ì´: {len(subcate_up_div)} characters")
                            log(f"  subcate-up div ë‚´ìš©: {subcate_up_div[:200]}...")
                        except Exception:
                            log("  subcate-up divë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                        
                        all_links = await self.page.locator(detail_selector).all()
                        log(f"    ë°œê²¬ëœ ì†Œë¶„ë¥˜ ë§í¬ ê°œìˆ˜: {len(all_links)}")
                        
                        for link in all_links:
                            try:
                                parent_li = link.locator('xpath=..')
                                sub_name = await parent_li.get_attribute('title')
                                sub_href = await link.get_attribute('href')
                                
                                if sub_href and sub_name and sub_name.strip():
                                    # category_extractor_optimized.pyì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì½”ë“œ ì¶”ì¶œ
                                    match = re.search(r'CATE_CD=([^&]+)', sub_href)
                                    if match:
                                        cate_cd = match.group(1)
                                        sub_categories_info.append({
                                            'name': sub_name.strip(),
                                            'code': cate_cd,
                                            'href': sub_href
                                        })
                                        log(f"    ë°œê²¬ëœ ì†Œë¶„ë¥˜: '{sub_name.strip()}'")
                                        log(f"    ë§í¬: {sub_href}")
                            except Exception as e:
                                log(f"    ë§í¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                                continue
                    except Exception as e:
                        log(f"  ì†Œë¶„ë¥˜ ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)}")

                    if not sub_categories_info:
                        log(f"    ì¤‘ë¶„ë¥˜ '{middle_name}'ì˜ "
                            f"ì†Œë¶„ë¥˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        continue

                    sub_count = len(sub_categories_info)
                    log(f"    ì¤‘ë¶„ë¥˜ '{middle_name}' - "
                        f"ë°œê²¬ëœ ì†Œë¶„ë¥˜ ê°œìˆ˜: {sub_count}")

                    # ìˆ˜ì§‘ëœ ì†Œë¶„ë¥˜ ì •ë³´ë¡œ ë³‘ë ¬ ë°ì´í„° í¬ë¡¤ë§
                    await self._crawl_subcategories_parallel(
                        major['name'], middle_name, sub_categories_info)

                except Exception as e:
                    log(f"  ì¤‘ë¶„ë¥˜ '{middle_name}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    # ì¤‘ë¶„ë¥˜ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ í˜ì´ì§€ ìƒíƒœ ë³µêµ¬ ì‹œë„
                    try:
                        await self.page.reload()
                        await self.page.wait_for_load_state('networkidle')
                        await asyncio.sleep(2)
                    except Exception as recovery_error:
                        log(f"  í˜ì´ì§€ ë³µêµ¬ ì‹¤íŒ¨: {str(recovery_error)}")
                    continue

    async def _crawl_subcategories_parallel(self, major_name,
                                            middle_name,
                                            sub_categories_info):
        """ì†Œë¶„ë¥˜ë“¤ì„ ë³‘ë ¬ë¡œ í¬ë¡¤ë§"""
        
        # í¬ë¡¤ë§ ëª¨ë“œì— ë”°ë¥¸ ì†Œë¶„ë¥˜ í•„í„°ë§
        if self.crawl_mode == "sub_only" and self.target_sub_category:
            filtered_subs = []
            log(f"    [DEBUG] íƒ€ê²Ÿ ì†Œë¶„ë¥˜: '{self.target_sub_category}' (ê¸¸ì´: {len(self.target_sub_category)})")
            log(f"    [DEBUG] íƒ€ê²Ÿ ì†Œë¶„ë¥˜ ë°”ì´íŠ¸: {self.target_sub_category.encode('utf-8')}")
            
            # ìœ ë‹ˆì½”ë“œ ì •ê·œí™” import
            import unicodedata
            import re
            normalized_target = unicodedata.normalize('NFKC', self.target_sub_category)
            log(f"    [DEBUG] íƒ€ê²Ÿ ì†Œë¶„ë¥˜ ì •ê·œí™”: '{normalized_target}'")
            
            for sub_info in sub_categories_info:
                web_name = sub_info['name']
                normalized_web = unicodedata.normalize('NFKC', web_name)
                
                log(f"    [DEBUG] ì›¹ì‚¬ì´íŠ¸ ì†Œë¶„ë¥˜: '{web_name}' -> ì •ê·œí™”: '{normalized_web}'")
                log(f"    [DEBUG] íƒ€ê²Ÿ ì†Œë¶„ë¥˜ ì •ê·œí™” ê°’: '{normalized_target}'")
                log(f"    [DEBUG] ì›¹ì‚¬ì´íŠ¸ ì†Œë¶„ë¥˜ ì •ê·œí™” ê°’: '{normalized_web}'")
                log(f"    [DEBUG] ì •ê·œí™”ëœ ë¬¸ìì—´ ë¹„êµ ê²°ê³¼: {normalized_web == normalized_target}")
                
                # ì†Œë¶„ë¥˜ ì´ë¦„ì— ê´„í˜¸ ì•ˆì˜ ë‚´ìš©ì´ ë‹¤ë¥¸ ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬, ê´„í˜¸ ì•ˆì˜ ë‚´ìš©ì„ ì œì™¸í•˜ê³  ë¹„êµ
                # ì˜ˆ: 'ìŠ¤í…Œì¸ë¦¬ìŠ¤ë¬¼íƒ±í¬(1)-1'ê³¼ 'ìŠ¤í…Œì¸ë¦¬ìŠ¤ë¬¼íƒ±í¬(2)-1'ì„ êµ¬ë¶„í•˜ê¸° ìœ„í•¨
                web_name_without_parentheses = re.sub(r'\([^)]*\)', '', normalized_web).strip()
                target_name_without_parentheses = re.sub(r'\([^)]*\)', '', normalized_target).strip()

                # íƒ€ê²Ÿ ì†Œë¶„ë¥˜ì— ê´„í˜¸ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                if '(' in self.target_sub_category and ')' in self.target_sub_category:
                    # ê´„í˜¸ê°€ í¬í•¨ëœ ê²½ìš°, ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²½ìš°ë§Œ í—ˆìš©
                    if normalized_web == normalized_target:
                        log(f"    [MATCH] ì†Œë¶„ë¥˜ ë§¤ì¹­ ì„±ê³µ (ì •í™• ì¼ì¹˜): '{web_name}'")
                        filtered_subs.append(sub_info)
                    else:
                        log(f"    [SKIP] íƒ€ê²Ÿ ì†Œë¶„ë¥˜ê°€ ì•„ë‹˜ (ê´„í˜¸ í¬í•¨): '{web_name}' ê±´ë„ˆëœë‹ˆë‹¤.")
                        sub_info['skip_reason'] = "íƒ€ê²Ÿ ì†Œë¶„ë¥˜ê°€ ì•„ë‹˜"
                else:
                    # ê´„í˜¸ê°€ ì—†ëŠ” ê²½ìš°, ê´„í˜¸ ì œì™¸ ë¹„êµë„ í—ˆìš©
                    if (normalized_web == normalized_target or 
                        web_name_without_parentheses == target_name_without_parentheses):
                        log(f"    [MATCH] ì†Œë¶„ë¥˜ ë§¤ì¹­ ì„±ê³µ (ìœ ì—° ì¼ì¹˜): '{web_name}'")
                        filtered_subs.append(sub_info)
                    else:
                        log(f"    [SKIP] íƒ€ê²Ÿ ì†Œë¶„ë¥˜ê°€ ì•„ë‹˜ (ìœ ì—° ì¼ì¹˜): '{web_name}' ê±´ë„ˆëœë‹ˆë‹¤.")
                        sub_info['skip_reason'] = "íƒ€ê²Ÿ ì†Œë¶„ë¥˜ê°€ ì•„ë‹˜"
            sub_categories_info = filtered_subs
        elif self.crawl_mode in ["major_only", "middle_only"]:
            # major_only, middle_only ëª¨ë“œì—ì„œëŠ” ëª¨ë“  ì†Œë¶„ë¥˜ ì²˜ë¦¬
            pass
        elif self.crawl_mode == "all":
            # ìºì‹±ëœ JSONC ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì„ ì²˜ë¦¬ë˜ì§€ ì•Šì€ í•­ëª©ë§Œ ë¯¸ë¦¬ í•„í„°ë§
            filtered_subs = []
            
            # ìºì‹±ëœ ë°ì´í„°ì—ì„œ í•´ë‹¹ ì¤‘ë¶„ë¥˜ì˜ í¬í•¨ëœ ì†Œë¶„ë¥˜ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            included_subs = []
            if (major_name in self.included_categories_cache and 
                middle_name in self.included_categories_cache[major_name]):
                included_subs = self.included_categories_cache[major_name][middle_name]
            
            for sub_info in sub_categories_info:
                sub_name = sub_info['name']
                
                # ìºì‹±ëœ ëª©ë¡ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                if sub_name in included_subs:
                    filtered_subs.append(sub_info)
                else:
                    log(f"    [SKIP] JSONCì—ì„œ ì£¼ì„ ì²˜ë¦¬ë¨: ì†Œë¶„ë¥˜ '{sub_name}' ê±´ë„ˆëœë‹ˆë‹¤.")
                    sub_info['skip_reason'] = "JSONCì—ì„œ ì£¼ì„ ì²˜ë¦¬ë¨"
            
            sub_categories_info = filtered_subs
            
            if not sub_categories_info:
                log(f"    ì¤‘ë¶„ë¥˜ '{middle_name}': JSONCì— í¬í•¨ëœ ì†Œë¶„ë¥˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return

        if not sub_categories_info:
            log(f"    ì¤‘ë¶„ë¥˜ '{middle_name}': "
                f"ì²˜ë¦¬í•  ì†Œë¶„ë¥˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        sub_count = len(sub_categories_info)
        log(f"    ì¤‘ë¶„ë¥˜ '{middle_name}': {sub_count}ê°œ "
            f"ì†Œë¶„ë¥˜ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

        # ë³‘ë ¬ ì‘ì—… ìƒì„±
        tasks = []
        for sub_info in sub_categories_info:
            task = self._crawl_single_subcategory(
                major_name, middle_name, sub_info)
            tasks.append(task)

        # ë³‘ë ¬ ì‹¤í–‰
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # ê²°ê³¼ ì²˜ë¦¬ ë° ë°°ì¹˜ ë°ì´í„° ìˆ˜ì§‘
        success_count = 0
        failed_count = 0
        
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                sub_name = sub_categories_info[i]['name']
                log(f"    âŒ ì†Œë¶„ë¥˜ '{sub_name}' ì²˜ë¦¬ ì‹¤íŒ¨: {str(result)}", "ERROR")
                failed_count += 1
            elif result is None:
                sub_name = sub_categories_info[i]['name']
                log(f"    âš ï¸ ì†Œë¶„ë¥˜ '{sub_name}' ì²˜ë¦¬ ê²°ê³¼ ì—†ìŒ", "WARNING")
                failed_count += 1
            else:
                success_count += 1
                # ì„±ê³µí•œ ì†Œë¶„ë¥˜ ë°ì´í„°ë¥¼ ë°°ì¹˜ì— ì¶”ê°€
                sub_info = sub_categories_info[i]
                self.batch_data.append({
                    'major': major_name,
                    'middle': middle_name,
                    'sub': sub_info['name'],
                    'result': result
                })

        log(f"    ì¤‘ë¶„ë¥˜ '{middle_name}' ì™„ë£Œ: {success_count}/{sub_count}ê°œ ì„±ê³µ, {failed_count}ê°œ ì‹¤íŒ¨")
        
        # ë°°ì¹˜ í¬ê¸°ì— ë„ë‹¬í•˜ë©´ ì²˜ë¦¬
        if len(self.batch_data) >= self.batch_size:
            await self._process_batch()

        total_count = len(sub_categories_info)
        log(f"    ì¤‘ë¶„ë¥˜ '{middle_name}' ì™„ë£Œ: "
            f"{success_count}/{total_count}ê°œ ì„±ê³µ")
    
    async def _process_batch(self):
        """ë°°ì¹˜ ë°ì´í„° ì²˜ë¦¬ (pandas ê°€ê³µ ë° supabase ì €ì¥)"""
        if not self.batch_data:
            return
            
        batch_count = len(self.batch_data)
        log(f"\n=== ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {batch_count}ê°œ ì†Œë¶„ë¥˜ ===\n")
        
        # ê° ì†Œë¶„ë¥˜ë³„ë¡œ ë°ì´í„° ì²˜ë¦¬
        total_processed = 0
        total_saved = 0
        
        for batch_item in self.batch_data:
            try:
                # pandas ê°€ê³µ
                processed_data = await self.processor.process_data(
                    batch_item['major'], 
                    batch_item['middle'], 
                    batch_item['sub']
                )
                
                if processed_data:
                    processed_count = len(processed_data)
                    total_processed += processed_count
                    
                    # supabase ì €ì¥
                    saved_count = await self.processor.save_to_supabase(
                        processed_data, force_refresh=self.force_refresh
                    )
                    total_saved += saved_count
                    
                    if saved_count == 0:
                        log(f"  - {batch_item['sub']}: pandas ê°€ê³µ {processed_count}ê°œ, "
                            f"supabase ì €ì¥ 0ê°œ (ëª¨ë‘ ì¤‘ë³µ ë°ì´í„°)")
                    else:
                        log(f"  - {batch_item['sub']}: pandas ê°€ê³µ {processed_count}ê°œ, "
                            f"supabase ì €ì¥ {saved_count}ê°œ")
                else:
                    # ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ëŠ” ì´ìœ ë¥¼ ëª…í™•íˆ êµ¬ë¶„
                    if batch_item.get('skip_reason') == 'not_found':
                        log(f"  - {batch_item['sub']}: ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì†Œë¶„ë¥˜ëª… ë¯¸ë°œê²¬")
                    elif batch_item.get('skip_reason') == 'no_target':
                        log(f"  - {batch_item['sub']}: íƒ€ê²Ÿ ì†Œë¶„ë¥˜ê°€ ì•„ë‹˜ (í•„í„°ë§ë¨)")
                    else:
                        log(f"  - {batch_item['sub']}: ì²˜ë¦¬í•  ë°ì´í„° ì—†ìŒ (ì›ì¸ ë¯¸ìƒ)")
                    
            except Exception as e:
                log(f"  - {batch_item['sub']} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        
        log(f"\n=== ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: pandas ê°€ê³µ {total_processed}ê°œ, "
            f"supabase ì €ì¥ {total_saved}ê°œ ===\n")
        
        # ë°°ì¹˜ ë°ì´í„° ì´ˆê¸°í™”
        self.batch_data = []
        self.processed_count += batch_count
    
    async def _process_final_batch(self):
        """ë§ˆì§€ë§‰ ë‚¨ì€ ë°°ì¹˜ ë°ì´í„° ì²˜ë¦¬"""
        if self.batch_data:
            log(f"\n=== ìµœì¢… ë°°ì¹˜ ì²˜ë¦¬: {len(self.batch_data)}ê°œ ì†Œë¶„ë¥˜ ===\n")
            await self._process_batch()

    async def _crawl_single_subcategory(self, major_name,
                                        middle_name, sub_info):
        """ë‹¨ì¼ ì†Œë¶„ë¥˜ í¬ë¡¤ë§ (ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì œí•œ)"""
        async with self.page_semaphore:  # í˜ì´ì§€ ì„¸ë§ˆí¬ì–´ ì‚¬ìš©
            sub_name = sub_info['name']
            sub_code = sub_info.get('code', '')
            
            # category_extractor_optimized.pyì™€ ë™ì¼í•œ URL êµ¬ì„± ë°©ì‹
            sub_url = f"https://www.kpi.or.kr/www/price/detail.asp?CATE_CD={sub_code}"
            if len(sub_code) >= 4:
                item_cd = sub_code[-4:]
                sub_url += f"&ITEM_CD={item_cd}"

            log(f"  - ì¤‘ë¶„ë¥˜ '{middle_name}' > "
                f"ì†Œë¶„ë¥˜ '{sub_name}' ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")

            # category_extractor_optimized.py ë°©ì‹: ê°„ë‹¨í•˜ê³  í™•ì‹¤í•œ í˜ì´ì§€ ê´€ë¦¬
            new_page = await self.context.new_page()
            log(f"    ìƒˆ í˜ì´ì§€ ìƒì„±: {sub_name}")
            
            try:
                # í˜ì´ì§€ ë¡œë“œ (ì•ˆì „í•œ ì‘ì—…ìœ¼ë¡œ ë˜í•‘)
                async def load_page(page):
                    await page.goto(sub_url, timeout=60000)
                    await page.wait_for_load_state('networkidle', timeout=45000)
                
                await self._safe_page_operation(new_page, load_page)
                
                # "ë¬¼ê°€ì¶”ì´ ë³´ê¸°" íƒ­ ì´ë™ ë° ê·œê²©ëª… ì¶”ì¶œ (ê°œì„ ëœ ì¬ì‹œë„ ë¡œì§)
                for retry in range(5):
                    try:
                        # ë‹¤ì–‘í•œ ì…€ë ‰í„°ë¡œ íƒ­ ì°¾ê¸° ì‹œë„ (ì‚¬ìš©ì ì œê³µ HTML êµ¬ì¡° ë°˜ì˜)
                        selectors = [
                            'a[href*="detail_change.asp"] span:has-text("ë¬¼ê°€ì¶”ì´ ë³´ê¸°")',
                            'a[href*="detail_change.asp"]',
                            'span:has-text("ë¬¼ê°€ì¶”ì´ ë³´ê¸°")',
                            'a:has(span:has-text("ë¬¼ê°€ì¶”ì´ ë³´ê¸°"))',
                            'a:has-text("ë¬¼ê°€ì¶”ì´ ë³´ê¸°")',
                            'text=ë¬¼ê°€ì¶”ì´ ë³´ê¸°',
                            'a[href*="price_trend"]',
                            'a:contains("ë¬¼ê°€ì¶”ì´")',
                            '.tab-menu a:has-text("ë¬¼ê°€ì¶”ì´")',
                            'ul.tab-list a:has-text("ë¬¼ê°€ì¶”ì´")'
                        ]
                        
                        clicked = False
                        for selector in selectors:
                            try:
                                async def wait_and_click(page):
                                    await page.wait_for_selector(selector, timeout=15000)
                                    await page.click(selector, timeout=45000)
                                await self._safe_page_operation(new_page, wait_and_click)
                                clicked = True
                                break
                            except:
                                continue
                        
                        if not clicked:
                            raise Exception("ë¬¼ê°€ì¶”ì´ ë³´ê¸° íƒ­ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                        
                        async def wait_for_selt_list(page):
                            await page.wait_for_selector(".selt-list", timeout=15000)
                        await self._safe_page_operation(new_page, wait_for_selt_list)
                        break
                    except Exception as e:
                        if retry == 4:
                            log(f"    âŒ ë¬¼ê°€ì¶”ì´ ë³´ê¸° íƒ­ í´ë¦­ ì™„ì „ ì‹¤íŒ¨: {str(e)}")
                            raise e
                        log(f"    âš ï¸ ë¬¼ê°€ì¶”ì´ ë³´ê¸° íƒ­ í´ë¦­ ì¬ì‹œë„ {retry + 1}/5: {str(e)}")
                        await asyncio.sleep(3)
                        await new_page.reload()
                        await new_page.wait_for_load_state('networkidle', timeout=30000)
                
                # ê·œê²©ëª… ì¶”ì¶œ - ìˆ¨ê²¨ì§„ ìš”ì†Œë„ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ìˆ˜ì •
                spec_selector = "#ITEM_SPEC_CD option:first-child"
                try:
                    # ë¨¼ì € ìš”ì†Œê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                    async def wait_for_spec_selector(page):
                        await page.wait_for_selector(spec_selector, timeout=5000, state='attached')
                    await self._safe_page_operation(new_page, wait_for_spec_selector)
                    # ìˆ¨ê²¨ì§„ ìš”ì†Œë„ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë„ë¡ ìˆ˜ì •
                    spec_name = await new_page.evaluate(f'document.querySelector("{spec_selector}").textContent')
                except Exception as e:
                    # ëŒ€ì•ˆ ì…€ë ‰í„° ì‹œë„
                    try:
                        spec_name = await new_page.locator("#ITEM_SPEC_CD option").first.inner_text()
                    except:
                        spec_name = "ê·œê²©ëª… ì¶”ì¶œ ì‹¤íŒ¨"
                        log(f"    ê·œê²©ëª… ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
                
                # "ë¬¼ê°€ì •ë³´ ë³´ê¸°" íƒ­ ì´ë™
                async def click_price_info_tab(page):
                    await page.click("text=ë¬¼ê°€ì •ë³´ ë³´ê¸°")
                    await page.wait_for_selector("table", timeout=5000)
                await self._safe_page_operation(new_page, click_price_info_tab)
                
                # JSONC íŒŒì¼ì—ì„œ ì‹¤ì œ ë‹¨ìœ„ ì¶”ì¶œ
                cate_cd = sub_url.split('CATE_CD=')[1].split('&')[0] if 'CATE_CD=' in sub_url else None
                item_cd = sub_url.split('ITEM_CD=')[1].split('&')[0] if 'ITEM_CD=' in sub_url else None
                
                # í•˜ë“œì½”ë”©ëœ ë‹¨ìœ„ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (JSONC íŒŒì¼ì—ì„œë§Œ)
                unit = self._get_hardcoded_unit(major_name, middle_name, sub_name, spec_name)
                if not unit:
                    log(f"    âŒ JSONC íŒŒì¼ì—ì„œ ë‹¨ìœ„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {major_name} > {middle_name} > {sub_name} > {spec_name}")
                    return None  # ë‹¨ìœ„ê°€ ì—†ìœ¼ë©´ í•´ë‹¹ ê·œê²© ê±´ë„ˆë›°ê¸°
                else:
                    log(f"    âœ… JSONC íŒŒì¼ì—ì„œ ë‹¨ìœ„ ì¶”ì¶œ: {unit}")
                
                # ê°€ê²© ë°ì´í„° ìˆ˜ì§‘
                result = await self._get_price_data_with_page(
                    new_page, major_name, middle_name, sub_name, sub_url, spec_name, unit)

                # ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì¦‰ì‹œ ì²˜ë¦¬í•˜ê³  ì €ì¥
                has_data = (result and hasattr(result, 'raw_data_list')
                            and result.raw_data_list)
                if has_data:
                    log(f"  - ì†Œë¶„ë¥˜ '{sub_name}' ë°ì´í„° ì²˜ë¦¬ ë° ì €ì¥ ì‹œì‘")

                    # DataFrameìœ¼ë¡œ ë³€í™˜
                    df = result.to_dataframe()

                    if not df.empty:
                        # DataFrameì„ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
                        processed_data = df.to_dict(orient='records')
                        # Supabaseì— ì €ì¥ (ì¤‘ë³µ ì²´í¬ ë¹„í™œì„±í™” - í…ŒìŠ¤íŠ¸ìš©)
                        saved_count = await result.save_to_supabase(processed_data, 'kpi_price_data', check_duplicates=False)
                        log(f"  âœ… '{sub_name}' ì™„ë£Œ: "
                            f"{len(df)}ê°œ ë°ì´í„° â†’ Supabase ì €ì¥ {saved_count}ê°œ ì„±ê³µ")
                    else:
                        log(f"  âš ï¸ '{sub_name}' ì™„ë£Œ: ì €ì¥í•  ë°ì´í„° ì—†ìŒ")
                else:
                    log(f"  âš ï¸ '{sub_name}' ì™„ë£Œ: ì²˜ë¦¬í•  ë°ì´í„° ì—†ìŒ")

                return result

            except Exception as e:
                error_msg = (f"  âŒ ì†Œë¶„ë¥˜ '{sub_name}' ì²˜ë¦¬ ì‹¤íŒ¨ "
                             f"[ëŒ€ë¶„ë¥˜: {major_name}, ì¤‘ë¶„ë¥˜: {middle_name}]: {str(e)}")
                log(error_msg, "ERROR")
                raise e
            
            finally:
                # category_extractor_optimized.py ë°©ì‹: finally ë¸”ë¡ì—ì„œ í™•ì‹¤í•œ í˜ì´ì§€ ë‹«ê¸°
                if new_page:
                    try:
                        await new_page.close()
                        log(f"    í˜ì´ì§€ ë‹«ê¸° ì™„ë£Œ: {sub_name}")
                    except Exception as e:
                        log(f"    í˜ì´ì§€ ë‹«ê¸° ì‹¤íŒ¨: {str(e)}")
                    new_page = None

    async def _get_price_data(self, major_name, middle_name,
                             sub_name, sub_url):
        """ê¸°ì¡´ ë©”ì„œë“œ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼"""
        return await self._get_price_data_with_page(
            self.page, major_name, middle_name, sub_name, sub_url)



    async def _check_existing_data(self, major_name, middle_name, 
                                  sub_name, spec_name):
        """Supabaseì—ì„œ ê¸°ì¡´ ë°ì´í„° í™•ì¸í•˜ì—¬ ì¤‘ë³µ ì²´í¬ - 6ê°œ í•„ë“œ ë™ì¼í•  ë•Œ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼ (ê°€ê²© ì œì™¸)"""
        try:
            response = self.supabase.table('kpi_price_data').select(
                'major_category, middle_category, sub_category, specification, region, date'
            ).eq(
                'major_category', major_name
            ).eq(
                'middle_category', middle_name
            ).eq(
                'sub_category', sub_name
            ).eq(
                'specification', spec_name
            ).execute()
            
            if response.data:
                # 6ê°œ í•„ë“œ ë™ì¼í•  ë•Œ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼ (ê°€ê²© ì œì™¸ - ê°€ê²© ë³€ë™ ì‹œ ì—…ë°ì´íŠ¸ í—ˆìš©)
                existing_data = set()
                for item in response.data:
                    existing_data.add((
                        item['major_category'], 
                        item['middle_category'], 
                        item['sub_category'], 
                        item['specification'], 
                        item['region'], 
                        item['date']
                    ))
                log(f"        - ê¸°ì¡´ ë°ì´í„° ë°œê²¬: {len(existing_data)}ê°œ (6ê°œ í•„ë“œ ì¼ì¹˜ ì¡°í•©, ê°€ê²© ë³€ë™ í—ˆìš©)")
                return existing_data
            else:
                log("        - ê¸°ì¡´ ë°ì´í„° ì—†ìŒ: ì „ì²´ ì¶”ì¶œ í•„ìš”")
                return set()
                
        except Exception as e:
            log(f"        - ê¸°ì¡´ ë°ì´í„° í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return set()  # ì˜¤ë¥˜ ì‹œ ì „ì²´ ì¶”ì¶œ
    
    async def _get_available_date_range(self, page):
        """í˜ì´ì§€ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œ ë²”ìœ„ í™•ì¸"""
        try:
            # í—¤ë”ì—ì„œ ë‚ ì§œ ì •ë³´ ì¶”ì¶œ
            selector = "#priceTrendDataArea th"
            await page.wait_for_selector(selector, timeout=5000)
            header_elements = await page.locator(selector).all()

            if len(header_elements) > 1:
                dates = []
                for i in range(1, len(header_elements)):
                    header_text = await header_elements[i].inner_text()
                    dates.append(header_text.strip())
                return dates
            else:
                return []

        except Exception as e:
            log(f"        - ë‚ ì§œ ë²”ìœ„ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return []

    async def _get_price_data_with_page(
        self,
        page: Page,
        major_name: str,
        middle_name: str,
        sub_name: str,
        sub_url: str,
        specification: str,
        unit: str,
    ):
        """ì†Œë¶„ë¥˜ í˜ì´ì§€ì—ì„œ ì›”ë³„ ê°€ê²© ë°ì´í„°ë¥¼ ì¶”ì¶œ"""
        try:
            # í˜ì´ì§€ ìƒíƒœ í™•ì¸
            if page.is_closed():
                log(f"í˜ì´ì§€ê°€ ë‹«í˜€ìˆì–´ '{sub_name}' ì²˜ë¦¬ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.", "ERROR")
                return None
                
            # í˜ì´ì§€ëŠ” ì´ë¯¸ ë¡œë“œëœ ìƒíƒœë¡œ ì „ë‹¬ë¨
            await page.wait_for_load_state('networkidle', timeout=60000)

            # 'ë¬¼ê°€ì¶”ì´ ë³´ê¸°' íƒ­ìœ¼ë¡œ ì´ë™
            try:
                # GitHub Actions í™˜ê²½ì„ ìœ„í•œ ë” ê¸´ ëŒ€ê¸°ì‹œê°„
                await page.wait_for_load_state('networkidle', timeout=60000)
                await asyncio.sleep(3)  # ì¶”ê°€ ì•ˆì •í™” ëŒ€ê¸°
                
                # ë‹¤ì–‘í•œ ì…€ë ‰í„°ë¡œ íƒ­ ì°¾ê¸° ì‹œë„
                selectors = [
                    'a:has-text("ë¬¼ê°€ì¶”ì´ ë³´ê¸°")',
                    'a[href*="price_trend"]',
                    'a:contains("ë¬¼ê°€ì¶”ì´")',
                    '.tab-menu a:has-text("ë¬¼ê°€ì¶”ì´")',
                    'ul.tab-list a:has-text("ë¬¼ê°€ì¶”ì´")',
                    'a[href*="detail_change.asp"]',
                    'a:has-text("ë¬¼ê°€ì¶”ì´")',
                    'a:has-text("ì¶”ì´")',
                    'li:has-text("ë¬¼ê°€ì¶”ì´") a',
                    '.nav-tabs a:has-text("ë¬¼ê°€ì¶”ì´")',
                    'ul li a:has-text("ë¬¼ê°€ì¶”ì´")',
                    'a[title*="ë¬¼ê°€ì¶”ì´"]'
                ]
                
                # í˜ì´ì§€ ì•ˆì •í™” ëŒ€ê¸° ì¶”ê°€
                await page.wait_for_load_state('domcontentloaded', timeout=60000)
                await asyncio.sleep(3)  # í˜ì´ì§€ ì•ˆì •í™” ëŒ€ê¸°
                
                tab_found = False
                for selector in selectors:
                    try:
                        await page.wait_for_selector(selector, timeout=30000)
                        tab_found = True
                        break
                    except:
                        continue
                
                if not tab_found:
                    raise Exception("ë¬¼ê°€ì¶”ì´ ë³´ê¸° íƒ­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                
                # ì¬ì‹œë„ ë¡œì§ ê°œì„  (7íšŒë¡œ ì¡°ì •)
                for retry in range(7):
                    try:
                        # ë‹¤ì–‘í•œ ì…€ë ‰í„°ë¡œ í´ë¦­ ì‹œë„
                        clicked = False
                        for selector in selectors:
                            try:
                                tab_element = page.locator(selector)
                                element_count = await tab_element.count()
                                if element_count > 0:
                                    # ì²« ë²ˆì§¸ ìš”ì†Œ ì„ íƒ
                                    first_element = tab_element.first
                                    await first_element.wait_for(state='visible', timeout=30000)
                                    await first_element.click(timeout=60000)
                                    clicked = True
                                    log(f"    âœ… ë¬¼ê°€ì¶”ì´ ë³´ê¸° íƒ­ í´ë¦­ ì„±ê³µ (ì…€ë ‰í„°: {selector})")
                                    break
                            except Exception as click_error:
                                log(f"    âš ï¸ ì…€ë ‰í„° {selector} í´ë¦­ ì‹¤íŒ¨: {str(click_error)}")
                                continue
                        
                        if not clicked:
                            raise Exception("ëª¨ë“  ì…€ë ‰í„°ë¡œ íƒ­ í´ë¦­ ì‹¤íŒ¨")
                        
                        # í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ ëŒ€ê¸° (ë” ê¸´ íƒ€ì„ì•„ì›ƒ)
                        await page.wait_for_selector("#ITEM_SPEC_CD", timeout=60000)
                        await page.wait_for_load_state('networkidle', timeout=45000)
                        await asyncio.sleep(2)  # ì¶”ê°€ ì•ˆì •í™” ëŒ€ê¸°
                        break
                    except Exception as e:
                        if retry == 6:
                            raise e
                        log(f"    âš ï¸ ë¬¼ê°€ì¶”ì´ ë³´ê¸° íƒ­ í´ë¦­ ì¬ì‹œë„ {retry + 1}/7: {e}", "WARNING")
                        await asyncio.sleep(5)  # ì¬ì‹œë„ ê°„ ëŒ€ê¸°ì‹œê°„
                        
                        # í˜ì´ì§€ ìƒíƒœ ë³µêµ¬
                        try:
                            await page.reload()
                            await page.wait_for_load_state('networkidle', timeout=60000)
                            await asyncio.sleep(3)
                        except Exception as reload_error:
                            log(f"    í˜ì´ì§€ ë¦¬ë¡œë“œ ì‹¤íŒ¨: {str(reload_error)}")
                        
            except Exception as e:
                log(f"ë¬¼ê°€ì¶”ì´ ë³´ê¸° íƒ­ í´ë¦­ ì™„ì „ ì‹¤íŒ¨: {str(e)}", "ERROR")
                # í˜ì´ì§€ ìƒíƒœ í™•ì¸ ë° ë³µêµ¬ ì‹œë„
                try:
                    # í˜ì´ì§€ ìƒíƒœ ê²€ì¦ ê°•í™”
                    current_url = page.url
                    log(f"í˜„ì¬ í˜ì´ì§€ URL: {current_url}", "INFO")
                    
                    # í˜ì´ì§€ê°€ ì˜¬ë°”ë¥¸ ìƒíƒœì¸ì§€ í™•ì¸
                    page_title = await page.title()
                    log(f"í˜„ì¬ í˜ì´ì§€ ì œëª©: {page_title}", "INFO")
                    
                    # í˜ì´ì§€ ë¦¬ë¡œë“œ ë° ìƒíƒœ ë³µêµ¬
                    await page.reload()
                    await page.wait_for_load_state('networkidle', timeout=60000)
                    await asyncio.sleep(5)  # ë” ê¸´ ì•ˆì •í™” ëŒ€ê¸°
                    
                    # í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ ê²€ì¦
                    await page.wait_for_load_state('domcontentloaded', timeout=60000)
                    
                    # ë§ˆì§€ë§‰ ì‹œë„: ë‹¤ì–‘í•œ ëŒ€ì²´ ì…€ë ‰í„°ë¡œ íƒ­ ì°¾ê¸°
                    alternative_selectors = [
                        'a[href*="price_trend"]',
                        'a:has-text("ë¬¼ê°€ì¶”ì´")',
                        'a:has-text("ì¶”ì´")',
                        'a[onclick*="price_trend"]',
                        'li:has-text("ë¬¼ê°€ì¶”ì´") a',
                        '.nav-tabs a:has-text("ë¬¼ê°€ì¶”ì´")',
                        'ul li a:has-text("ë¬¼ê°€ì¶”ì´")',
                        'a[title*="ë¬¼ê°€ì¶”ì´"]'
                    ]
                    
                    tab_clicked = False
                    for selector in alternative_selectors:
                        try:
                            # ìš”ì†Œ ì¡´ì¬ í™•ì¸
                            element_count = await page.locator(selector).count()
                            if element_count > 0:
                                log(f"ëŒ€ì²´ ì…€ë ‰í„° ë°œê²¬: {selector} (ê°œìˆ˜: {element_count})", "INFO")
                                
                                # ìš”ì†Œê°€ ë³´ì´ëŠ”ì§€ í™•ì¸
                                element = page.locator(selector).first
                                await element.wait_for(state='visible', timeout=15000)
                                
                                # í´ë¦­ ì‹œë„
                                await element.click(timeout=30000)
                                
                                # í´ë¦­ í›„ í˜ì´ì§€ ìƒíƒœ í™•ì¸
                                await page.wait_for_selector("#ITEM_SPEC_CD", timeout=45000)
                                await page.wait_for_load_state('networkidle', timeout=60000)
                                
                                log(f"ëŒ€ì²´ ì…€ë ‰í„°ë¡œ íƒ­ í´ë¦­ ì„±ê³µ: {selector}", "INFO")
                                tab_clicked = True
                                break
                        except Exception as selector_error:
                            log(f"ëŒ€ì²´ ì…€ë ‰í„° {selector} ì‹¤íŒ¨: {selector_error}", "DEBUG")
                            continue
                    
                    if not tab_clicked:
                        log(f"ëª¨ë“  ëŒ€ì²´ ë°©ë²• ì‹¤íŒ¨ - ì†Œë¶„ë¥˜ ê±´ë„ˆëœ€: {sub_name}", "WARNING")
                        return None
                        
                except Exception as recovery_error:
                    log(f"í˜ì´ì§€ ë³µêµ¬ ì‹œë„ ì‹¤íŒ¨: {recovery_error}", "ERROR")
                    log(f"ëª¨ë“  ëŒ€ì²´ ë°©ë²• ì‹¤íŒ¨ - ì†Œë¶„ë¥˜ ê±´ë„ˆëœ€: {sub_name}", "WARNING")
                    return None

            # ê·œê²© ì„ íƒ ì˜µì…˜ë“¤ ê°€ì ¸ì˜¤ê¸°
            spec_options = await page.locator('#ITEM_SPEC_CD option').all()
            log(f"    - í˜ì´ì§€ì—ì„œ ê°ì§€ëœ ì´ ê·œê²© ì˜µì…˜ ìˆ˜: {len(spec_options)}", "DEBUG")

            raw_item_data = {
                'major_category_name': major_name,
                'middle_category_name': middle_name,
                'sub_category_name': sub_name,
                'specification': specification,
                'unit': unit,
                'spec_data': []
            }

            # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•´ ê·œê²© ë°ì´í„°ë¥¼ ë¨¼ì € ìˆ˜ì§‘
            spec_list = []
            for i, option in enumerate(spec_options):
                spec_value = await option.get_attribute('value')
                spec_name = await option.inner_text()
                log(f"    - ê·œê²© ì˜µì…˜ {i+1}: value='{spec_value}', name='{spec_name}'", "DEBUG")
                if spec_value and spec_name.strip():  # ë¹ˆ ê°’ ì œì™¸
                    spec_list.append({'value': spec_value, 'name': spec_name})
                    log(f"    - ìœ íš¨í•œ ê·œê²©ìœ¼ë¡œ ì¶”ê°€ë¨: {spec_name} (value: {spec_value})", "DEBUG")
                else:
                    log(f"    - ë¹ˆ ê°’ìœ¼ë¡œ ì œì™¸ë¨: value='{spec_value}', name='{spec_name}'", "DEBUG")

            log(f"    - ìµœì¢… ìœ íš¨ ê·œê²© ëª©ë¡: {[spec['name'] for spec in spec_list]}", "DEBUG")
            log(f"    - ìµœì¢… ìœ íš¨ ê·œê²© ìˆ˜: {len(spec_list)}", "DEBUG")
            # ëª¨ë“  ê·œê²©ì„ ìµœì í™”ëœ ìˆœì°¨ ì²˜ë¦¬ë¡œ ì§„í–‰
            log(f"    - ì´ {len(spec_list)}ê°œ ìœ íš¨í•œ ê·œê²©ì„ "
                f"ìµœì í™”ëœ ìˆœì°¨ ì²˜ë¦¬ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
            log(f"    - ê·œê²© ëª©ë¡: {[spec['name'] for spec in spec_list]}", "DEBUG")
            await self._process_specs_optimized(
                page, spec_list, raw_item_data,
                major_name, middle_name, sub_name, unit)

        except Exception as e:
            log(f"  ì†Œë¶„ë¥˜ '{sub_name}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ "
                f"[ëŒ€ë¶„ë¥˜: {major_name}, ì¤‘ë¶„ë¥˜: {middle_name}]: {str(e)}", "ERROR")
            return None

        # ìˆ˜ì§‘ëœ ë°ì´í„° ì²˜ë¦¬ - ìƒˆë¡œìš´ DataProcessor ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        local_processor = create_data_processor('kpi')
        if raw_item_data['spec_data']:
            local_processor.add_raw_data(raw_item_data)
            spec_count = len(raw_item_data['spec_data'])
            log(f"  - '{sub_name}' ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ "
                f"(ì´ {spec_count}ê°œ ê·œê²©)")
        else:
            log(f"  - '{sub_name}': ìˆ˜ì§‘ëœ ê·œê²© ë°ì´í„° ì—†ìŒ")

        return local_processor

    async def _process_specs_optimized(
            self, page, spec_list, raw_item_data,
            major_name, middle_name, sub_name, unit):
        """ìµœì í™”ëœ ìˆœì°¨ ì²˜ë¦¬ - í˜ì´ì§€ ë¦¬ë¡œë“œ ì—†ì´ ë¹ ë¥¸ ê·œê²© ë³€ê²½"""
        
        # ë‹¨ìœ„ ì •ë³´ ì‚¬ìš©
        raw_item_data['unit'] = unit
        log(f"      ë‹¨ìœ„: {unit}")
        log(f"      _process_specs_optimized ì‹œì‘ - ì²˜ë¦¬í•  ê·œê²© ìˆ˜: {len(spec_list)}", "DEBUG")
        
        # ê·œê²© ì²˜ë¦¬ í†µê³„ ì´ˆê¸°í™”
        success_count = 0
        error_count = 0
        error_details = []
        
        for i, spec in enumerate(spec_list):
            try:
                spec_value = spec['value']
                spec_name = spec['name']
                log(f"      - ê·œê²© {i + 1}/{len(spec_list)}: "
                    f"'{spec_name}' ì¡°íšŒ ì¤‘...")
                log(f"      - ê·œê²© ìƒì„¸ì •ë³´: value='{spec_value}', name='{spec_name}'", "DEBUG")

                # ê¸°ì¡´ ë°ì´í„° í™•ì¸
                existing_dates = await self._check_existing_data(
                    major_name, middle_name, sub_name, spec_name
                )
                log(f"      - ê¸°ì¡´ ë°ì´í„° í™•ì¸ ì™„ë£Œ: {len(existing_dates) if existing_dates else 0}ê°œ ë‚ ì§œ", "DEBUG")

                # ê·œê²© ì„ íƒ (ëŒ€ê¸° ì‹œê°„ ì¦ê°€ ë° ì¬ì‹œë„ ë¡œì§ ê°•í™”)
                spec_selector = '#ITEM_SPEC_CD'
                log(f"      - ê·œê²© ì„ íƒ ì‹œë„: {spec_selector} -> {spec_value}", "DEBUG")
                
                # ì¬ì‹œë„ ë¡œì§ ì¶”ê°€
                max_retries = 3
                for retry in range(max_retries):
                    try:
                        # ë“œë¡­ë‹¤ìš´ì´ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
                        await page.wait_for_selector(spec_selector, timeout=5000)
                        await page.locator(spec_selector).select_option(value=spec_value)
                        
                        # ë” ê¸´ ëŒ€ê¸° ì‹œê°„ìœ¼ë¡œ í˜ì´ì§€ ì•ˆì •í™”
                        await asyncio.sleep(1)  # ì¶”ê°€ ëŒ€ê¸°
                        await page.wait_for_load_state('networkidle', timeout=8000)
                        
                        # ì„ íƒì´ ì œëŒ€ë¡œ ë˜ì—ˆëŠ”ì§€ í™•ì¸
                        selected_value = await page.locator(spec_selector).input_value()
                        if selected_value == spec_value:
                            log(f"      - ê·œê²© ì„ íƒ ì„±ê³µ (ì‹œë„ {retry + 1}/{max_retries})", "DEBUG")
                            break
                        else:
                            log(f"      - ê·œê²© ì„ íƒ ì‹¤íŒ¨: ì˜ˆìƒê°’={spec_value}, ì‹¤ì œê°’={selected_value} (ì‹œë„ {retry + 1}/{max_retries})", "DEBUG")
                            if retry < max_retries - 1:
                                await asyncio.sleep(2)  # ì¬ì‹œë„ ì „ ëŒ€ê¸°
                                continue
                            else:
                                raise Exception(f"ê·œê²© ì„ íƒ ì‹¤íŒ¨: {spec_name}")
                    except Exception as e:
                        log(f"      - ê·œê²© ì„ íƒ ì˜¤ë¥˜ (ì‹œë„ {retry + 1}/{max_retries}): {str(e)}", "DEBUG")
                        if retry < max_retries - 1:
                            await asyncio.sleep(2)  # ì¬ì‹œë„ ì „ ëŒ€ê¸°
                            continue
                        else:
                            raise e
                
                log(f"      - ê·œê²© ì„ íƒ ì™„ë£Œ ë° í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì™„ë£Œ", "DEBUG")

                # ê¸°ê°„ ì„ íƒ (ì²« ë²ˆì§¸ ê·œê²©ì—ì„œë§Œ ì„¤ì •)
                if i == 0:
                    log(f"      - ì²« ë²ˆì§¸ ê·œê²©ì´ë¯€ë¡œ ê¸°ê°„ ì„¤ì • ì§„í–‰", "DEBUG")
                    # í˜„ì¬ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°
                    now = datetime.now()
                    current_year = str(now.year)
                    current_month = str(now.month).zfill(2)
                    
                    # ì‹œì‘ ê¸°ê°„: 2020ë…„ 1ì›”
                    year_from_selector = '#DATA_YEAR_F'
                    month_from_selector = '#DATA_MONTH_F'
                    await page.locator(year_from_selector).select_option(
                        value='2020')
                    await page.locator(month_from_selector).select_option(
                        value='01')
                    
                    # ì¢…ë£Œ ê¸°ê°„: ë‹¤ìŒ ë‹¬
                    # í˜„ì¬ ë‹¬ì´ 12ì›”ì´ë©´ ë‹¤ìŒ í•´ 1ì›”ë¡œ ì„¤ì •
                    current_month_int = int(current_month)
                    current_year_int = int(current_year)
                    if current_month_int == 12:
                        next_year = str(current_year_int + 1)
                        next_month = "01"
                    else:
                        next_year = current_year
                        next_month = str(current_month_int + 1).zfill(2)
                    
                    year_to_selector = '#DATA_YEAR_T'
                    month_to_selector = '#DATA_MONTH_T'
                    await page.locator(year_to_selector).select_option(
                        value=next_year)
                    await page.locator(month_to_selector).select_option(
                        value=next_month)
                    
                    await page.wait_for_load_state(
                        'networkidle', timeout=10000)
                    
                    # ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­ (ê¸°ê°„ ì„¤ì • í›„ ë°˜ë“œì‹œ ì‹¤í–‰) - ì¬ì‹œë„ ë¡œì§ ì¶”ê°€
                    search_selector = 'form[name="sForm"] input[type="image"]'
                    search_button = page.locator(search_selector)
                    
                    # ì¬ì‹œë„ ë¡œì§ (ìµœëŒ€ 3íšŒ ì‹œë„)
                    for attempt in range(3):
                        try:
                            await search_button.click(timeout=10000)
                            break
                        except Exception as e:
                            if attempt == 2:  # ë§ˆì§€ë§‰ ì‹œë„
                                raise e
                            log(f"        - ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/3), 2ì´ˆ í›„ ì¬ì‹œë„...")
                            await asyncio.sleep(2)
                    
                    log(f"        - ê¸°ê°„ ì„¤ì • ì™„ë£Œ: 2020.01 ~ "
            f"{next_year}.{next_month}")
                else:
                    # ì²« ë²ˆì§¸ ê·œê²©ì´ ì•„ë‹Œ ê²½ìš°ì—ë„ ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­ - ì¬ì‹œë„ ë¡œì§ ì¶”ê°€
                    search_selector = 'form[name="sForm"] input[type="image"]'
                    search_button = page.locator(search_selector)
                    
                    # ì¬ì‹œë„ ë¡œì§ (ìµœëŒ€ 3íšŒ ì‹œë„)
                    for attempt in range(3):
                        try:
                            await search_button.click(timeout=10000)
                            break
                        except Exception as e:
                            if attempt == 2:  # ë§ˆì§€ë§‰ ì‹œë„
                                raise e
                            log(f"        - ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/3), 2ì´ˆ í›„ ì¬ì‹œë„...")
                            await asyncio.sleep(2)

                # í…Œì´ë¸” ë¡œë”© ëŒ€ê¸° (ë°ì´í„°ê°€ ë¡œë“œë  ë•Œê¹Œì§€) - ì¬ì‹œë„ ë¡œì§ ì¶”ê°€
                table_selector = "#priceTrendDataArea tr"
                
                # ì¬ì‹œë„ ë¡œì§ (ìµœëŒ€ 3íšŒ ì‹œë„)
                for attempt in range(3):
                    try:
                        await page.wait_for_selector(table_selector, timeout=15000)
                        await page.wait_for_load_state('networkidle', timeout=10000)
                        break
                    except Exception as e:
                        if attempt == 2:  # ë§ˆì§€ë§‰ ì‹œë„
                            raise e
                        log(f"        - í…Œì´ë¸” ë¡œë”© ëŒ€ê¸° ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/3), 2ì´ˆ í›„ ì¬ì‹œë„...")
                        await asyncio.sleep(2)

                # ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œ ë²”ìœ„ í™•ì¸
                available_dates = await self._get_available_date_range(page)
                if not available_dates:
                    log("        - ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œ ì—†ìŒ")
                    continue
                
                # ê¸°ì¡´ ë°ì´í„°ì—ì„œ ë‚ ì§œë§Œ ì¶”ì¶œí•˜ì—¬ ë¹„êµ
                existing_date_set = set()
                if existing_dates:
                    existing_date_set = {item[0] for item in existing_dates}  # íŠœí”Œì˜ ì²« ë²ˆì§¸ ìš”ì†Œ(ë‚ ì§œ)ë§Œ ì¶”ì¶œ
                
                # ëˆ„ë½ëœ ë‚ ì§œë§Œ ì¶”ì¶œ
                missing_dates = [date for date in available_dates 
                               if date not in existing_date_set]
                
                if not missing_dates:
                    continue
                
                # ëˆ„ë½ëœ ë‚ ì§œì— ëŒ€í•´ì„œë§Œ ê°€ê²© ë°ì´í„° ì¶”ì¶œ
                await self._extract_price_data_fast(
                    page, spec_name, raw_item_data, existing_dates, unit)
                
                # ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ ê·œê²© ì¹´ìš´íŠ¸
                success_count += 1
                log(f"      - ê·œê²© '{spec_name}' ì²˜ë¦¬ ì™„ë£Œ âœ“", "DEBUG")

            except Exception as e:
                # ê·œê²© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ - ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
                error_count += 1
                error_info = {
                    'spec_name': spec['name'],
                    'spec_value': spec['value'],
                    'error_type': type(e).__name__,
                    'error_message': str(e),
                    'spec_index': i + 1
                }
                error_details.append(error_info)
                
                log(f"      - ê·œê²© '{spec['name']}' ì²˜ë¦¬ ì‹¤íŒ¨ âœ— "
                    f"({error_info['error_type']}: {error_info['error_message']})", "ERROR")
                
                # ê°œë³„ ê·œê²© ì˜¤ë¥˜ëŠ” ì „ì²´ ì²˜ë¦¬ë¥¼ ì¤‘ë‹¨í•˜ì§€ ì•Šê³  ê³„ì† ì§„í–‰
                continue
        
        # ì²˜ë¦¬ ì™„ë£Œ í›„ í†µê³„ ì¶œë ¥
        total_specs = len(spec_list)
        log(f"      ê·œê²© ì²˜ë¦¬ ì™„ë£Œ - ì´ {total_specs}ê°œ ì¤‘ ì„±ê³µ: {success_count}ê°œ, "
            f"ì‹¤íŒ¨: {error_count}ê°œ", "INFO")
        
        if error_details:
            log(f"      ì‹¤íŒ¨í•œ ê·œê²© ìƒì„¸ ì •ë³´:", "ERROR")
            for error in error_details:
                log(f"        - {error['spec_index']}/{total_specs}: "
                    f"'{error['spec_name']}' ({error['error_type']})", "ERROR")

    async def _extract_price_data_fast(self, page, spec_name,
                                       raw_item_data, existing_dates=None, unit_info=None):
        """ë¹ ë¥¸ ê°€ê²© ë°ì´í„° ì¶”ì¶œ - ëˆ„ë½ëœ ë°ì´í„°ë§Œ ì¶”ì¶œ"""
        try:
            # í•˜ë“œì½”ë”©ëœ ë‹¨ìœ„ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
            if unit_info:
                log(f"      - ìì¬: '{spec_name}' | ë‹¨ìœ„: {unit_info}")
            else:
                # ë‹¨ìœ„ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ì„¤ì •
                if "ë‚˜ì„ ì² ì„ " in spec_name or "ì² ì„ " in spec_name:
                    unit_info = "kg"
                    log(f"      - âš ï¸ ìì¬: '{spec_name}' | ë‹¨ìœ„ ì •ë³´ ì—†ìŒ - ê¸°ë³¸ê°’ 'kg' ì‚¬ìš©")
                elif "ì² ê·¼" in spec_name or "ë´‰ê°•" in spec_name:
                    unit_info = "kg"
                    log(f"      - âš ï¸ ìì¬: '{spec_name}' | ë‹¨ìœ„ ì •ë³´ ì—†ìŒ - ê¸°ë³¸ê°’ 'kg' ì‚¬ìš©")
                elif "ê°•íŒ" in spec_name or "í˜•ê°•" in spec_name:
                    unit_info = "kg"
                    log(f"      - âš ï¸ ìì¬: '{spec_name}' | ë‹¨ìœ„ ì •ë³´ ì—†ìŒ - ê¸°ë³¸ê°’ 'kg' ì‚¬ìš©")
                else:
                    unit_info = "ê°œ"
                    log(f"      - âš ï¸ ìì¬: '{spec_name}' | ë‹¨ìœ„ ì •ë³´ ì—†ìŒ - ê¸°ë³¸ê°’ 'ê°œ' ì‚¬ìš©")
            
            # í…Œì´ë¸” êµ¬ì¡° ê°ì§€ ë° ì²˜ë¦¬
            # 1. ì§€ì—­ í—¤ë”ê°€ ìˆëŠ” ë³µí•© í…Œì´ë¸” (ì²« ë²ˆì§¸ ì´ë¯¸ì§€ í˜•íƒœ)
            # 2. ë‚ ì§œì™€ ê°€ê²©ë§Œ ìˆëŠ” ë‹¨ìˆœ í…Œì´ë¸” (ë‘ ë²ˆì§¸ ì´ë¯¸ì§€ í˜•íƒœ)
            
            # í…Œì´ë¸”ì—ì„œ ì²« ë²ˆì§¸ í–‰ í™•ì¸
            all_table_rows = await page.locator("table").nth(1).locator("tr").all()
            if not all_table_rows:
                log(f"      - ê·œê²© '{spec_name}': í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return

            # ì²« ë²ˆì§¸ í–‰ ë¶„ì„í•˜ì—¬ í…Œì´ë¸” íƒ€ì… ê²°ì •
            first_row = all_table_rows[0]
            first_row_elements = await first_row.locator("td").all()
            if not first_row_elements:
                first_row_elements = await first_row.locator("th").all()
            
            if not first_row_elements:
                log(f"      - ê·œê²© '{spec_name}': ì²« ë²ˆì§¸ í–‰ì´ ë¹„ì–´ìˆìŒ")
                return

            # ì²« ë²ˆì§¸ í–‰ì˜ ì²« ë²ˆì§¸ ì…€ í…ìŠ¤íŠ¸ í™•ì¸
            first_cell_text = await first_row_elements[0].inner_text()
            first_cell_clean = first_cell_text.strip()
            
            # í—¤ë” í–‰ì—ì„œ spec_name ê¸°ë°˜ í…Œì´ë¸”ì¸ì§€ í™•ì¸ (1.5ãŸ, 2.5ãŸ ë“±)
            is_spec_name_table = False
            if len(first_row_elements) > 1:
                header_texts = []
                for i in range(1, min(len(first_row_elements), 4)):  # ìµœëŒ€ 3ê°œ í—¤ë” í™•ì¸
                    header_text = await first_row_elements[i].inner_text()
                    header_texts.append(header_text.strip())
                
                # spec_name íŒ¨í„´ í™•ì¸ (ãŸ, mm, ë“±ì˜ ë‹¨ìœ„ê°€ í¬í•¨ëœ ê²½ìš°)
                spec_patterns = ['ãŸ', 'mm', 'pair', 'core', 'sq', 'Ã—']
                for header in header_texts:
                    if any(pattern in header for pattern in spec_patterns):
                        is_spec_name_table = True
                        break
            
            # í…Œì´ë¸” íƒ€ì… ê²°ì •
            is_simple_table = (
                self._is_valid_date_value(first_cell_clean) or 
                self._is_valid_date_header(first_cell_clean)
            )
            
            if is_spec_name_table:
                # spec_name ê¸°ë°˜ í…Œì´ë¸” ì²˜ë¦¬ (êµ¬ë¶„ + ìƒì„¸ê·œê²©ë“¤)
                await self._extract_spec_name_table_data(
                    all_table_rows, spec_name, raw_item_data, existing_dates, unit_info
                )
            elif is_simple_table:
                # ë‹¨ìˆœ í…Œì´ë¸” ì²˜ë¦¬ (ë‚ ì§œ + ê°€ê²©)
                await self._extract_simple_table_data(
                    all_table_rows, spec_name, raw_item_data, existing_dates, unit_info
                )
            else:
                # ë³µí•© í…Œì´ë¸” ì²˜ë¦¬ (ì§€ì—­ í—¤ë” + ë‚ ì§œë³„ ë°ì´í„°)
                await self._extract_complex_table_data(
                    all_table_rows, spec_name, raw_item_data, existing_dates, unit_info
                )

        except Exception as e:
            log(f"'{spec_name}' ì˜¤ë¥˜: {str(e)}", "ERROR")

    async def _extract_simple_table_data(self, all_table_rows, spec_name, 
                                       raw_item_data, existing_dates, unit_info=None):
        """ë‹¨ìˆœ í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ (ë‚ ì§œ + ê°€ê²© í˜•íƒœ)"""
        try:
            extracted_count = 0
            default_region = "ì „êµ­"  # ì§€ì—­ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’
            
            for row_idx, row in enumerate(all_table_rows):
                try:
                    # í–‰ì˜ ëª¨ë“  ì…€ ì¶”ì¶œ
                    cells = await row.locator("td").all()
                    if not cells:
                        cells = await row.locator("th").all()
                    
                    if len(cells) < 2:  # ìµœì†Œ ë‚ ì§œì™€ ê°€ê²© í•„ìš”
                        continue
                    
                    # ì²« ë²ˆì§¸ ì…€ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
                    date_str = await cells[0].inner_text()
                    date_clean = date_str.strip()
                    
                    # ë‚ ì§œ ìœ íš¨ì„± ê²€ì¦
                    if not self._is_valid_date_value(date_clean):
                        continue
                    
                    formatted_date = self._format_date_header(date_clean)
                    if not formatted_date:
                        continue
                    
                    # ë‘ ë²ˆì§¸ ì…€ì—ì„œ ê°€ê²© ì¶”ì¶œ
                    price_str = await cells[1].inner_text()
                    
                    if self._is_valid_price(price_str):
                        clean_price = price_str.strip().replace(',', '')
                        try:
                            price_value = float(clean_price)
                            
                            # ì¤‘ë³µ ì²´í¬ (6ê°œ í•„ë“œ: major_category, middle_category, sub_category, specification, region, date - ê°€ê²© ì œì™¸)
                            duplicate_key = (raw_item_data['major_category_name'], raw_item_data['middle_category_name'], 
                                           raw_item_data['sub_category_name'], spec_name, default_region, formatted_date)
                            if existing_dates and duplicate_key in existing_dates:
                                continue
                            
                            if not unit_info:
                                raise ValueError(f"ë‹¨ìœ„ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. spec_name: {spec_name}")
                            
                            price_data = {
                                'spec_name': spec_name,
                                'region': default_region,
                                'date': formatted_date,
                                'price': price_value,
                                'unit': unit_info
                            }
                            spec_data = raw_item_data['spec_data']
                            spec_data.append(price_data)
                            extracted_count += 1
                            
                            if extracted_count % 50 == 0:
                                log(f"ì§„í–‰: {extracted_count}ê°œ ì¶”ì¶œë¨")
                        except ValueError:
                            continue
                
                except Exception as e:
                    log(f"      - í–‰ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    continue
            
            if extracted_count > 0:
                log(f"'{spec_name}' (ë‹¨ìˆœí˜•): {extracted_count}ê°œ ì™„ë£Œ", "SUCCESS")
                
        except Exception as e:
            log(f"ë‹¨ìˆœ í…Œì´ë¸” ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}", "ERROR")

    async def _extract_spec_name_table_data(self, all_table_rows, spec_name, 
                                          raw_item_data, existing_dates, unit_info=None):
        """spec_name ê¸°ë°˜ í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ (êµ¬ë¶„ + ìƒì„¸ê·œê²©ë“¤)"""
        try:
            extracted_count = 0
            default_region = "ì „êµ­"  # spec_name ê¸°ë°˜ í…Œì´ë¸”ì€ ì§€ì—­ êµ¬ë¶„ì´ ì—†ìœ¼ë¯€ë¡œ ì „êµ­ìœ¼ë¡œ ì„¤ì •
            
            # ì²« ë²ˆì§¸ í–‰ì—ì„œ í—¤ë” ì •ë³´ ì¶”ì¶œ
            if not all_table_rows:
                return
                
            header_row = all_table_rows[0]
            header_cells = await header_row.locator("th").all()
            if not header_cells:
                header_cells = await header_row.locator("td").all()
            
            if len(header_cells) < 2:
                log(f"'{spec_name}' (spec_nameí˜•): í—¤ë”ê°€ ë¶€ì¡±í•¨")
                return
            
            # í—¤ë”ì—ì„œ spec_nameë“¤ ì¶”ì¶œ (ì²« ë²ˆì§¸ëŠ” 'êµ¬ë¶„', ë‚˜ë¨¸ì§€ëŠ” ìƒì„¸ê·œê²©ë“¤)
            spec_names = []
            for i in range(1, len(header_cells)):
                header_text = await header_cells[i].inner_text()
                spec_names.append(header_text.strip())
            
            log(f"'{spec_name}' (spec_nameí˜•): ë°œê²¬ëœ ìƒì„¸ê·œê²©ë“¤: {spec_names}")
            
            # ë°ì´í„° í–‰ë“¤ ì²˜ë¦¬ (ì²« ë²ˆì§¸ í–‰ì€ í—¤ë”ì´ë¯€ë¡œ ì œì™¸)
            for row_idx, row in enumerate(all_table_rows[1:], 1):
                try:
                    # í–‰ì˜ ëª¨ë“  ì…€ ì¶”ì¶œ
                    cells = await row.locator("td").all()
                    if not cells:
                        cells = await row.locator("th").all()
                    
                    if len(cells) < 2:  # ìµœì†Œ ë‚ ì§œì™€ ê°€ê²© 1ê°œ í•„ìš”
                        continue
                    
                    # ì²« ë²ˆì§¸ ì…€ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
                    date_str = await cells[0].inner_text()
                    date_clean = date_str.strip()
                    
                    # ë‚ ì§œ ìœ íš¨ì„± ê²€ì¦
                    if not self._is_valid_date_value(date_clean):
                        continue
                    
                    formatted_date = self._format_date_header(date_clean)
                    if not formatted_date:
                        continue
                    
                    # ê° ìƒì„¸ê·œê²©ë³„ë¡œ ê°€ê²© ì¶”ì¶œ
                    for spec_idx, current_spec_name in enumerate(spec_names):
                        cell_idx = spec_idx + 1  # ì²« ë²ˆì§¸ ì…€ì€ ë‚ ì§œì´ë¯€ë¡œ +1
                        
                        if cell_idx >= len(cells):
                            continue
                        
                        price_str = await cells[cell_idx].inner_text()
                        
                        if self._is_valid_price(price_str):
                            clean_price = price_str.strip().replace(',', '')
                            try:
                                price_value = float(clean_price)
                                
                                # ì¤‘ë³µ ì²´í¬ (6ê°œ í•„ë“œ: major_category, middle_category, sub_category, specification, region, date - ê°€ê²© ì œì™¸)
                                duplicate_key = (raw_item_data['major_category_name'], 
                                               raw_item_data['middle_category_name'], 
                                               raw_item_data['sub_category_name'], 
                                               spec_name, default_region, formatted_date)
                                if existing_dates and duplicate_key in existing_dates:
                                    continue
                                
                                if not unit_info:
                                    raise ValueError(f"ë‹¨ìœ„ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. spec_name: {spec_name}")
                                
                                price_data = {
                                    'spec_name': current_spec_name,  # ìƒì„¸ê·œê²©ëª… ì‚¬ìš©
                                    'region': default_region,
                                    'date': formatted_date,
                                    'price': price_value,
                                    'unit': unit_info
                                }
                                spec_data = raw_item_data['spec_data']
                                spec_data.append(price_data)
                                extracted_count += 1
                                
                                if extracted_count % 50 == 0:
                                    log(f"ì§„í–‰: {extracted_count}ê°œ ì¶”ì¶œë¨")
                            except ValueError:
                                continue
                
                except Exception as e:
                    log(f"      - í–‰ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    continue
            
            if extracted_count > 0:
                log(f"'{spec_name}' (spec_nameí˜•): {extracted_count}ê°œ ì™„ë£Œ", "SUCCESS")
                
        except Exception as e:
            log(f"spec_name í…Œì´ë¸” ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}", "ERROR")

    async def _extract_complex_table_data(self, all_table_rows, spec_name, 
                                        raw_item_data, existing_dates, unit_info=None):
        """ë³µí•© í…Œì´ë¸” ë°ì´í„° ì¶”ì¶œ (ì§€ì—­ í—¤ë” + ë‚ ì§œë³„ ë°ì´í„°)"""
        try:
            # ì§€ì—­ í—¤ë” í–‰ ì¶”ì¶œ (ì²« ë²ˆì§¸ í–‰)
            if len(all_table_rows) < 1:
                return
                
            region_header_row = all_table_rows[0]
            region_header_elements = await region_header_row.locator("td").all()
            if not region_header_elements:
                region_header_elements = await region_header_row.locator("th").all()
            
            if not region_header_elements:
                log(f"      - ê·œê²© '{spec_name}': ì§€ì—­ í—¤ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return

            # ì§€ì—­ í—¤ë” ì¶”ì¶œ (ì²« ë²ˆì§¸ ì»¬ëŸ¼ 'êµ¬ë¶„' ì œì™¸, ìœ íš¨í•œ ì§€ì—­ë§Œ)
            regions = []
            valid_region_indices = []
            for i in range(1, len(region_header_elements)):
                header_text = await region_header_elements[i].inner_text()
                region_name = self._clean_region_name(header_text.strip())
                if self._is_valid_region_name(region_name):
                    regions.append(region_name)
                    valid_region_indices.append(i)

            if not regions:
                return

            # ë°ì´í„° í–‰ ì¶”ì¶œ (ë‘ ë²ˆì§¸ í–‰ë¶€í„°)
            data_rows = all_table_rows[1:] if len(all_table_rows) >= 2 else []
            if not data_rows:
                return

            extracted_count = 0
            # ê° ë‚ ì§œë³„ ë°ì´í„° ì²˜ë¦¬
            for row_idx, row in enumerate(data_rows):
                try:
                    # ì²« ë²ˆì§¸ ì…€ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
                    date_element = row.locator("td").first
                    if not await date_element.count():
                        date_element = row.locator("th").first

                    if not await date_element.count():
                        continue

                    date_str = await date_element.inner_text()
                    date_clean = date_str.strip()
                    
                    # ë‚ ì§œ í˜•ì‹ ë³€í™˜ ë° ì¤‘ë³µ ì²´í¬
                    if not self._is_valid_date_value(date_clean):
                        continue
                    
                    formatted_date = self._format_date_header(date_clean)
                    if not formatted_date:
                        continue
                    
                    # ë‚ ì§œ ìœ íš¨ì„± ì¬ê²€ì¦
                    if not self._is_valid_date_header(date_clean):
                        continue

                    # í•´ë‹¹ í–‰ì˜ ëª¨ë“  ê°€ê²© ì…€ ì¶”ì¶œ (ì²« ë²ˆì§¸ ì…€ ì œì™¸)
                    price_cells = await row.locator("td").all()
                    if not price_cells:
                        all_cells = await row.locator("th").all()
                        price_cells = all_cells[1:] if len(all_cells) > 1 else []

                    # ê° ì§€ì—­ë³„ ê°€ê²© ì²˜ë¦¬
                    for region_idx, region_name in enumerate(regions):
                        cell_idx = region_idx + 1
                        if cell_idx >= len(price_cells):
                            continue
                            
                        price_cell = price_cells[cell_idx]
                        price_str = await price_cell.inner_text()
                        
                        if self._is_valid_price(price_str):
                            clean_price = price_str.strip().replace(',', '')
                            try:
                                price_value = float(clean_price)
                                
                                # ì¤‘ë³µ ì²´í¬ (7ê°œ í•„ë“œ: major_category, middle_category, sub_category, specification, region, date, price)
                                duplicate_key = (raw_item_data['major_category_name'], raw_item_data['middle_category_name'], 
                                               raw_item_data['sub_category_name'], spec_name, region_name, formatted_date, str(price_value))
                                if existing_dates and duplicate_key in existing_dates:
                                    continue
                                    
                                if not unit_info:
                                    raise ValueError(f"ë‹¨ìœ„ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. spec_name: {spec_name}")
                                
                                price_data = {
                                    'spec_name': spec_name,
                                    'region': region_name,
                                    'date': formatted_date,
                                    'price': price_value,
                                    'unit': unit_info
                                }
                                spec_data = raw_item_data['spec_data']
                                spec_data.append(price_data)
                                extracted_count += 1
                                
                                if extracted_count % 50 == 0:
                                    log(f"ì§„í–‰: {extracted_count}ê°œ ì¶”ì¶œë¨")
                            except ValueError:
                                continue
                        else:
                            continue
                except Exception as e:
                    log(f"      - í–‰ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                    continue

            if extracted_count > 0:
                log(f"'{spec_name}' (ë³µí•©í˜•): {extracted_count}ê°œ ì™„ë£Œ", "SUCCESS")

        except Exception as e:
            log(f"ë³µí•© í…Œì´ë¸” ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}", "ERROR")

    def _clean_region_name(self, region_str):
        """ì§€ì—­ëª… ì •ë¦¬ í•¨ìˆ˜ - 'ì„œìš¸1', 'ë¶€ì‚°2' í˜•íƒœë¡œ ì •ê·œí™”, 'ê°€ê²©1', 'ê°€ê²©2', 'ê°€â‘ ê²©', 'ê°€ê²©â‘ ' ë“±ì€ 'ì „êµ­'ìœ¼ë¡œ ë³€í™˜
        ìƒì„¸ê·œê²©ëª…(PVC, STS304, STS316, PTFE ë“±)ì´ ë“¤ì–´ì˜¨ ê²½ìš°ë„ 'ì „êµ­'ìœ¼ë¡œ ë³€í™˜"""
        import re
        
        # ë™ê·¸ë¼ë¯¸ ìˆ«ìë¥¼ ì¼ë°˜ ìˆ«ìë¡œ ë³€í™˜ (ë¨¼ì € ì²˜ë¦¬í•˜ì—¬ ì´í›„ íŒ¨í„´ ë§¤ì¹­ì— í™œìš©)
        circle_to_num = {
            'â‘ ': '1', 'â‘¡': '2', 'â‘¢': '3', 'â‘£': '4', 'â‘¤': '5',
            'â‘¥': '6', 'â‘¦': '7', 'â‘§': '8', 'â‘¨': '9', 'â‘©': '10'
        }
        
        clean_region = region_str.strip()
        for circle, num in circle_to_num.items():
            clean_region = clean_region.replace(circle, num)
        
        # ìƒì„¸ê·œê²©ëª… íŒ¨í„´ë“¤ì„ 'ì „êµ­'ìœ¼ë¡œ ë³€í™˜
        spec_patterns = [
            'PVC', 'STS304', 'STS316', 'PTFE', 'Coil', 'Sheet',
            r'^\d+\.\d+mm$',  # 2.0mm, 2.5mm ë“±
            r'^Coverë‘ê»˜\s*\d+ãœ$',  # Coverë‘ê»˜ 25ãœ ë“±
            r'^SCS13\s+\d+K\s+(BLFF|SOFF)$',  # SCS13 10K BLFF ë“±
            r'^SCS13\s+\d+LB\s+HUB$',  # SCS13 150LB HUB
            r'^\([^)]+\)$',  # (ë‹¨íŒ), (ë³´ì˜¨) ë“± ê´„í˜¸ë¡œ ë‘˜ëŸ¬ì‹¸ì¸ íŒ¨í„´
            r'^ë¶„ë§ì‹\s+PE\s+3ì¸µ\s+í”¼ë³µê°•ê´€',  # ë¶„ë§ì‹ PE 3ì¸µ í”¼ë³µê°•ê´€ ê´€ë ¨
            r'^í´ë¦¬ìš°ë ˆíƒ„ê°•ê´€',  # í´ë¦¬ìš°ë ˆíƒ„ê°•ê´€ ê´€ë ¨
            r'^ê³ ê¸‰í˜•$', r'^ë³´ì˜¨$'  # ê³ ê¸‰í˜•, ë³´ì˜¨
        ]
        
        # 'ê°€ê²©1', 'ê°€ê²©2', 'ê°€1ê²©', 'ê°€2ê²©' ë“±ì˜ íŒ¨í„´ì€ 'ì „êµ­'ìœ¼ë¡œ ë³€í™˜
        price_pattern1 = r'^ê°€ê²©\d*$'
        price_pattern2 = r'^ê°€\d+ê²©$'
        
        if re.match(price_pattern1, clean_region) or re.match(price_pattern2, clean_region):
            return "ì „êµ­"
        
        # ìƒì„¸ê·œê²©ëª… íŒ¨í„´ ì²´í¬
        for pattern in spec_patterns:
            if isinstance(pattern, str):
                if clean_region == pattern:
                    return "ì „êµ­"
            else:
                if re.match(pattern, clean_region):
                    return "ì „êµ­"
        
        # 'ì„œ1ìš¸' â†’ 'ì„œìš¸1' í˜•íƒœë¡œ ë³€í™˜
        pattern = r'^([ê°€-í£])(\d+)([ê°€-í£]+)$'
        match = re.match(pattern, clean_region)
        
        if match:
            first_char, number, rest = match.groups()
            clean_region = f"{first_char}{rest}{number}"
        
        return clean_region

    def _is_valid_date_header(self, header_text):
        """ë‚ ì§œ í—¤ë” ìœ íš¨ì„± ê²€ì¦ í•¨ìˆ˜"""
        if not header_text or not header_text.strip():
            return False

        header_str = header_text.strip()
        
        # ë¹ˆ ë¬¸ìì—´ì´ë‚˜ ê³µë°±ë§Œ ìˆëŠ” ê²½ìš°
        if not header_str or header_str.isspace():
            return False

        # ë‚ ì§œ íŒ¨í„´ í™•ì¸ (ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›)
        date_patterns = [
            r'^\d{4}\.\d{1,2}$',  # YYYY.M
            r'^\d{4}-\d{1,2}$',   # YYYY-M
            r'^\d{4}/\d{1,2}$',   # YYYY/M
            r'^\d{4}\.\s*\d{1,2}$',  # YYYY. M (ê³µë°± í¬í•¨)
            r'^\d{4}ë…„\s*\d{1,2}ì›”$',  # YYYYë…„ Mì›”
            r'^\d{4}\s+\d{1,2}$'  # YYYY M (ê³µë°±ìœ¼ë¡œ êµ¬ë¶„)
        ]
        
        for pattern in date_patterns:
            if re.match(pattern, header_str):
                return True

        # ê¸°íƒ€ ì˜ëª»ëœ ê°’ ì²´í¬ (ë” í¬ê´„ì ìœ¼ë¡œ)
        # ì£¼ì˜: 'êµ¬ë¶„'ì€ í…Œì´ë¸”ì˜ ì²« ë²ˆì§¸ ì»¬ëŸ¼ í—¤ë”ë¡œ ì‚¬ìš©ë˜ë¯€ë¡œ ì œì™¸í•˜ì§€ ì•ŠìŒ
        invalid_patterns = [
            'ì§€ì—­', 'í‰ê· ', 'ì „êµ­', 'ê¸°ì¤€', 'í•©ê³„', 'ì´ê³„', 'ì†Œê³„',
            'ë‹¨ìœ„', 'ì›', 'ì²œì›', 'ë§Œì›', 'ì–µì›',
            'ê·œê²©', 'í’ˆëª©', 'ìì¬', 'ì¬ë£Œ'
        ]
        
        # ê°€ê²© ê´€ë ¨ íŒ¨í„´ ì²´í¬ (ë™ê·¸ë¼ë¯¸ ìˆ«ì í¬í•¨)
        price_patterns = [
            'ê°€ê²©', 'ê°€â‘ ê²©', 'ê°€â‘¡ê²©', 'ê°€â‘¢ê²©', 'ê°€â‘£ê²©', 'ê°€â‘¤ê²©',
            'ê°€â‘¥ê²©', 'ê°€â‘¦ê²©', 'ê°€â‘§ê²©', 'ê°€â‘¨ê²©', 'ê°€â‘©ê²©'
        ]
        
        for pattern in invalid_patterns + price_patterns:
            if pattern in header_str:
                log(f"        - ì˜ëª»ëœ íŒ¨í„´ìœ¼ë¡œ ì¸ì‹ëœ ë‚ ì§œ í—¤ë” ì œì™¸: {header_str}")
                return False
        
        # ìˆ«ìë§Œìœ¼ë¡œ êµ¬ì„±ëœ ê²½ìš° (ì—°ë„ë‚˜ ì›”ë§Œ ìˆëŠ” ê²½ìš°)
        if header_str.isdigit():
            # 4ìë¦¬ ìˆ«ìëŠ” ì—°ë„ë¡œ ì¸ì •
            if len(header_str) == 4 and 1900 <= int(header_str) <= 2100:
                return True
            # 1-2ìë¦¬ ìˆ«ìëŠ” ì›”ë¡œ ì¸ì •
            elif len(header_str) <= 2 and 1 <= int(header_str) <= 12:
                return True
            else:
                return False

        return False

    def _format_date_header(self, header_text):
        """ë‚ ì§œ í—¤ë”ë¥¼ YYYY-MM-01 í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        if not header_text or not header_text.strip():
            return header_text
            
        header_str = header_text.strip()
        
        # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ íŒ¨í„´ ì²˜ë¦¬
        date_patterns = [
            (r'^(\d{4})\.(\d{1,2})$', '-'),  # YYYY.M
            (r'^(\d{4})-(\d{1,2})$', '-'),   # YYYY-M
            (r'^(\d{4})/(\d{1,2})$', '-'),   # YYYY/M
            (r'^(\d{4})\.\s*(\d{1,2})$', '-'),  # YYYY. M (ê³µë°± í¬í•¨)
            (r'^(\d{4})ë…„\s*(\d{1,2})ì›”$', '-'),  # YYYYë…„ Mì›”
            (r'^(\d{4})\s+(\d{1,2})$', '-')  # YYYY M (ê³µë°±ìœ¼ë¡œ êµ¬ë¶„)
        ]
        
        for pattern, separator in date_patterns:
            match = re.match(pattern, header_str)
            if match:
                year = match.group(1)
                month = match.group(2).zfill(2)  # ì›”ì„ 2ìë¦¬ë¡œ íŒ¨ë”©
                formatted_date = f"{year}-{month}-01"
                # ë¡œê·¸ ê°„ì†Œí™”: ì²« ë²ˆì§¸ ë³€í™˜ë§Œ ë¡œê·¸ ì¶œë ¥
                if not hasattr(self, '_date_conversion_logged'):
                    log(f"        - ë‚ ì§œ í—¤ë” ë³€í™˜ ì˜ˆì‹œ: {header_str} -> {formatted_date}")
                    self._date_conversion_logged = True
                return formatted_date
        
        # ìˆ«ìë§Œìœ¼ë¡œ êµ¬ì„±ëœ ê²½ìš° ì²˜ë¦¬
        if header_str.isdigit():
            # 4ìë¦¬ ìˆ«ìëŠ” ì—°ë„ë¡œ ì²˜ë¦¬ (1ì›”ë¡œ ì„¤ì •)
            if len(header_str) == 4 and 1900 <= int(header_str) <= 2100:
                formatted_date = f"{header_str}-01-01"
                return formatted_date
            # 1-2ìë¦¬ ìˆ«ìëŠ” ì›”ë¡œ ì²˜ë¦¬ (í˜„ì¬ ì—°ë„ ì‚¬ìš©)
            elif len(header_str) <= 2 and 1 <= int(header_str) <= 12:
                current_year = datetime.now().year
                month = header_str.zfill(2)
                formatted_date = f"{current_year}-{month}-01"
                return formatted_date
        
        # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜ (ë¡œê·¸ ìƒëµ)
        return header_text

    def _is_valid_region_name(self, region_name):
        """ì§€ì—­ëª… ìœ íš¨ì„± ê²€ì¦ í•¨ìˆ˜ - 'ì„œìš¸1', 'ë¶€ì‚°2' í˜•íƒœ í—ˆìš©"""
        if not region_name or not region_name.strip():
            return False

        region_str = region_name.strip()
        
        # ë¹ˆ ë¬¸ìì—´ì´ë‚˜ ê³µë°±ë§Œ ìˆëŠ” ê²½ìš°
        if not region_str or region_str.isspace():
            return False

        # í•œêµ­ ì§€ì—­ëª… íŒ¨í„´ í™•ì¸ (ë” í¬ê´„ì ìœ¼ë¡œ)
        valid_regions = [
            'ê°•ì›', 'ê²½ê¸°', 'ê²½ë‚¨', 'ê²½ë¶', 'ê´‘ì£¼', 'ëŒ€êµ¬', 'ëŒ€ì „', 'ë¶€ì‚°',
            'ì„œìš¸', 'ì„¸ì¢…', 'ìš¸ì‚°', 'ì¸ì²œ', 'ì „ë‚¨', 'ì „ë¶', 'ì œì£¼', 'ì¶©ë‚¨', 'ì¶©ë¶',
            'ìˆ˜ì›', 'ì„±ë‚¨', 'ì¶˜ì²œ'  # ì¶”ê°€ ì§€ì—­ëª…
        ]

        # ìˆ«ìê°€ í¬í•¨ëœ ì§€ì—­ëª…ë„ í—ˆìš© (ì˜ˆ: ì„œìš¸1, ë¶€ì‚°2)
        # ìƒˆë¡œìš´ íŒ¨í„´: ì§€ì—­ëª… + ìˆ«ì (ì„œìš¸1, ë¶€ì‚°2 ë“±)
        for region in valid_regions:
            if region in region_str:
                # ì§€ì—­ëª…ì´ í¬í•¨ë˜ì–´ ìˆê³ , ìˆ«ìê°€ ë’¤ì— ì˜¤ëŠ” íŒ¨í„´ í—ˆìš©
                pattern = f"{region}\\d*$"
                if re.search(pattern, region_str):
                    return True
                # ê¸°ì¡´ íŒ¨í„´ë„ í—ˆìš© (ì§€ì—­ëª…ë§Œ)
                if region_str == region:
                    return True

        # ë‚ ì§œ íŒ¨í„´ì´ í¬í•¨ëœ ê²½ìš° ì§€ì—­ëª…ì´ ì•„ë‹˜ (ë” ì—„ê²©í•˜ê²Œ)
        date_patterns = [
            r'\d{4}[./-]\d{1,2}',  # YYYY.M, YYYY/M, YYYY-M
            r'\d{4}\.\s*\d{1,2}',  # YYYY. M (ê³µë°± í¬í•¨)
            r'^\d{4}$',  # ì—°ë„ë§Œ
            r'^\d{1,2}$',  # ì›”ë§Œ
            r'^\d{4}ë…„',  # YYYYë…„
            r'^\d{1,2}ì›”'  # Mì›”
        ]
        
        for pattern in date_patterns:
            if re.search(pattern, region_str):
                # ë¡œê·¸ ìµœì í™”: ë‚ ì§œ íŒ¨í„´ ì œì™¸ ë¡œê·¸ ìƒëµ
                return False

        # ê¸°íƒ€ ì˜ëª»ëœ ê°’ ì²´í¬ (ë” í¬ê´„ì ìœ¼ë¡œ)
        invalid_patterns = [
            'êµ¬ë¶„', 'í‰ê· ', 'ê¸°ì¤€', 'í•©ê³„', 'ì´ê³„', 'ì†Œê³„',
            'ë‹¨ìœ„', 'ì›', 'ì²œì›', 'ë§Œì›', 'ì–µì›',
            'ë…„', 'ì›”', 'ì¼', 'ê¸°ê°„',
            '-', '/', '\\', '|', '+', '='
        ]
        
        for pattern in invalid_patterns:
            if pattern in region_str:
                # ë¡œê·¸ ìµœì í™”: ì˜ëª»ëœ íŒ¨í„´ ì œì™¸ ë¡œê·¸ ìƒëµ
                return False
        
        # 'ê°€ê²©1', 'ê°€ê²©2' ë“±ì˜ íŒ¨í„´ì€ ìœ íš¨í•œ ì§€ì—­ëª…ìœ¼ë¡œ ê°„ì£¼í•˜ë˜, ë‚˜ì¤‘ì— 'ì „êµ­'ìœ¼ë¡œ ë³€í™˜
        price_pattern = r'^ê°€ê²©\d*$'
        if re.match(price_pattern, region_str):
            return True
        
        # ìˆ«ìë§Œìœ¼ë¡œ êµ¬ì„±ëœ ê²½ìš° ì œì™¸
        if region_str.isdigit():
            return False
        
        # íŠ¹ìˆ˜ë¬¸ìë§Œìœ¼ë¡œ êµ¬ì„±ëœ ê²½ìš° ì œì™¸
        if not re.search(r'[ê°€-í£a-zA-Z]', region_str):
            return False

        return True

    def _is_valid_date_value(self, date_value):
        """ë‚ ì§œ ê°’ì´ ìœ íš¨í•œì§€ í™•ì¸"""
        if date_value is None:
            return False

        # datetime ê°ì²´ì¸ ê²½ìš° ìœ íš¨
        if hasattr(date_value, 'strftime'):
            return True

        # ë¬¸ìì—´ì¸ ê²½ìš° ê²€ì¦
        if isinstance(date_value, str):
            # ë™ê·¸ë¼ë¯¸ ìˆ«ìë‚˜ íŠ¹ìˆ˜ë¬¸ìê°€ í¬í•¨ëœ ê²½ìš° ì œì™¸
            circle_chars = ['â‘ ', 'â‘¡', 'â‘¢', 'â‘£', 'â‘¤', 'â‘¥', 'â‘¦', 'â‘§', 'â‘¨', 'â‘©']
            if any(char in date_value for char in circle_chars):
                return False

            # 'ê°€ê²©' ë“±ì˜ í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ ê²½ìš° ì œì™¸
            if any(char in date_value for char in ['ê°€', 'ê²©', 'êµ¬', 'ë¶„']):
                return False

            # 'YYYY-MM-DD' í˜•íƒœ í™•ì¸
            if re.match(r'^\d{4}-\d{2}-\d{2}$', date_value.strip()):
                return True

            # 'YYYY. M' í˜•íƒœ í™•ì¸
            date_pattern = r'^\d{4}\.\s*\d{1,2}$'
            if re.match(date_pattern, date_value.strip()):
                return True

        return False

    def _is_valid_price(self, price_str):
        """ê°€ê²© ë°ì´í„° ìœ íš¨ì„± ê²€ì¦ - ë³€í˜•ëœ í˜•íƒœ í¬í•¨"""
        if not price_str:
            return False
        
        price_stripped = price_str.strip()
        if (not price_stripped or 
            price_stripped == '-' or 
            price_stripped == ''):
            return False
        
        # ë³€í˜•ëœ ê°€ê²© ì»¬ëŸ¼ëª… ì²˜ë¦¬ ('ê°€â‘ ê²©', 'ê°€â‘¡ê²©' ë“±)
        # ìˆ«ìì™€ íŠ¹ìˆ˜ë¬¸ìê°€ í¬í•¨ëœ ê°€ê²© í—¤ë”ëŠ” ì œì™¸
        if any(char in price_stripped for char in ['â‘ ', 'â‘¡', 'â‘¢', 'â‘£', 'â‘¤', 'â‘¥', 'â‘¦', 'â‘§', 'â‘¨', 'â‘©']):
            return False
        
        # í•œê¸€ì´ í¬í•¨ëœ ê²½ìš° ì œì™¸ (í—¤ë”ì¼ ê°€ëŠ¥ì„±)
        if any('\u3131' <= char <= '\u3163' or '\uac00' <= char <= '\ud7a3' for char in price_stripped):
            return False
        
        # ì˜ë¬¸ìê°€ í¬í•¨ëœ ê²½ìš° ì œì™¸ (í—¤ë”ì¼ ê°€ëŠ¥ì„±)
        if any(char.isalpha() for char in price_stripped):
            return False
            
        clean_price = price_stripped.replace(',', '')
        try:
            float(clean_price)
            return True
        except ValueError:
            return False

            # await page.close()
            
            # if unit:
            #     log(f"ë‹¨ìœ„ ì •ë³´ ì¶”ì¶œ ì„±ê³µ: {cate_cd}-{item_cd} -> {unit}")
            #     return unit
            # else:
            #     log(f"ë‹¨ìœ„ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {cate_cd}-{item_cd}")
            #     return None
                
        # except Exception as e:
        #     log(f"ë‹¨ìœ„ ì •ë³´ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {cate_cd}-{item_cd}, {str(e)}", "ERROR")
        #     if 'page' in locals():
        #         await page.close()
        #     return None

    # ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” unit ì¶”ì¶œ í•¨ìˆ˜ (í•˜ë“œì½”ë”©ëœ unit ì‚¬ìš©ìœ¼ë¡œ ëŒ€ì²´)
    # def _get_unit_with_caching(self, cate_cd, item_cd):
    #     """ìºì‹±ì„ ì‚¬ìš©í•˜ì—¬ ë‹¨ìœ„ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    #     # ìºì‹œ í‚¤ ìƒì„±
    #     cache_key = f"unit_{cate_cd}_{item_cd}"
    #     
    #     # ë©”ëª¨ë¦¬ ìºì‹œì—ì„œ í™•ì¸
    #     if not hasattr(self, '_unit_cache'):
    #         self._unit_cache = {}
    #     
    #     if cache_key in self._unit_cache:
    #         return self._unit_cache[cache_key]
    #     
    #     # íŒŒì¼ ìºì‹œì—ì„œ í™•ì¸
    #     unit = self._load_unit_from_file_cache(cate_cd, item_cd)
    #     if unit:
    #         # ë©”ëª¨ë¦¬ ìºì‹œì—ë„ ì €ì¥
    #         self._unit_cache[cache_key] = unit
    #         return unit
    #     
    #     # ìºì‹œì— ì—†ìœ¼ë©´ None ë°˜í™˜ (ë¹„ë™ê¸° í•¨ìˆ˜ì—ì„œ ì‹¤ì œ ì¶”ì¶œ)
    #     return None
    
    # ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” unit ì¶”ì¶œ í•¨ìˆ˜ (í•˜ë“œì½”ë”©ëœ unit ì‚¬ìš©ìœ¼ë¡œ ëŒ€ì²´)
    # def _cache_unit(self, cate_cd, item_cd, unit):
    #     """ë‹¨ìœ„ ì •ë³´ë¥¼ ìºì‹œì— ì €ì¥í•©ë‹ˆë‹¤."""
    #     if not hasattr(self, '_unit_cache'):
    #         self._unit_cache = {}
    #     
    #     cache_key = f"unit_{cate_cd}_{item_cd}"
    #     self._unit_cache[cache_key] = unit
    #     log(f"ë‹¨ìœ„ ì •ë³´ ìºì‹œ ì €ì¥: {cache_key} -> {unit}")
    #     
    #     # íŒŒì¼ ê¸°ë°˜ ìºì‹œì—ë„ ì €ì¥
    #     self._save_unit_to_file_cache(cate_cd, item_cd, unit)

    # ë” ì´ìƒ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” unit ì¶”ì¶œ í•¨ìˆ˜ (í•˜ë“œì½”ë”©ëœ unit ì‚¬ìš©ìœ¼ë¡œ ëŒ€ì²´)
    # def _save_unit_to_file_cache(self, cate_cd, item_cd, unit):
    #     """ë‹¨ìœ„ ì •ë³´ë¥¼ íŒŒì¼ ìºì‹œì— ì €ì¥í•©ë‹ˆë‹¤."""
    #     try:
    #         import json
    #         import os
    #         
    #         cache_dir = os.path.join(os.path.dirname(__file__), 'cache')
    #         os.makedirs(cache_dir, exist_ok=True)
    #         
    #         cache_file = os.path.join(cache_dir, 'unit_cache.json')
    #         
    #         # ê¸°ì¡´ ìºì‹œ ë¡œë“œ
    #         cache_data = {}
    #         if os.path.exists(cache_file):
    #             with open(cache_file, 'r', encoding='utf-8') as f:
    #                 cache_data = json.load(f)
    #         
    #         # ìƒˆ ë°ì´í„° ì¶”ê°€
    #         cache_key = f"{cate_cd}_{item_cd}"
    #         cache_data[cache_key] = unit
    #         
    #         # íŒŒì¼ì— ì €ì¥
    #         with open(cache_file, 'w', encoding='utf-8') as f:
    #             json.dump(cache_data, f, ensure_ascii=False, indent=2)
    #             
    #         log(f"íŒŒì¼ ìºì‹œ ì €ì¥: {cache_key} -> {unit}")
    #         
    #     except Exception as e:
    #         log(f"íŒŒì¼ ìºì‹œ ì €ì¥ ì˜¤ë¥˜: {str(e)}", "ERROR")

    def _get_hardcoded_unit(self, major_name, middle_name, sub_name, spec_name=None):
        """JSONC íŒŒì¼ì—ì„œ ë‹¨ìœ„ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤ - ë™ì  ë§¤í•‘ ë°©ì‹"""
        try:
            print(f"[DEBUG] JSONC íŒŒì¼ì—ì„œ ë‹¨ìœ„ ì¡°íšŒ ì‹œì‘: major='{major_name}', middle='{middle_name}', sub='{sub_name}'")
            print(f"[DEBUG] UNIT_DATA_FROM_JSONC keys: {list(UNIT_DATA_FROM_JSONC.keys())}")
            
            # 1. ë¨¼ì € ì •í™•í•œ ëŒ€ë¶„ë¥˜ëª…ìœ¼ë¡œ ì°¾ê¸° ì‹œë„
            if major_name in UNIT_DATA_FROM_JSONC:
                print(f"[DEBUG] ëŒ€ë¶„ë¥˜ '{major_name}' ì •í™• ë§¤ì¹­ ì„±ê³µ")
                major_data = UNIT_DATA_FROM_JSONC[major_name]
            else:
                # 2. ì •í™•í•œ ë§¤ì¹­ì´ ì‹¤íŒ¨í•˜ë©´ JSONC íŒŒì¼ì˜ ëª¨ë“  ëŒ€ë¶„ë¥˜ì—ì„œ ì¤‘ë¶„ë¥˜ì™€ ì†Œë¶„ë¥˜ë¥¼ ì°¾ì•„ì„œ ë§¤í•‘
                print(f"[DEBUG] ëŒ€ë¶„ë¥˜ '{major_name}' ì •í™• ë§¤ì¹­ ì‹¤íŒ¨, ë™ì  ë§¤í•‘ ì‹œë„")
                found_major = None
                
                for jsonc_major_name, jsonc_major_data in UNIT_DATA_FROM_JSONC.items():
                    if isinstance(jsonc_major_data, dict):
                        # ì¤‘ë¶„ë¥˜ì—ì„œ ì°¾ê¸°
                        if middle_name in jsonc_major_data:
                            middle_data = jsonc_major_data[middle_name]
                            if isinstance(middle_data, dict) and sub_name in middle_data:
                                found_major = jsonc_major_name
                                log(f"[DEBUG] ë™ì  ë§¤í•‘ ì„±ê³µ: '{major_name}' -> '{found_major}' (ì¤‘ë¶„ë¥˜ '{middle_name}', ì†Œë¶„ë¥˜ '{sub_name}' ê¸°ì¤€)")
                                break
                        
                        # ì¤‘ë¶„ë¥˜ ì •í™• ë§¤ì¹­ì´ ì‹¤íŒ¨í•˜ë©´ ìœ ì‚¬ ë§¤ì¹­ ì‹œë„
                        for jsonc_middle_name, jsonc_middle_data in jsonc_major_data.items():
                            if isinstance(jsonc_middle_data, dict):
                                # ì¤‘ë¶„ë¥˜ ìœ ì‚¬ ë§¤ì¹­
                                middle_without_hanja = re.sub(r'\([^)]*\)', '', middle_name).strip()
                                jsonc_middle_without_hanja = re.sub(r'\([^)]*\)', '', jsonc_middle_name).strip()
                                
                                if (middle_without_hanja == jsonc_middle_without_hanja or 
                                    middle_name == jsonc_middle_name):
                                    # ì†Œë¶„ë¥˜ì—ì„œ ì°¾ê¸°
                                    if sub_name in jsonc_middle_data:
                                        found_major = jsonc_major_name
                                        middle_name = jsonc_middle_name  # ë§¤í•‘ëœ ì¤‘ë¶„ë¥˜ëª…ìœ¼ë¡œ ì—…ë°ì´íŠ¸
                                        log(f"[DEBUG] ë™ì  ë§¤í•‘ ì„±ê³µ (ì¤‘ë¶„ë¥˜ ìœ ì‚¬ë§¤ì¹­): '{major_name}' -> '{found_major}', ì¤‘ë¶„ë¥˜ '{middle_name}' -> '{jsonc_middle_name}'")
                                        break
                                    
                                    # ì†Œë¶„ë¥˜ ìœ ì‚¬ ë§¤ì¹­ ì‹œë„
                                    for jsonc_sub_name in jsonc_middle_data.keys():
                                        sub_without_hanja = re.sub(r'\([^)]*\)', '', sub_name).strip()
                                        jsonc_sub_without_hanja = re.sub(r'\([^)]*\)', '', jsonc_sub_name).strip()
                                        
                                        if (sub_without_hanja == jsonc_sub_without_hanja or 
                                            sub_name == jsonc_sub_name):
                                            found_major = jsonc_major_name
                                            middle_name = jsonc_middle_name  # ë§¤í•‘ëœ ì¤‘ë¶„ë¥˜ëª…ìœ¼ë¡œ ì—…ë°ì´íŠ¸
                                            sub_name = jsonc_sub_name  # ë§¤í•‘ëœ ì†Œë¶„ë¥˜ëª…ìœ¼ë¡œ ì—…ë°ì´íŠ¸
                                            log(f"[DEBUG] ë™ì  ë§¤í•‘ ì„±ê³µ (ì†Œë¶„ë¥˜ ìœ ì‚¬ë§¤ì¹­): '{major_name}' -> '{found_major}', ì¤‘ë¶„ë¥˜ -> '{jsonc_middle_name}', ì†Œë¶„ë¥˜ -> '{jsonc_sub_name}'")
                                            break
                            
                            if found_major:
                                break
                    
                    if found_major:
                        break
                
                if found_major:
                    major_data = UNIT_DATA_FROM_JSONC[found_major]
                    major_name = found_major  # ë§¤í•‘ëœ ëŒ€ë¶„ë¥˜ëª…ìœ¼ë¡œ ì—…ë°ì´íŠ¸
                else:
                    log(f"[DEBUG] ë™ì  ë§¤í•‘ ì‹¤íŒ¨: ëŒ€ë¶„ë¥˜ '{major_name}'ì— í•´ë‹¹í•˜ëŠ” ì¤‘ë¶„ë¥˜ '{middle_name}', ì†Œë¶„ë¥˜ '{sub_name}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    return None
            
            # UNIT_DATA_FROM_JSONCì—ì„œ í•´ë‹¹ ê²½ë¡œì˜ ë‹¨ìœ„ ì •ë³´ ì°¾ê¸°
            if major_name in UNIT_DATA_FROM_JSONC:
                log(f"[DEBUG] ëŒ€ë¶„ë¥˜ '{major_name}' ë°œê²¬")
                major_data = UNIT_DATA_FROM_JSONC[major_name]
                
                if middle_name in major_data:
                    log(f"[DEBUG] ì¤‘ë¶„ë¥˜ '{middle_name}' ë°œê²¬")
                    middle_data = major_data[middle_name]
                    
                    log(f"[DEBUG] ì¤‘ë¶„ë¥˜ ë°ì´í„° í‚¤ë“¤: {list(middle_data.keys())}")
                    
                    # ì •í™•í•œ ë§¤ì¹­ ì‹œë„
                    if isinstance(middle_data, dict) and sub_name in middle_data:
                        log(f"[DEBUG] ì†Œë¶„ë¥˜ '{sub_name}' ì •í™• ë§¤ì¹­ ì„±ê³µ")
                        unit_data = middle_data[sub_name]
                        
                        # ë‹¨ìˆœ ë¬¸ìì—´ì¸ ê²½ìš° (ê¸°ì¡´ ë°©ì‹)
                        if isinstance(unit_data, str):
                                log(f"JSONC ë‹¨ìœ„ ì •ë³´ ë°œê²¬: {major_name} > {middle_name} > {sub_name} = {unit_data}")
                                return unit_data
                        
                        # ê°ì²´ì¸ ê²½ìš° (ìƒˆë¡œìš´ JSONC êµ¬ì¡°)
                        elif isinstance(unit_data, dict):
                            # unit í‚¤ê°€ ìˆëŠ” ê²½ìš° ì§ì ‘ ë°˜í™˜
                            if 'unit' in unit_data:
                                unit = unit_data['unit']
                                log(f"JSONC ë‹¨ìœ„ ì •ë³´ ë°œê²¬: {major_name} > {middle_name} > {sub_name} = {unit}")
                                return unit
                            
                            # ê·œê²©ëª…ì´ ì œê³µëœ ê²½ìš° specificationsì—ì„œ ì°¾ê¸°
                            if spec_name and 'specifications' in unit_data:
                                specifications = unit_data['specifications']
                                
                                # ì •í™•í•œ ê·œê²©ëª… ë§¤ì¹­
                                if spec_name in specifications:
                                    unit = specifications[spec_name]
                                    log(f"JSONC ê·œê²©ë³„ ë‹¨ìœ„ ì •ë³´ ë°œê²¬: {major_name} > {middle_name} > {sub_name} > {spec_name} = {unit}")
                                    return unit
                                
                                # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„
                                for spec_key, spec_unit in specifications.items():
                                    if spec_name in spec_key or spec_key in spec_name:
                                        log(f"JSONC ê·œê²©ë³„ ë‹¨ìœ„ ì •ë³´ ë¶€ë¶„ ë§¤ì¹­: {major_name} > {middle_name} > {sub_name} > {spec_name} â‰ˆ {spec_key} = {spec_unit}")
                                        return spec_unit
                            
                            # ê¸°ë³¸ ë‹¨ìœ„ ë°˜í™˜
                            if 'default' in unit_data:
                                default_unit = unit_data['default']
                                log(f"JSONC ê¸°ë³¸ ë‹¨ìœ„ ì •ë³´ ì‚¬ìš©: {major_name} > {middle_name} > {sub_name} = {default_unit}")
                                return default_unit
                    else:
                        # ì •í™•í•œ ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ìœ ì‚¬ ë§¤ì¹­ ì‹œë„
                        log(f"[DEBUG] ì •í™•í•œ ë§¤ì¹­ ì‹¤íŒ¨, ìœ ì‚¬ ë§¤ì¹­ ì‹œë„")
                        for key in middle_data.keys():
                            # í•œì ë¶€ë¶„ì„ ì œê±°í•˜ê³  ë¹„êµ
                            key_without_hanja = re.sub(r'\([^)]*\)', '', key).strip()
                            sub_without_hanja = re.sub(r'\([^)]*\)', '', sub_name).strip()
                            
                            log(f"[DEBUG] í‚¤ ë¹„êµ: '{key}' vs '{sub_name}'")
                            log(f"[DEBUG] í•œì ì œê±° í›„: '{key_without_hanja}' vs '{sub_without_hanja}'")
                            
                            # ì •ê·œí™”ëœ ë¬¸ìì—´ë¡œ ë¹„êµ
                            if key_without_hanja == sub_without_hanja or key == sub_name:
                                log(f"[DEBUG] ìœ ì‚¬ ë§¤ì¹­ ì„±ê³µ: '{key}' â‰ˆ '{sub_name}'")
                                unit_data = middle_data[key]
                                
                                if isinstance(unit_data, str):
                                    log(f"JSONC ë‹¨ìœ„ ì •ë³´ ë°œê²¬ (ìœ ì‚¬ë§¤ì¹­): {major_name} > {middle_name} > {key} = {unit_data}")
                                    return unit_data
                                elif isinstance(unit_data, dict) and 'default' in unit_data:
                                    default_unit = unit_data['default']
                                    log(f"JSONC ê¸°ë³¸ ë‹¨ìœ„ ì •ë³´ ì‚¬ìš© (ìœ ì‚¬ë§¤ì¹­): {major_name} > {middle_name} > {key} = {default_unit}")
                                    return default_unit
                        
                        log(f"[DEBUG] ì†Œë¶„ë¥˜ '{sub_name}' ë§¤ì¹­ ì‹¤íŒ¨. ì‚¬ìš© ê°€ëŠ¥í•œ ì†Œë¶„ë¥˜: {list(middle_data.keys()) if isinstance(middle_data, dict) else 'dictê°€ ì•„ë‹˜'}")
                else:
                    log(f"[DEBUG] ì¤‘ë¶„ë¥˜ '{middle_name}' ì—†ìŒ. ì‚¬ìš© ê°€ëŠ¥í•œ ì¤‘ë¶„ë¥˜: {list(major_data.keys())}")
            else:
                log(f"[DEBUG] ëŒ€ë¶„ë¥˜ '{major_name}' ì—†ìŒ. ì‚¬ìš© ê°€ëŠ¥í•œ ëŒ€ë¶„ë¥˜: {list(UNIT_DATA_FROM_JSONC.keys())}")
            
            log(f"JSONC ë‹¨ìœ„ ì •ë³´ ì—†ìŒ: {major_name} > {middle_name} > {sub_name}")
            return None
            
        except Exception as e:
            log(f"JSONC ë‹¨ìœ„ ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}", "ERROR")
            return None

    def _build_included_categories_cache(self):
        """JSONC íŒŒì¼ì—ì„œ ì£¼ì„ ì²˜ë¦¬ë˜ì§€ ì•Šì€ í•­ëª©ë§Œ ìºì‹±í•©ë‹ˆë‹¤."""
        included_cache = {}
        
        try:
            json_data = UNIT_DATA_FROM_JSONC
            
            for major_name, major_data in json_data.items():
                if major_name not in included_cache:
                    included_cache[major_name] = {}
                
                for middle_name, middle_data in major_data.items():
                    if middle_name not in included_cache[major_name]:
                        included_cache[major_name][middle_name] = []
                    
                    # ì¤‘ë¶„ë¥˜ ë°ì´í„°ê°€ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°ì—ë§Œ ì²˜ë¦¬
                    if isinstance(middle_data, dict):
                        for sub_name in middle_data.keys():
                            included_cache[major_name][middle_name].append(sub_name)
            
            log(f"JSONC í¬í•¨ í•­ëª© ìºì‹± ì™„ë£Œ: {sum(len(subs) for subs in included_cache.values())}ê°œ ì†Œë¶„ë¥˜")
            return included_cache
            
        except Exception as e:
            log(f"JSONC í¬í•¨ í•­ëª© ìºì‹± ì˜¤ë¥˜: {str(e)}", "ERROR")
            return {}

    def _is_subcategory_included_in_jsonc(self, major_name, middle_name, sub_name):
        """JSONC íŒŒì¼ì—ì„œ í•´ë‹¹ ì†Œë¶„ë¥˜ê°€ ì£¼ì„ ì²˜ë¦¬ë˜ì§€ ì•Šê³  í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
        try:
            # ì´ë¯¸ ìºì‹±ëœ ì „ì—­ ë°ì´í„° ì‚¬ìš©
            json_data = UNIT_DATA_FROM_JSONC
            
            # ëŒ€ë¶„ë¥˜ í™•ì¸
            if major_name not in json_data:
                log(f"[DEBUG] JSONCì— ëŒ€ë¶„ë¥˜ '{major_name}' ì—†ìŒ")
                return False
            
            major_data = json_data[major_name]
            
            # ì¤‘ë¶„ë¥˜ í™•ì¸
            if middle_name not in major_data:
                log(f"[DEBUG] JSONCì— ì¤‘ë¶„ë¥˜ '{middle_name}' ì—†ìŒ")
                return False
            
            middle_data = major_data[middle_name]
            
            # ì†Œë¶„ë¥˜ í™•ì¸
            if sub_name not in middle_data:
                log(f"[DEBUG] JSONCì— ì†Œë¶„ë¥˜ '{sub_name}' ì—†ìŒ")
                return False
            
            # ì†Œë¶„ë¥˜ê°€ ì¡´ì¬í•˜ë©´ í¬í•¨ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼
            log(f"[DEBUG] JSONCì— ì†Œë¶„ë¥˜ '{sub_name}' í¬í•¨ë¨")
            return True
            
        except Exception as e:
            log(f"JSONC í¬í•¨ ì—¬ë¶€ í™•ì¸ ì˜¤ë¥˜: {str(e)}", "ERROR")
            return False

    def _load_unit_from_file_cache(self, cate_cd, item_cd):
        """íŒŒì¼ ìºì‹œì—ì„œ ë‹¨ìœ„ ì •ë³´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            import json
            import os
            
            cache_file = os.path.join(os.path.dirname(__file__), 'cache', 'unit_cache.json')
            
            if not os.path.exists(cache_file):
                return None
                
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            cache_key = f"{cate_cd}_{item_cd}"
            unit = cache_data.get(cache_key)
            
            if unit:
                log(f"íŒŒì¼ ìºì‹œì—ì„œ ë‹¨ìœ„ ì •ë³´ ë¡œë“œ: {cache_key} -> {unit}")
                return unit
                
        except Exception as e:
            log(f"íŒŒì¼ ìºì‹œ ë¡œë“œ ì˜¤ë¥˜: {str(e)}", "ERROR")
            
        return None

    async def _process_specs_parallel(self, spec_list, raw_item_data,
                                     major_name, middle_name, sub_name,
                                     sub_url):
        """ì—¬ëŸ¬ ê·œê²©ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ëŠ” ë©”ì„œë“œ"""
        semaphore = asyncio.Semaphore(1)  # ìµœëŒ€ 1ê°œ ë™ì‹œ ì²˜ë¦¬ë¡œ ê°ì†Œ

        async def process_spec_with_new_page(spec):
            async with semaphore:
                try:
                    # ìƒˆë¡œìš´ í˜ì´ì§€ ìƒì„± (ìƒˆ íƒ­ ë°©ì‹)
                    new_page = await self.context.new_page()
                    # ì „ë‹¬ë°›ì€ sub_url ì‚¬ìš© (ì´ë¯¸ ì˜¬ë°”ë¥¸ í˜•ì‹ìœ¼ë¡œ êµ¬ì„±ë¨)
                    await new_page.goto(sub_url, wait_until='networkidle')
                    await new_page.wait_for_load_state(
                        'networkidle', timeout=60000)
                    await new_page.wait_for_selector(
                        'body', timeout=5000)

                    # 'ë¬¼ê°€ì¶”ì´ ë³´ê¸°' íƒ­ìœ¼ë¡œ ì´ë™ (ê°œì„ ëœ ì¬ì‹œë„ ë¡œì§)
                    for retry in range(5):
                        try:
                            # ì‹¤ì œ HTML êµ¬ì¡°ì— ë§ëŠ” ì…€ë ‰í„°ë“¤ (ì‚¬ìš©ì ì œê³µ êµ¬ì¡° ê¸°ë°˜)
                            selectors = [
                                'a[href*="detail_change.asp"] span:has-text("ë¬¼ê°€ì¶”ì´ ë³´ê¸°")',
                                'a[href*="detail_change.asp"]:has-text("ë¬¼ê°€ì¶”ì´ ë³´ê¸°")',
                                'a[href*="detail_change.asp"]',
                                'a:has-text("ë¬¼ê°€ì¶”ì´ ë³´ê¸°")',
                                'text=ë¬¼ê°€ì¶”ì´ ë³´ê¸°',
                                'a:contains("ë¬¼ê°€ì¶”ì´")',
                                '.tab-menu a:has-text("ë¬¼ê°€ì¶”ì´")',
                                'ul.tab-list a:has-text("ë¬¼ê°€ì¶”ì´")'
                            ]
                            
                            clicked = False
                            for selector in selectors:
                                try:
                                    # í˜ì´ì§€ ë¡œë”© ìƒíƒœ í™•ì¸
                                    await new_page.wait_for_load_state('domcontentloaded', timeout=10000)
                                    
                                    # ì…€ë ‰í„° ëŒ€ê¸° ë° í´ë¦­
                                    await new_page.wait_for_selector(selector, timeout=20000)
                                    await new_page.click(selector, timeout=60000)
                                    clicked = True
                                    log(f"    âœ… ë¬¼ê°€ì¶”ì´ ë³´ê¸° íƒ­ í´ë¦­ ì„±ê³µ (ì…€ë ‰í„°: {selector})")
                                    break
                                except Exception as click_error:
                                    log(f"    âš ï¸ ì…€ë ‰í„° {selector} í´ë¦­ ì‹¤íŒ¨: {str(click_error)}")
                                    continue
                            
                            if not clicked:
                                # ë””ë²„ê¹…: í˜ì´ì§€ì˜ ì‹¤ì œ HTML êµ¬ì¡° í™•ì¸
                                try:
                                    # ëª¨ë“  ë§í¬ ìš”ì†Œ ì°¾ê¸°
                                    all_links = await new_page.query_selector_all('a')
                                    log(f"    ğŸ” ë””ë²„ê¹…: í˜ì´ì§€ì—ì„œ ë°œê²¬ëœ ì´ ë§í¬ ìˆ˜: {len(all_links)}")
                                    
                                    # ë¬¼ê°€ì¶”ì´ ê´€ë ¨ ë§í¬ ì°¾ê¸°
                                    price_trend_links = []
                                    for link in all_links:
                                        try:
                                            href = await link.get_attribute('href')
                                            text = await link.inner_text()
                                            if href and ('detail_change' in href or 'ë¬¼ê°€ì¶”ì´' in text):
                                                price_trend_links.append(f"href='{href}', text='{text.strip()}'")
                                        except Exception:
                                            continue
                                    
                                    if price_trend_links:
                                        log(f"    ğŸ” ë°œê²¬ëœ ë¬¼ê°€ì¶”ì´ ê´€ë ¨ ë§í¬ë“¤:")
                                        for link_info in price_trend_links[:5]:  # ìµœëŒ€ 5ê°œë§Œ ì¶œë ¥
                                            log(f"      - {link_info}")
                                    else:
                                        log(f"    ğŸ” ë¬¼ê°€ì¶”ì´ ê´€ë ¨ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                                        
                                    # í˜„ì¬ í˜ì´ì§€ URL í™•ì¸
                                    current_url = new_page.url
                                    log(f"    ğŸ” í˜„ì¬ í˜ì´ì§€ URL: {current_url}")
                                    
                                except Exception as debug_error:
                                    log(f"    ğŸ” ë””ë²„ê¹… ì¤‘ ì˜¤ë¥˜: {str(debug_error)}")
                                
                                # ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì¬ì‹œë„ (ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸ í¬í•¨)
                                try:
                                    # ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸
                                    login_check = await new_page.query_selector('a[href*="logout"]')
                                    if not login_check:
                                        log(f"    âš ï¸ ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨ - ì¬ë¡œê·¸ì¸ í•„ìš”í•  ìˆ˜ ìˆìŒ")
                                    
                                    link_name = 'ë¬¼ê°€ì¶”ì´ ë³´ê¸°'
                                    link_locator = new_page.get_by_role('link', name=link_name)
                                    await link_locator.click(timeout=60000)
                                    clicked = True
                                    log(f"    âœ… ë¬¼ê°€ì¶”ì´ ë³´ê¸° íƒ­ í´ë¦­ ì„±ê³µ (ê¸°ì¡´ ë°©ì‹)")
                                except Exception as fallback_error:
                                    log(f"    âŒ ê¸°ì¡´ ë°©ì‹ í´ë¦­ë„ ì‹¤íŒ¨: {str(fallback_error)}")
                            
                            if clicked:
                                await new_page.wait_for_selector("#ITEM_SPEC_CD", timeout=60000)
                                break
                            else:
                                raise Exception("ëª¨ë“  ì…€ë ‰í„°ë¡œ ë¬¼ê°€ì¶”ì´ ë³´ê¸° íƒ­ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                        except Exception as e:
                            if retry == 4:
                                raise e
                            log(f"    âš ï¸ ë¬¼ê°€ì¶”ì´ ë³´ê¸° íƒ­ í´ë¦­ ì¬ì‹œë„ {retry + 1}/5: {e}", "WARNING")
                            await asyncio.sleep(5)
                            await new_page.reload()
                            await new_page.wait_for_load_state('domcontentloaded', timeout=30000)
                            await new_page.wait_for_load_state('networkidle', timeout=60000)

                    # ì„ì‹œ ë°ì´í„° êµ¬ì¡°
                    temp_data = {
                        'major_category_name': major_name,
                        'middle_category_name': middle_name,
                        'sub_category_name': sub_name,
                        'spec_data': []
                    }

                    # ê·œê²© ì²˜ë¦¬
                    await self._process_single_spec(new_page, spec, temp_data)

                    # í˜ì´ì§€ë¥¼ í’€ì— ë°˜í™˜
                    try:
                        await self._return_page_to_pool(new_page)
                    except Exception as return_error:
                        log(f"í˜ì´ì§€ í’€ ë°˜í™˜ ì‹¤íŒ¨: {return_error}")
                        try:
                            await new_page.close()
                        except:
                            pass
                    return temp_data['spec_data']

                except Exception as e:
                    error_msg = (f"    - ë³‘ë ¬ ì²˜ë¦¬ ì¤‘ ê·œê²© '{spec['name']}' ì˜¤ë¥˜ "
                                f"[ëŒ€ë¶„ë¥˜: {major_name}, ì¤‘ë¶„ë¥˜: {middle_name}, "
                                f"ì†Œë¶„ë¥˜: {sub_name}]: {str(e)}")
                    log(error_msg)
                    # ì˜ˆì™¸ ë°œìƒ ì‹œì—ë„ í˜ì´ì§€ë¥¼ í’€ì— ë°˜í™˜
                    try:
                        await self._return_page_to_pool(new_page)
                    except Exception as return_error:
                        log(f"ì˜ˆì™¸ ì²˜ë¦¬ ì¤‘ í˜ì´ì§€ í’€ ë°˜í™˜ ì‹¤íŒ¨: {return_error}")
                        try:
                            await new_page.close()
                        except:
                            pass
                    return []

        # ëª¨ë“  ê·œê²©ì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬
        # ëª¨ë“  ê·œê²©ì— ëŒ€í•œ ë³‘ë ¬ ì²˜ë¦¬ íƒœìŠ¤í¬ ìƒì„±
        tasks = [process_spec_with_new_page(spec)
                 for spec in spec_list]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # ê²°ê³¼ ë³‘í•©
        for result in results:
            if isinstance(result, list) and result:
                raw_item_data['spec_data'].extend(result)

    async def _process_single_spec(self, page, spec, raw_item_data):
        """ë‹¨ì¼ ê·œê²©ì— ëŒ€í•œ ë°ì´í„° ì²˜ë¦¬"""
        spec_value = spec['value']
        spec_name = spec['name']
        log(f"    - ê·œê²©: '{spec_name}' ì¡°íšŒ ì¤‘...")

        max_retries = 3
        for attempt in range(max_retries):
            try:
                # ê·œê²© ì„ íƒ
                # ê·œê²© ì„ íƒ
                spec_locator = page.locator('#ITEM_SPEC_CD')
                await spec_locator.select_option(value=spec_value)
                await page.wait_for_load_state(
                    'networkidle', timeout=10000)

                # ê¸°ê°„ ì„ íƒ
                year_locator = page.locator('#DATA_YEAR_F')
                await year_locator.select_option(value=self.start_year)
                month_locator = page.locator('#DATA_MONTH_F')
                await month_locator.select_option(
                    value=self.start_month)
                await page.wait_for_load_state(
                    'networkidle', timeout=10000)

                # ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­ (ì¬ì‹œë„ ë¡œì§ ì¶”ê°€)
                search_selector = 'form[name="sForm"] input[type="image"]'
                search_button = page.locator(search_selector)
                
                # ì¬ì‹œë„ ë¡œì§ (ìµœëŒ€ 3íšŒ ì‹œë„)
                for search_attempt in range(3):
                    try:
                        await search_button.click(timeout=15000)
                        await page.wait_for_load_state(
                            'networkidle', timeout=20000)
                        break
                    except Exception as search_e:
                        if search_attempt == 2:  # ë§ˆì§€ë§‰ ì‹œë„
                            raise search_e
                        log(f"        - ê²€ìƒ‰ ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨ (ì‹œë„ {search_attempt + 1}/3), 2ì´ˆ í›„ ì¬ì‹œë„...")
                        await asyncio.sleep(2)
                break

            except Exception as e:
                if attempt < max_retries - 1:
                    attempt_num = attempt + 1
                    log(f"    - ê·œê²© '{spec_name}' ì‹œë„ {attempt_num} "
                        f"ì‹¤íŒ¨, ì¬ì‹œë„ ì¤‘...")
                    await page.wait_for_timeout(1000)
                    continue
                else:
                    log(f"    - ê·œê²© '{spec_name}' ìµœì¢… ì‹¤íŒ¨: "
                        f"{str(e)}")
                    return

        # ë°ì´í„° í…Œì´ë¸” íŒŒì‹±
        try:
            # ì¬ì‹œë„ ë¡œì§ìœ¼ë¡œ í…Œì´ë¸” ëŒ€ê¸° (ìµœëŒ€ 3íšŒ ì‹œë„)
            for table_attempt in range(3):
                try:
                    await page.wait_for_selector(
                        "#priceTrendDataArea tr", timeout=15000)
                    break
                except Exception as table_e:
                    if table_attempt == 2:  # ë§ˆì§€ë§‰ ì‹œë„
                        raise table_e
                    log(f"        - í…Œì´ë¸” ë¡œë”© ì‹¤íŒ¨ (ì‹œë„ {table_attempt + 1}/3), 2ì´ˆ í›„ ì¬ì‹œë„...")
                    await asyncio.sleep(2)

            # ì „ì²´ í…Œì´ë¸” HTMLì„ ë¡œê·¸ë¡œ ì¶œë ¥í•´ì„œ êµ¬ì¡° í™•ì¸
            table_html = await page.locator("#priceTrendDataArea").inner_html()
            log(f"    - ê·œê²© '{spec_name}': í…Œì´ë¸” HTML êµ¬ì¡°:\n{table_html[:500]}...")

            header_elements = await page.locator(
            "#priceTrendDataArea th").all()
            if not header_elements:
                log(f"    - ê·œê²© '{spec_name}': ë°ì´í„° í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return

            # ì²« ë²ˆì§¸ í—¤ë”ëŠ” 'êµ¬ë¶„'ì´ë¯€ë¡œ ì œì™¸
            dates = [await h.inner_text() for h in header_elements[1:]]
            log(f"    - ê·œê²© '{spec_name}': í—¤ë” ë°ì´í„° = {dates}")

            # ì²« ë²ˆì§¸ ì§€ì—­ ë°ì´í„° í–‰ë§Œ ì¶”ì¶œ (ì˜ˆ: 'ì„œâ‘ ìš¸')
            data_rows = await page.locator("#priceTrendDataArea tr").all()
            log(f"    - ê·œê²© '{spec_name}': ì´ {len(data_rows)}ê°œ í–‰ ë°œê²¬")
            if len(data_rows) < 2:
                log(f"    - ê·œê²© '{spec_name}': ë°ì´í„° í–‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return

            first_row_tds = await data_rows[1].locator("td").all()
            if not first_row_tds:
                log(f"    - ê·œê²© '{spec_name}': ë°ì´í„° ì…€ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return

            region = await first_row_tds[0].inner_text()
            prices = [await td.inner_text() for td in first_row_tds[1:]]
            log(f"    - ê·œê²© '{spec_name}': ì§€ì—­ = {region}, "
                f"ê°€ê²© ë°ì´í„° = {prices}")

            # í—¤ë” ì •ë³´ ì¶”ì¶œ (í…Œì´ë¸” êµ¬ì¡° íŒŒì•…ìš©)
            header_cells = await data_rows[0].locator("th").all()
            headers = []
            for i in range(1, len(header_cells)):  # ì²« ë²ˆì§¸ ì»¬ëŸ¼(êµ¬ë¶„) ì œì™¸
                header_text = await header_cells[i].inner_text()
                # ë™ê·¸ë¼ë¯¸ ìˆ«ìë¥¼ ì¼ë°˜ ìˆ«ìë¡œ ë³€í™˜ (â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘© ë“± -> 1,2,3)
                circle_to_num = {
                    'â‘ ': '1', 'â‘¡': '2', 'â‘¢': '3', 'â‘£': '4', 'â‘¤': '5',
                    'â‘¥': '6', 'â‘¦': '7', 'â‘§': '8', 'â‘¨': '9', 'â‘©': '10'
                }
                cleaned_header = header_text.strip()
                for circle, num in circle_to_num.items():
                    cleaned_header = cleaned_header.replace(circle, num)

                # ë‚ ì§œ í˜•ì‹ ê²€ì¦
                # (YYYY.M ë˜ëŠ” YYYY. M í˜•íƒœë§Œ í—ˆìš©)
                if self._is_valid_date_header(cleaned_header):
                    headers.append(cleaned_header)
                else:
                    log(f"    - ì˜ëª»ëœ ë‚ ì§œ í—¤ë” ì œì™¸: "
                        f"'{cleaned_header}' (ì›ë³¸: '{header_text}')")

            log(f"    - ê·œê²© '{spec_name}': "
                f"í—¤ë” ë°ì´í„° = {headers}")

            price_list = []
            # í…Œì´ë¸” êµ¬ì¡° ë¶„ì„: í—¤ë”ê°€ ë‚ ì§œì´ê³  í–‰ì´ ì§€ì—­ë³„ ë°ì´í„°ì¸ êµ¬ì¡°
            # ê° ì§€ì—­(í–‰)ì— ëŒ€í•´ ì²˜ë¦¬
            # í—¤ë” ì œì™¸í•˜ê³  ëª¨ë“  í–‰ ì²˜ë¦¬
            for i in range(1, len(data_rows)):
                try:
                    row_tds = await data_rows[i].locator("td").all()
                    if len(row_tds) >= 2:
                        # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì€ ì§€ì—­ëª…
                        region_str = await row_tds[0].inner_text()
                        # ë™ê·¸ë¼ë¯¸ ìˆ«ìë¥¼ ì¼ë°˜ ìˆ«ìë¡œ ë³€í™˜ (â‘ â‘¡â‘¢ ë“± -> 1,2,3)
                        circle_to_num = {
                            'â‘ ': '1', 'â‘¡': '2', 'â‘¢': '3', 'â‘£': '4',
                            'â‘¤': '5', 'â‘¥': '6', 'â‘¦': '7', 'â‘§': '8',
                            'â‘¨': '9', 'â‘©': '10'
                        }
                        clean_region = region_str.strip()
                        for circle, num in circle_to_num.items():
                            clean_region = clean_region.replace(
                                circle, num)

                        # ê° ë‚ ì§œë³„ ê°€ê²© ì²˜ë¦¬
                        for j, date_header in enumerate(headers):
                            if j + 1 < len(row_tds):
                                price_str = await row_tds[j + 1].inner_text()
                                # ê°€ê²© ë°ì´í„° ìœ íš¨ì„± ê²€ì¦ ê°•í™”
                                price_stripped = price_str.strip()
                                if price_stripped and price_stripped != '-':
                                    # ì‰¼í‘œê°€ í¬í•¨ëœ ìˆ«ìì¸ì§€ í™•ì¸
                                    clean_price = price_stripped.replace(
                                        ',', '')
                                    is_valid_price = (
                                        clean_price.isdigit() and
                                        int(clean_price) > 0)
                                    if is_valid_price:
                                        # ë‚ ì§œ íŒŒì‹± (YYYY. M í˜•íƒœ)
                                        try:
                                            is_valid_header = (
                                                self._is_valid_date_header(
                                                    date_header))
                                            if is_valid_header:
                                                year_month = (
                                                    date_header.strip()
                                                    .replace(' ', ''))
                                                if '.' in year_month:
                                                    year, month = year_month.split('.')
                                                    date_obj = datetime(
                                                        int(year),
                                                        int(month), 1)
                                                    price_data = {
                                                        'date': date_obj,
                                                        'region': clean_region,
                                                        'price': price_str.strip()
                                                    }
                                                    price_list.append(price_data)
                                                    # ìœ íš¨í•œ ê°€ê²© ë°ì´í„° ì¶”ê°€ ë¡œê·¸
                                                    log(
                                                        f"    - ìœ íš¨í•œ ê°€ê²© ë°ì´í„° ì¶”ê°€: "
                                                        f"{clean_region} "
                                                        f"({date_header}) = "
                                                        f"{price_str.strip()}")
                                            else:
                                                # ì˜ëª»ëœ ë‚ ì§œ í˜•ì‹ ì œì™¸ ë¡œê·¸
                                                log(
                                                    f"    - ì˜ëª»ëœ ë‚ ì§œ í˜•ì‹ ì œì™¸: "
                                                    f"{date_header}")
                                        except Exception as date_parse_error:
                                            # ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜ ë¡œê·¸
                                            log(
                                                f"    - ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: "
                                                f"{date_header} - "
                                                f"{str(date_parse_error)}")
                except Exception as row_error:
                    # í–‰ íŒŒì‹± ì˜¤ë¥˜ ë¡œê·¸
                    log(f"    - í–‰ {i} íŒŒì‹± ì˜¤ë¥˜: {str(row_error)}")
                    continue

            if price_list:
                raw_item_data['spec_data'].append({
                    'specification_name': spec_name,
                    'prices': price_list
                })
                log(f"    - ê·œê²© '{spec_name}': {len(price_list)}ê°œ ê°€ê²© ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
            else:
                log(f"    - ê·œê²© '{spec_name}': ìœ íš¨í•œ ê°€ê²© ë°ì´í„° ì—†ìŒ")

        except Exception as e:
            log(f"    - ê·œê²© '{spec_name}' ë°ì´í„° íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
            return


# --- 4. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ---
# <<< íŒŒì¼ ë§¨ ì•„ë˜ ë¶€ë¶„ì„ ì´ ì½”ë“œë¡œ ì „ì²´ êµì²´ (5/5) >>>

async def main():
    # ë¡œê·¸ íŒŒì¼ ì„¤ì •
    log_file_path = "c:\\JAJE\\materials-dashboard\\crawler\\kpi_crawler_debug.log"
    log("DEBUG: main í•¨ìˆ˜ ì‹œì‘ (kpi_crawler.py)", "DEBUG")
    log(f"DEBUG: ë¡œê·¸ íŒŒì¼ ê²½ë¡œ: {log_file_path}", "DEBUG")
    """ë©”ì¸ ì‹¤í–‰ ë¡œì§: ëª…ë ¹í–‰ ì¸ì íŒŒì‹± ë° í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
    # ëª…ë ¹í–‰ ì¸ì íŒŒì‹± - ë‘ ê°€ì§€ ë°©ì‹ ì§€ì›
    # ë°©ì‹ 1: --major="ê³µí†µìì¬" --middle="ë¹„ì² ê¸ˆì†" --sub="ì•Œë£¨ë¯¸ëŠ„" --mode="sub_only"
    # ë°©ì‹ 2: --major ê³µí†µìì¬ --middle ë¹„ì² ê¸ˆì† --sub ì•Œë£¨ë¯¸ëŠ„ --mode sub_only
    
    args = {}
    i = 1
    while i < len(sys.argv):
        arg = sys.argv[i]
        if arg.startswith('--'):
            key = arg.strip('-')
            if '=' in arg:
                # ë°©ì‹ 1: --key=value
                key, value = arg.split('=', 1)
                key = key.strip('-')
                value = value.strip('"\'')
                args[key] = value
            else:
                # ë°©ì‹ 2: --key value
                if i + 1 < len(sys.argv) and not sys.argv[i + 1].startswith('--'):
                    value = sys.argv[i + 1].strip('"\'')
                    args[key] = value
                    i += 1
                else:
                    args[key] = True
        i += 1
    
    target_major = args.get('target-major') or args.get('major')
    target_middle = args.get('target-middle') or args.get('middle')
    target_sub = args.get('target-sub') or args.get('sub')
    crawl_mode = args.get('crawl-mode') or args.get('mode', 'all')
    start_year = args.get('start-year', '2020')
    start_month = args.get('start-month', '01').zfill(2)
    
    # ìºì‹œ ìš°íšŒ ì˜µì…˜ íŒŒì‹±
    force_refresh = (args.get('force-refresh') or args.get('no-cache') or 
                    args.get('force_refresh') or args.get('no_cache') or False)
    if isinstance(force_refresh, str):
        force_refresh = force_refresh.lower() in ['true', '1', 'yes', 'on']

    log(f"í¬ë¡¤ë§ ì„¤ì •:")
    log(f"  - ëª¨ë“œ: {crawl_mode}")
    log(f"  - ëŒ€ë¶„ë¥˜: {target_major}")
    log(f"  - ì¤‘ë¶„ë¥˜: {target_middle}")
    log(f"  - ì†Œë¶„ë¥˜: {target_sub}")
    log(f"  - ì‹œì‘ ì‹œì : {start_year}-{start_month}")
    log(f"  - ìºì‹œ ìš°íšŒ: {force_refresh}")

    # í¬ë¡¤ë§ ëª¨ë“œì— ë”°ë¥¸ ì‹¤í–‰
    if crawl_mode == "all" and not target_major:
        # ì „ì²´ ëŒ€ë¶„ë¥˜ í¬ë¡¤ë§ (ê¸°ì¡´ ë°©ì‹)
        log("ì „ì²´ ëŒ€ë¶„ë¥˜ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.", "INFO")
        all_major_categories = list(INCLUSION_LIST.keys())
        log(f"í¬ë¡¤ë§í•  ëŒ€ë¶„ë¥˜: {all_major_categories}", "INFO")
        
        for major in all_major_categories:
            log(f"=== {major} í¬ë¡¤ë§ ì‹œì‘ ===", "SUMMARY")
            crawler = KpiCrawler(target_major=major, crawl_mode="all", 
                               start_year=start_year, start_month=start_month,
                               force_refresh=force_refresh)
            await crawler.run()
            log(f"ğŸŸ¢ {major} í¬ë¡¤ë§ ì™„ë£Œ", "SUCCESS")
        
        log("ğŸŸ¢ ì „ì²´ ëŒ€ë¶„ë¥˜ í¬ë¡¤ë§ ì™„ë£Œ", "SUCCESS")
    elif crawl_mode == "sub_only" and target_sub:
        # sub_only ëª¨ë“œ: íŠ¹ì • ì†Œë¶„ë¥˜ë§Œ í¬ë¡¤ë§
        log(f"=== sub_only ëª¨ë“œ: '{target_sub}' ì†Œë¶„ë¥˜ë§Œ í¬ë¡¤ë§ ===", "SUMMARY")
        
        # JSONCì—ì„œ í•´ë‹¹ ì†Œë¶„ë¥˜ê°€ ì†í•œ ëŒ€ë¶„ë¥˜ì™€ ì¤‘ë¶„ë¥˜ ì°¾ê¸°
        found_major = None
        found_middle = None
        
        for major, major_data in INCLUSION_LIST.items():
            for middle, middle_data in major_data.items():
                if target_sub in middle_data:
                    found_major = major
                    found_middle = middle
                    break
            if found_major:
                break
        
        if found_major and found_middle:
            log(f"ì†Œë¶„ë¥˜ '{target_sub}'ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤: {found_major} > {found_middle} > {target_sub}")
            crawler = KpiCrawler(
                target_major=found_major,
                target_middle=found_middle,
                target_sub=target_sub,
                crawl_mode=crawl_mode,
                start_year=start_year,
                start_month=start_month,
                force_refresh=force_refresh
            )
            await crawler.run()
            log(f"ğŸŸ¢ sub_only ëª¨ë“œ í¬ë¡¤ë§ ì™„ë£Œ", "SUCCESS")
        else:
            log(f"âŒ ì†Œë¶„ë¥˜ '{target_sub}'ë¥¼ JSONC íŒŒì¼ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "ERROR")
            return
    else:
        # ì„ íƒì  í¬ë¡¤ë§
        log(f"=== {crawl_mode} ëª¨ë“œ í¬ë¡¤ë§ ì‹œì‘ ===", "SUMMARY")
        crawler = KpiCrawler(
            target_major=target_major,
            target_middle=target_middle,
            target_sub=target_sub,
            crawl_mode=crawl_mode,
            start_year=start_year,
            start_month=start_month,
            force_refresh=force_refresh
        )
        await crawler.run()
        log(f"ğŸŸ¢ {crawl_mode} ëª¨ë“œ í¬ë¡¤ë§ ì™„ë£Œ", "SUCCESS")


async def test_unit_extraction():
    """ë‹¨ìœ„ ì¶”ì¶œ ë¡œì§ í…ŒìŠ¤íŠ¸"""
    log("=== ë‹¨ìœ„ ì¶”ì¶œ ë¡œì§ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===", "SUMMARY")
    
    # í…ŒìŠ¤íŠ¸í•  ë¹„ì² ê¸ˆì† ì†Œë¶„ë¥˜ ëª©ë¡
    test_categories = [
        ("ê³µí†µìì¬", "ë¹„ì² ê¸ˆì†", "ë™ì œí’ˆ(1)"),
        ("ê³µí†µìì¬", "ë¹„ì² ê¸ˆì†", "ë™ì œí’ˆ(2)"),
        ("ê³µí†µìì¬", "ë¹„ì² ê¸ˆì†", "ì•Œë£¨ë¯¸ëŠ„ì œí’ˆ(1)"),
        ("ê³µí†µìì¬", "ë¹„ì² ê¸ˆì†", "ì•Œë£¨ë¯¸ëŠ„ì œí’ˆ(2)"),
        ("ê³µí†µìì¬", "ë¹„ì² ê¸ˆì†", "ë¹„ì² ì§€ê¸ˆ(ééµåœ°é‡‘)"),
        ("ê³µí†µìì¬", "ë¹„ì² ê¸ˆì†", "ì—°(ë‚©)ì œí’ˆ(é‰›è£½å“)")
    ]
    
    crawler = KpiCrawler(target_major="ê³µí†µìì¬", crawl_mode="test")
    
    browser = None
    try:
        # ë¸Œë¼ìš°ì € ì‹œì‘ (run ë©”ì„œë“œì™€ ë™ì¼í•œ ë°©ì‹)
        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=True,
                args=[
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-gpu',
                    '--disable-web-security',
                    '--disable-features=VizDisplayCompositor',
                    '--window-size=1920,1080'
                ]
            )
            crawler.context = await browser.new_context()
            crawler.page = await crawler.context.new_page()
            
            await crawler._login()
            
            for major, middle, sub in test_categories:
                log(f"\n--- {sub} ë‹¨ìœ„ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ---", "INFO")
                
                try:
                    # ì†Œë¶„ë¥˜ ì •ë³´ ìƒì„± (ì‹¤ì œ í¬ë¡¤ë§ì—ì„œ ì‚¬ìš©í•˜ëŠ” í˜•íƒœì™€ ë™ì¼)
                    sub_info = {'name': sub, 'code': f'{major}{middle}{sub}'}
                    sub_url = f"https://www.kpi.or.kr/www/price/detail.asp?CATE_CD={sub_info['code']}"
                    if len(sub_info['code']) >= 4:
                        item_cd = sub_info['code'][-4:]
                        sub_url += f"&ITEM_CD={item_cd}"
                    
                    # í˜ì´ì§€ë¡œ ì´ë™
                    await crawler.page.goto(sub_url, timeout=60000)
                    await crawler.page.wait_for_load_state('networkidle', timeout=45000)
                    
                    # ë¬¼ê°€ì¶”ì´ í˜ì´ì§€ì—ì„œ ë‹¨ìœ„ ì¶”ì¶œ (ì˜¬ë°”ë¥¸ ë§¤ê°œë³€ìˆ˜ ê°œìˆ˜)
                    unit_from_trend = await crawler._extract_unit_from_price_trend_page(
                        crawler.page, sub
                    )
                    log(f"  ë¬¼ê°€ì¶”ì´ í˜ì´ì§€ ë‹¨ìœ„: {unit_from_trend}")
                    
                    # ë¬¼ê°€ì •ë³´ ë³´ê¸° íƒ­ í´ë¦­ (ë” ì•ˆì •ì ì¸ ì„ íƒì ì‚¬ìš©)
                    try:
                        # ì—¬ëŸ¬ ê°€ëŠ¥í•œ ì„ íƒìë¡œ ì‹œë„
                        tab_selectors = [
                            'a[href="#tab2"]',
                            'a[onclick*="tab2"]',
                            'a:has-text("ë¬¼ê°€ì •ë³´ ë³´ê¸°")',
                            'a:has-text("ë¬¼ê°€ì •ë³´")',
                            'li:nth-child(2) a',
                            '.tab-menu li:nth-child(2) a'
                        ]
                        
                        tab_clicked = False
                        for selector in tab_selectors:
                            try:
                                tab_element = await crawler.page.query_selector(selector)
                                if tab_element:
                                    await tab_element.click()
                                    await crawler.page.wait_for_timeout(2000)
                                    tab_clicked = True
                                    log(f"  íƒ­ í´ë¦­ ì„±ê³µ: {selector}")
                                    break
                            except Exception as tab_error:
                                log(f"  íƒ­ ì„ íƒì {selector} ì‹¤íŒ¨: {str(tab_error)}", "WARNING")
                                continue
                        
                        if not tab_clicked:
                            log("  âš ï¸ ë¬¼ê°€ì •ë³´ ë³´ê¸° íƒ­ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ. í˜„ì¬ í˜ì´ì§€ì—ì„œ ë‹¨ìœ„ ì¶”ì¶œ ì‹œë„", "WARNING")
                            
                    except Exception as tab_error:
                        log(f"  âš ï¸ íƒ­ í´ë¦­ ì‹¤íŒ¨: {str(tab_error)}", "WARNING")
                    
                    # ë¬¼ê°€ì •ë³´ ë³´ê¸° í˜ì´ì§€ì—ì„œ ë‹¨ìœ„ ì¶”ì¶œ (ì˜¬ë°”ë¥¸ ë§¤ê°œë³€ìˆ˜ ê°œìˆ˜)
                    unit_from_detail = await crawler._get_unit_from_detail_page(
                        crawler.page, sub
                    )
                    log(f"  ë¬¼ê°€ì •ë³´ ë³´ê¸° í˜ì´ì§€ ë‹¨ìœ„: {unit_from_detail}")
                    
                    # ìºì‹œì—ì„œ ë‹¨ìœ„ í™•ì¸ (redis_client ì´ˆê¸°í™” í™•ì¸)
                    cached_unit = None
                    try:
                        if hasattr(crawler, 'redis_client') and crawler.redis_client:
                            cache_key = f"unit_{major}_{middle}_{sub}"
                            cached_unit = await crawler.redis_client.get(cache_key)
                        else:
                            log("  Redis í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ", "WARNING")
                    except Exception as cache_error:
                        log(f"  ìºì‹œ í™•ì¸ ì‹¤íŒ¨: {str(cache_error)}", "WARNING")
                    
                    log(f"  ìºì‹œëœ ë‹¨ìœ„: {cached_unit}")
                    
                    # ê²°ê³¼ ë¹„êµ
                    if unit_from_trend and unit_from_detail:
                        if unit_from_trend == unit_from_detail:
                            log(f"  âœ… ë‹¨ìœ„ ì¼ì¹˜: {unit_from_trend}")
                        else:
                            log(f"  âš ï¸ ë‹¨ìœ„ ë¶ˆì¼ì¹˜ - ì¶”ì´: {unit_from_trend}, ìƒì„¸: {unit_from_detail}")
                    else:
                        log(f"  âŒ ë‹¨ìœ„ ì¶”ì¶œ ì‹¤íŒ¨ - ì¶”ì´: {unit_from_trend}, ìƒì„¸: {unit_from_detail}")
                        
                except Exception as e:
                    import traceback
                    log(f"  âŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "ERROR")
                    log(f"  ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}", "ERROR")
            
            log("\n=== ë‹¨ìœ„ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===", "SUMMARY")
            await browser.close()
        
    except Exception as e:
        log(f"âŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", "ERROR")
        import traceback
        log(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}", "ERROR")
        if browser:
            try:
                await browser.close()
            except:
                pass


if __name__ == "__main__":
    log("DEBUG: if __name__ == \"__main__\" ë¸”ë¡ ì§„ì… (kpi_crawler.py)", "DEBUG")
    # ëª…ë ¹í–‰ ì¸ìˆ˜ í™•ì¸
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        # ë‹¨ìœ„ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        asyncio.run(test_unit_extraction())
    else:
        # ì¼ë°˜ í¬ë¡¤ë§ ì‹¤í–‰
        # í”„ë¡œì„¸ìŠ¤ ê°ì§€ ë¡œì§ ë¹„í™œì„±í™” (ì„ì‹œ)
        # running_crawlers = check_running_crawler()
        # if running_crawlers:
        #     log(f"ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ í¬ë¡¤ëŸ¬ {len(running_crawlers)}ê°œ ë°œê²¬. ê¸°ì¡´ í¬ë¡¤ëŸ¬ ì™„ë£Œ í›„ ì¬ì‹¤í–‰í•˜ì„¸ìš”.", "ERROR")
        #     sys.exit(1)
        
        # ê¸°ë³¸ í¬ë¡¤ë§ ì‹¤í–‰ (ì „ì²´ ëŒ€ë¶„ë¥˜)
        
        asyncio.run(main())

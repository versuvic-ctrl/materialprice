# -*- coding: utf-8 -*-

import os
import asyncio
import sys
import re
import psutil
from datetime import datetime
from dotenv import load_dotenv
from playwright.async_api import async_playwright, TimeoutError
from upstash_redis import AsyncRedis
from jsonc_parser import parse_jsonc

# ì ˆëŒ€ importë¥¼ ìœ„í•œ ê²½ë¡œ ì„¤ì •
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

# --- 1. ì´ˆê¸° ì„¤ì • ë° í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ---
load_dotenv("../../.env.local")

# data_processor ëª¨ë“ˆì—ì„œ í•„ìš”í•œ í•¨ìˆ˜ì™€ ê°ì²´ë¥¼ import
from data_processor import log, create_data_processor, api_monitor as supabase

# --- 2. í¬ë¡¤ë§ ëŒ€ìƒ ì¹´í…Œê³ ë¦¬ ë° ë‹¨ìœ„ ì„¤ì • ---
INCLUSION_LIST_PATH = os.path.join(current_dir, "kpi_inclusion_list_compact.jsonc")
with open(INCLUSION_LIST_PATH, "r", encoding="utf-8") as f:
    jsonc_content = f.read()
INCLUSION_LIST = parse_jsonc(jsonc_content)


# --- 3. Playwright ì›¹ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤ ---
class KpiCrawler:
    def __init__(self, target_major: str = None, target_middle: str = None,
                 target_sub: str = None, crawl_mode: str = "all",
                 start_year: str = '2020', start_month: str = '01', max_concurrent=3):
        self.base_url = "https://www.kpi.or.kr"
        self.max_concurrent = max_concurrent
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.supabase = supabase

        self.target_major_category = target_major
        self.target_middle_category = target_middle
        self.target_sub_category = target_sub
        self.crawl_mode = crawl_mode
        self.start_year = start_year
        self.start_month = start_month

        self.processor = create_data_processor('kpi')

        self.base_regions = [
            'ì„œìš¸', 'ë¶€ì‚°', 'ëŒ€êµ¬', 'ì¸ì²œ', 'ê´‘ì£¼', 'ëŒ€ì „', 'ìš¸ì‚°', 'ì„¸ì¢…', 'ê²½ê¸°', 'ê°•ì›',
            'ì¶©ë¶', 'ì¶©ë‚¨', 'ì „ë¶', 'ì „ë‚¨', 'ê²½ë¶', 'ê²½ë‚¨', 'ì œì£¼', 'ìˆ˜ì›', 'ì„±ë‚¨', 'ì¶˜ì²œ',
            'ì²­ì£¼', 'ì „ì£¼', 'í¬í•­', 'ì°½ì›', 'ê¹€í•´', 'êµ¬ë¯¸', 'ì²œì•ˆ', 'ì§„ì£¼', 'ì›ì£¼', 'ê²½ì£¼',
            'ì¶©ì£¼', 'ì—¬ìˆ˜', 'ëª©í¬'
        ]

        # Redis í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        try:
            if 'UPSTASH_REDIS_REST_URL' in os.environ and 'UPSTASH_REDIS_REST_TOKEN' in os.environ:
                self.redis = AsyncRedis.from_env()
                log("âœ… Upstash Redis REST API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ")
            else:
                self.redis = None
                log("âš ï¸ Redis í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìºì‹œ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.", "WARNING")
        except Exception as e:
            self.redis = None
            log(f"âš ï¸ Redis ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}. ìºì‹œ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.", "WARNING")

        log(f"í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” - ëª¨ë“œ: {self.crawl_mode}, íƒ€ê²Ÿ: {self.target_major_category or 'ì „ì²´'}")

    async def run(self):
        """í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        browser = None
        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True, args=['--no-sandbox'])
                self.context = await browser.new_context()
                self.page = await self.context.new_page()

                await self._login()
                await self._navigate_to_category()
                await self._crawl_categories()

                log("\nğŸŸ¢ === ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ === ğŸŸ¢\n", "SUMMARY")
                await browser.close()
                return self.processor
        except Exception as e:
            log(f"í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘ ìµœìƒìœ„ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "ERROR")
            if browser:
                try: await browser.close()
                except: pass
            raise

    async def _login(self):
        """ë¡œê·¸ì¸ ìˆ˜í–‰ (ì•ˆì •í™” ë²„ì „)"""
        log("ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ì´ë™ ì¤‘...")
        await self.page.goto(f"{self.base_url}/www/member/login.asp", timeout=90000, wait_until="networkidle")
        
        username = os.environ.get("KPI_USERNAME")
        password = os.environ.get("KPI_PASSWORD")

        if not username or not password:
            raise ValueError("KPI_USERNAME, KPI_PASSWORD í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.")

        await self.page.locator("#user_id").fill(username)
        await self.page.locator("#user_pw").fill(password)
        await self.page.locator("#sendLogin").click()
        
        # í˜ì´ì§€ê°€ ì´ë™í•  ë•Œê¹Œì§€ ì¶©ë¶„íˆ ê¸°ë‹¤ë¦¼
        await self.page.wait_for_load_state('networkidle', timeout=60000)

        # ë¡œê·¸ì¸ ì„±ê³µ ì—¬ë¶€ í™•ì¸
        if "login.asp" in self.page.url:
             raise ValueError("ë¡œê·¸ì¸ ì‹¤íŒ¨: KPI ì›¹ì‚¬ì´íŠ¸ ë¡œê·¸ì¸ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        
        log("ë¡œê·¸ì¸ ì„±ê³µ", "SUCCESS")

    async def _navigate_to_category(self):
        """ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ë¡œ ì´ë™ ë° íŒì—… ì²˜ë¦¬"""
        log("ì¢…í•©ë¬¼ê°€ì •ë³´ í˜ì´ì§€ë¡œ ì´ë™ ì¤‘...")
        await self.page.goto(f"{self.base_url}/www/price/category.asp", timeout=90000, wait_until="networkidle")
        
        popups = await self.page.query_selector_all(".pop-btn-close")
        for popup_close in popups:
            try:
                if await popup_close.is_visible():
                    await popup_close.click(timeout=15000)
                    log("íŒì—… ë‹«ê¸° ì„±ê³µ")
            except Exception:
                continue
        
        log("ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ ì´ë™ ì™„ë£Œ", "SUCCESS")

    async def _crawl_categories(self):
        """ëŒ€ë¶„ë¥˜ -> ì¤‘ë¶„ë¥˜ -> ì†Œë¶„ë¥˜ ìˆœì°¨ì ìœ¼ë¡œ í¬ë¡¤ë§"""
        major_selector = '#left_menu_kpi > ul.panel > li.file-item > a'
        major_categories_elements = await self.page.locator(major_selector).all()

        major_links = []
        for cat in major_categories_elements:
            name = await cat.inner_text()
            href = await cat.get_attribute('href')
            if name in INCLUSION_LIST:
                major_links.append({'name': name, 'href': href})

        for major in major_links:
            if self.target_major_category and major['name'] != self.target_major_category:
                continue

            log(f"ëŒ€ë¶„ë¥˜ '{major['name']}' í¬ë¡¤ë§ ì‹œì‘...")
            await self.page.goto(f"{self.base_url}{major['href']}")
            await self.page.wait_for_load_state('networkidle', timeout=60000)

            try:
                close_button = self.page.locator("#right_quick .q_cl")
                if await close_button.is_visible(timeout=10000):
                    await close_button.click()
                    log("  'Right Quick' ë©”ë‰´ë¥¼ ìˆ¨ê²¼ìŠµë‹ˆë‹¤.")
                    await self.page.wait_for_timeout(1000)
            except Exception:
                log("  'Right Quick' ë©”ë‰´ê°€ ì—†ê±°ë‚˜ ì´ë¯¸ ìˆ¨ê²¨ì ¸ ìˆì–´ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.", "DEBUG")

            open_sub_button = self.page.locator('a[href="javascript:openSub();"]')
            if await open_sub_button.count() > 0:
                log("  openSub() ë²„íŠ¼ í´ë¦­í•˜ì—¬ ëª¨ë“  ë¶„ë¥˜ í¼ì¹˜ëŠ” ì¤‘...")
                await open_sub_button.click()
                await self.page.wait_for_timeout(5000)

            all_middle_elements = await self.page.locator('.part-open-list').all()
            
            for middle_element in all_middle_elements:
                try:
                    middle_link_element = middle_element.locator('.part-ttl > a').first
                    middle_name = (await middle_link_element.inner_text()).strip()

                    if middle_name not in INCLUSION_LIST.get(major['name'], {}):
                        continue
                    
                    if self.target_middle_category and middle_name != self.target_middle_category:
                        continue
                    
                    log(f"  ì¤‘ë¶„ë¥˜ '{middle_name}' ì²˜ë¦¬ ì‹œì‘...")

                    sub_links_elements = await middle_element.locator('.part-list li a').all()
                    sub_links_to_crawl = []
                    
                    for sub_link_element in sub_links_elements:
                        sub_name = (await sub_link_element.inner_text()).strip()
                        
                        if sub_name in INCLUSION_LIST.get(major['name'], {}).get(middle_name, {}):
                            if self.target_sub_category and sub_name != self.target_sub_category:
                                continue

                            href = await sub_link_element.get_attribute('href')
                            sub_links_to_crawl.append({'name': sub_name, 'href': href})

                    if sub_links_to_crawl:
                        await self._crawl_subcategories_parallel(major['name'], middle_name, sub_links_to_crawl)
                    else:
                        log(f"    ì¤‘ë¶„ë¥˜ '{middle_name}': INCLUSION_LISTì— í¬í•¨ëœ ì²˜ë¦¬í•  ì†Œë¶„ë¥˜ê°€ ì—†ìŠµë‹ˆë‹¤.")

                except Exception as e:
                    log(f"  ì¤‘ë¶„ë¥˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", "ERROR")
                    continue
            
            # ëŒ€ë¶„ë¥˜ í¬ë¡¤ë§ ì™„ë£Œ í›„ í•´ë‹¹ ëŒ€ë¶„ë¥˜ ìºì‹œ ë¬´íš¨í™”
            log(f"[ìºì‹œ ë¬´íš¨í™”] ëŒ€ë¶„ë¥˜ '{major['name']}' í¬ë¡¤ë§ ì™„ë£Œ í›„ ê´€ë ¨ ìºì‹œë¥¼ ë¬´íš¨í™”í•©ë‹ˆë‹¤.")
            await self.clear_redis_cache(major_name=major['name'])

    async def _crawl_subcategories_parallel(self, major_name, middle_name, sub_categories_info):
        """ì†Œë¶„ë¥˜ ë³‘ë ¬ í¬ë¡¤ë§ í›„ ì¤‘ë¶„ë¥˜ ë‹¨ìœ„ë¡œ ì €ì¥ ë° ìºì‹œ ë¬´íš¨í™”"""
        if not sub_categories_info:
            log(f"    ì¤‘ë¶„ë¥˜ '{middle_name}': ì²˜ë¦¬í•  ì†Œë¶„ë¥˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        log(f"    ì¤‘ë¶„ë¥˜ '{middle_name}': {len(sub_categories_info)}ê°œ ì†Œë¶„ë¥˜ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

        tasks = [self._crawl_single_subcategory(major_name, middle_name, sub_info) for sub_info in sub_categories_info]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        all_data_for_middle_category = []
        for i, result in enumerate(results):
            sub_name = sub_categories_info[i]['name']
            if isinstance(result, Exception):
                log(f"    âŒ ì†Œë¶„ë¥˜ '{sub_name}' ì²˜ë¦¬ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜: {result}", "ERROR")
            elif result:
                all_data_for_middle_category.extend(result)
        
        if all_data_for_middle_category:
            log(f"  [DB ì €ì¥] ì¤‘ë¶„ë¥˜ '{middle_name}'ì˜ ì „ì²´ ë°ì´í„° {len(all_data_for_middle_category)}ê°œë¥¼ ì €ì¥í•©ë‹ˆë‹¤.")
            saved_count = await self.processor.save_to_supabase(all_data_for_middle_category, 'kpi_price_data')
            if saved_count > 0:
                log(f"  [ìºì‹œ ë¬´íš¨í™”] ì¤‘ë¶„ë¥˜ '{middle_name}' ê´€ë ¨ ìºì‹œë¥¼ ë¬´íš¨í™”í•©ë‹ˆë‹¤.")
                await self.clear_redis_cache(major_name=major_name, middle_name=middle_name)
        else:
            log(f"  ì¤‘ë¶„ë¥˜ '{middle_name}'ì—ì„œ ìµœì¢… ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    async def _crawl_single_subcategory(self, major_name, middle_name, sub_info):
        """
        [ìµœì¢… ìˆ˜ì •ë³¸] ë‹¨ì¼ ì†Œë¶„ë¥˜ì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
        í…Œì´ë¸” íƒ€ì…(ì§€ì—­, ê°€ê²©, ìƒì„¸ê·œê²©)ì„ ì •í™•íˆ íŒë³„í•˜ì—¬ ë°ì´í„°ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤.
        """
        async with self.semaphore:
            sub_name = sub_info['name']
            sub_href = sub_info['href']
            sub_url = f"{self.base_url}/www/price/{sub_href}"

            log(f"    - '{sub_name}' ìˆ˜ì§‘ ì‹œì‘")
            new_page = None
            max_retries = 3
            
            for attempt in range(max_retries):
                try:
                    new_page = await self.context.new_page()
                    await new_page.goto(sub_url, timeout=90000, wait_until="networkidle")
                    await new_page.click('a[href*="detail_change.asp"]', timeout=15000)
                    
                    spec_dropdown_selector = 'select#ITEM_SPEC_CD'
                    await new_page.wait_for_selector(spec_dropdown_selector, timeout=60000) # íƒ€ì„ì•„ì›ƒ 60ì´ˆë¡œ ì¦ê°€

                    options = await new_page.locator(f'{spec_dropdown_selector} option').all()
                    specs_to_crawl = [{'name': (await o.inner_text()).strip(), 'value': await o.get_attribute('value')} for o in options if await o.get_attribute('value')]
                    
                    if not specs_to_crawl:
                        await new_page.close()
                        return []

                    all_crawled_data = []
                    for i, spec in enumerate(specs_to_crawl):
                        try:
                            await new_page.wait_for_timeout(5000)
                            await new_page.select_option(spec_dropdown_selector, value=spec['value'], timeout=60000) # select_optionì—ë„ íƒ€ì„ì•„ì›ƒ 60ì´ˆ ì§€ì •

                            if i == 0:
                                await new_page.select_option('select#DATA_YEAR_F', value=self.start_year)
                                await new_page.select_option('select#DATA_MONTH_F', value=self.start_month)
                                
                                end_year_selector = 'select#DATA_YEAR_T'
                                latest_year = await new_page.locator(f'{end_year_selector} option').first.get_attribute('value')
                                await new_page.select_option(end_year_selector, value=latest_year)
                                
                                end_month_selector = 'select#DATA_MONTH_T'
                                latest_month = await new_page.locator(f'{end_month_selector} option').last.get_attribute('value')
                                await new_page.select_option(end_month_selector, value=latest_month)
                            
                            async with new_page.expect_response(lambda r: "detail_change.asp" in r.url, timeout=60000):
                                await new_page.click('form[name="sForm"] input[type="image"]')
                            
                            try:
                                await new_page.wait_for_selector('table#priceTrendDataArea tr:nth-child(2)', timeout=30000)
                            except Exception:
                                log(f"      - Spec '{spec['name'][:20]}...': ì¡°íšŒ ê¸°ê°„ ë‚´ ë°ì´í„° ì—†ìŒ (ê±´ë„ˆëœ€).", "INFO")
                                continue

                            unit = self._get_unit_from_inclusion_list(major_name, middle_name, sub_name, spec['name'])
                            
                            headers = [th.strip() for th in await new_page.locator('table#priceTrendDataArea th').all_inner_texts()]
                            
                            rows = await new_page.locator('table#priceTrendDataArea tr').all()
                            for row in rows[1:]:
                                cols_text = await row.locator('td').all_inner_texts()
                                if not cols_text: continue
                                
                                date = cols_text[0].strip()
                                prices_text = [p.strip().replace(',', '') for p in cols_text[1:]]
                                data_headers = headers[1:]
                                
                                for idx, header_text in enumerate(data_headers):
                                    if idx < len(prices_text) and prices_text[idx].isdigit():
                                        is_price_header = 'ê°€ê²©' in header_text or re.match(r'ê°€[â‘ -â‘©]', header_text)
                                        is_region = self._is_region_header(header_text)

                                        region = "ì „êµ­"
                                        detail_spec = None
                                        
                                        if is_region:
                                            region = self._remove_roman_numerals(header_text)
                                            detail_spec = None
                                        elif is_price_header:
                                            detail_spec = header_text
                                        else:
                                            detail_spec = header_text
                                        
                                        all_crawled_data.append(self._create_data_entry(
                                            major_name, middle_name, sub_name, spec['name'], 
                                            region, detail_spec, date, prices_text[idx], unit
                                        ))
                        except Exception as spec_e:
                            log(f"      - Spec '{spec.get('name', 'N/A')[:20]}...' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {spec_e}", "WARNING")
                            continue

                    await new_page.close()
                    log(f"    - '{sub_name}' ì™„ë£Œ: {len(all_crawled_data)}ê°œ ë°ì´í„° ìˆ˜ì§‘.")
                    return all_crawled_data

                except Exception as e:
                    if new_page: await new_page.close()
                    if attempt == max_retries - 1:
                        log(f"    âŒ '{sub_name}' ìµœì¢… ì‹¤íŒ¨: {str(e)}", "ERROR")
                        return []
                    log(f"    âš ï¸ '{sub_name}' ì¬ì‹œë„ {attempt + 1}/{max_retries}: {str(e)}", "WARNING")
                    await asyncio.sleep(5)
        return []

    async def clear_redis_cache(self, major_name: str = None, middle_name: str = None):
        """AsyncRedisì— ë§ëŠ” ë¹„ë™ê¸° ë°©ì‹ìœ¼ë¡œ ìºì‹œë¥¼ ë¬´íš¨í™”í•©ë‹ˆë‹¤."""
        if self.redis is None:
            log("  âš ï¸ Redis ë¹„í™œì„±í™”, ìºì‹œ ì‚­ì œ ê±´ë„ˆëœ€.", "WARNING")
            return
            
        try:
            if major_name and middle_name:
                match_pattern = f"material_prices:{major_name}:{middle_name}:*"
                log(f"  [ìºì‹œ ë¬´íš¨í™”] ì¤‘ë¶„ë¥˜ '{middle_name}' ê´€ë ¨ ìºì‹œë¥¼ ë¬´íš¨í™”í•©ë‹ˆë‹¤. íŒ¨í„´: {match_pattern}")
            elif major_name:
                match_pattern = f"material_prices:{major_name}:*"
                log(f"  [ìºì‹œ ë¬´íš¨í™”] ëŒ€ë¶„ë¥˜ '{major_name}' ê´€ë ¨ ìºì‹œë¥¼ ë¬´íš¨í™”í•©ë‹ˆë‹¤. íŒ¨í„´: {match_pattern}")
            else:
                match_pattern = "material_prices:*"
                log(f"  [ìºì‹œ ë¬´íš¨í™”] ëª¨ë“  material_prices ìºì‹œë¥¼ ë¬´íš¨í™”í•©ë‹ˆë‹¤. íŒ¨í„´: {match_pattern}")

            cursor, keys = await self.redis.scan(0, match=match_pattern, count=500)
            keys_to_delete = keys
            
            while cursor != 0:
                cursor, keys = await self.redis.scan(cursor, match=match_pattern, count=500)
                keys_to_delete.extend(keys)

            if keys_to_delete:
                await self.redis.delete(*keys_to_delete)
                log(f"  âœ… Redis ìºì‹œ ë¬´íš¨í™” ì„±ê³µ: {len(keys_to_delete)}ê°œ í‚¤ ì‚­ì œ")
            else:
                log(f"  âœ… ì‚­ì œí•  Redis '{match_pattern}' ìºì‹œê°€ ì—†ìŠµë‹ˆë‹¤.")
                
            # í¬ë¡¤ë§ ì™„ë£Œ ì‹œ ëŒ€ì‹œë³´ë“œ ê´€ë ¨ ìºì‹œë„ ë¬´íš¨í™”
            if not major_name:  # ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ ì‹œ
                dashboard_keys = ['dashboard_summary_data', 'total_materials_count']
                for key in dashboard_keys:
                    try:
                        await self.redis.delete(key)
                        log(f"  âœ… ëŒ€ì‹œë³´ë“œ ìºì‹œ ë¬´íš¨í™”: {key}")
                    except Exception as e:
                        log(f"  âŒ ëŒ€ì‹œë³´ë“œ ìºì‹œ ë¬´íš¨í™” ì‹¤íŒ¨ ({key}): {str(e)}", "ERROR")
                
                # ì§‘ê³„ í…Œì´ë¸” ì—…ë°ì´íŠ¸ (Supabase í•¨ìˆ˜ í˜¸ì¶œ)
                try:
                    from supabase import create_client
                    supabase_url = os.getenv('SUPABASE_URL')
                    supabase_key = os.getenv('SUPABASE_KEY')
                    
                    if supabase_url and supabase_key:
                        supabase = create_client(supabase_url, supabase_key)
                        # update_material_statistics í•¨ìˆ˜ í˜¸ì¶œ
                        result = supabase.rpc('update_material_statistics').execute()
                        log(f"  âœ… ì§‘ê³„ í…Œì´ë¸” ì—…ë°ì´íŠ¸ ì™„ë£Œ")
                    else:
                        log(f"  âš ï¸ Supabase í™˜ê²½ë³€ìˆ˜ ì—†ìŒ, ì§‘ê³„ í…Œì´ë¸” ì—…ë°ì´íŠ¸ ê±´ë„ˆëœ€", "WARNING")
                except Exception as e:
                    log(f"  âŒ ì§‘ê³„ í…Œì´ë¸” ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}", "ERROR")
                        
        except Exception as e:
            log(f"  âŒ Redis ìºì‹œ ì‚­ì œ ì‹¤íŒ¨: {str(e)}", "ERROR")

    def _remove_roman_numerals(self, text):
        # Remove Roman numerals in circles (â‘  to â‘©)
        return re.sub(r'[â‘ -â‘©]', '', text)

    def _is_region_header(self, header_text):
        cleaned_header = self._remove_roman_numerals(header_text)
        return cleaned_header in self.base_regions

    def _create_data_entry(self, major, middle, sub, spec, region, detail_spec, date, price, unit):
        """ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ì„ ìœ„í•œ ë”•ì…”ë„ˆë¦¬ ê°ì²´ ìƒì„±"""
        return {
            'major_category': major, 'middle_category': middle, 'sub_category': sub,
            'specification': spec,
            'region': region,
            'detail_spec': detail_spec, # detail_specì„ ë³„ë„ í•„ë“œë¡œ ì¶”ê°€
            'date': f"{date.replace('.', '-')}-01",
            'price': int(price), 'unit': unit
        }

    def _get_unit_from_inclusion_list(self, major_name, middle_name, sub_name, spec_name=None):
        """INCLUSION_LISTì—ì„œ ë‹¨ìœ„ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        try:
            unit_data = INCLUSION_LIST.get(major_name, {}).get(middle_name, {}).get(sub_name)
            if isinstance(unit_data, dict):
                if spec_name and 'specifications' in unit_data:
                    specs = unit_data['specifications']
                    if spec_name in specs: return specs[spec_name]
                    for key, val in specs.items():
                        if spec_name in key or key in spec_name: return val
                return unit_data.get("unit") or unit_data.get("default")
            elif isinstance(unit_data, str):
                return unit_data
        except Exception as e:
            log(f"ë‹¨ìœ„ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}", "ERROR")
        return None

# --- 5. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ---
async def main():
    """ë©”ì¸ ì‹¤í–‰ ë¡œì§: ëª…ë ¹í–‰ ì¸ì íŒŒì‹± ë° í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
    args = {arg.split('=', 1)[0].strip('-'): arg.split('=', 1)[1].strip('"\'') for arg in sys.argv[1:] if '=' in arg}
    
    target_major = args.get('major')
    crawl_mode = "major_only" if target_major else "all"
    
    start_year = args.get('start-year', '2020')
    start_month = args.get('start-month', '01').zfill(2)

    log(f"í¬ë¡¤ë§ ëª¨ë“œ: {crawl_mode}, íƒ€ê²Ÿ: {target_major or 'ì „ì²´'}, ì‹œì‘: {start_year}-{start_month}", "SUMMARY")

    if crawl_mode == "all":
        for major_name in INCLUSION_LIST.keys():
            log(f"\n=== ëŒ€ë¶„ë¥˜: {major_name} í¬ë¡¤ë§ ì‹œì‘ ===", "SUMMARY")
            crawler = KpiCrawler(target_major=major_name, crawl_mode="major_only", start_year=start_year, start_month=start_month)
            await crawler.run()
    else:
        crawler = KpiCrawler(target_major=target_major, crawl_mode=crawl_mode, start_year=start_year, start_month=start_month)
        await crawler.run()

if __name__ == "__main__":
    asyncio.run(main())
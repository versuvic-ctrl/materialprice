# -*- coding: utf-8 -*-

import os
import asyncio
import sys
import re
import psutil
from datetime import datetime
from dotenv import load_dotenv
from playwright.async_api import async_playwright
from upstash_redis import AsyncRedis
from jsonc_parser import parse_jsonc
# data_processorëŠ” ì•„ëž˜ì—ì„œ ì´ˆê¸°í™” í›„ ì‚¬ìš©í•˜ë¯€ë¡œ ì—¬ê¸°ì„œ ì§ì ‘ importí•  í•„ìš”ê°€ ì¤„ì–´ë“­ë‹ˆë‹¤.
# from data_processor import create_data_processor, log

# ì ˆëŒ€ importë¥¼ ìœ„í•œ ê²½ë¡œ ì„¤ì •
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

# --- 1. ì´ˆê¸° ì„¤ì • ë° í™˜ê²½ë³€ìˆ˜ ë¡œë“œ ---
load_dotenv("../../.env.local")

# data_processor ëª¨ë“ˆì—ì„œ í•„ìš”í•œ í•¨ìˆ˜ì™€ ê°ì²´ë¥¼ import
from data_processor import log, create_data_processor, api_monitor as supabase

# --- 2. í¬ë¡¤ë§ ëŒ€ìƒ ì¹´í…Œê³ ë¦¬ ë° ë‹¨ìœ„ ì„¤ì • ---
INCLUSION_LIST_PATH = os.path.join(current_dir, "kpi_inclusion_list_compact.jsonc")
with open(INCLUSION_LIST_PATH, "r", encoding="utf-8") as f:
    jsonc_content = f.read()
INCLUSION_LIST = parse_jsonc(jsonc_content)


# --- 3. Playwright ì›¹ í¬ë¡¤ëŸ¬ í´ëž˜ìŠ¤ ---
class KpiCrawler:
    def __init__(self, target_major: str = None, target_middle: str = None,
                 target_sub: str = None, crawl_mode: str = "all",
                 start_year: str = '2020', start_month: str = '01', max_concurrent=3):
        self.base_url = "https://www.kpi.or.kr"
        self.max_concurrent = max_concurrent
        self.semaphore = asyncio.Semaphore(max_concurrent)
        self.supabase = supabase

        self.target_major_category = target_major
        self.target_middle_category = target_middle
        self.target_sub_category = target_sub
        self.crawl_mode = crawl_mode
        # ì‹œìž‘ ë‚ ì§œëŠ” ê¸°ë³¸ê°’ìœ¼ë¡œ 2020ë…„ 1ì›”ì„ ì‚¬ìš©
        self.start_year = start_year
        self.start_month = start_month

        self.processor = create_data_processor('kpi')

        # Redis í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        try:
            if 'UPSTASH_REDIS_REST_URL' in os.environ and 'UPSTASH_REDIS_REST_TOKEN' in os.environ:
                self.redis = AsyncRedis.from_env()
                log("âœ… Upstash Redis REST API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ")
            else:
                self.redis = None
                log("âš ï¸ Redis í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìºì‹œ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.", "WARNING")
        except Exception as e:
            self.redis = None
            log(f"âš ï¸ Redis ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}. ìºì‹œ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.", "WARNING")

        log(f"í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” - ëª¨ë“œ: {self.crawl_mode}, íƒ€ê²Ÿ: {self.target_major_category or 'ì „ì²´'}")

    async def run(self):
        """í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        browser = None
        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True, args=['--no-sandbox'])
                self.context = await browser.new_context()
                self.page = await self.context.new_page()

                await self._login()
                await self._navigate_to_category()
                await self._crawl_categories()

                log("\nðŸŸ¢ === ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ === ðŸŸ¢\n", "SUMMARY")
                await browser.close()
                return self.processor
        except Exception as e:
            log(f"í¬ë¡¤ë§ ì‹¤í–‰ ì¤‘ ìµœìƒìœ„ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "ERROR")
            if browser:
                try: await browser.close()
                except: pass
            raise

    async def _login(self):
        """ë¡œê·¸ì¸ ìˆ˜í–‰"""
        log("ë¡œê·¸ì¸ íŽ˜ì´ì§€ë¡œ ì´ë™ ì¤‘...")
        await self.page.goto(f"{self.base_url}/www/member/login.asp", timeout=60000, wait_until="networkidle")
        
        username = os.environ.get("KPI_USERNAME")
        password = os.environ.get("KPI_PASSWORD")

        if not username or not password:
            raise ValueError("KPI_USERNAME, KPI_PASSWORD í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.")

        await self.page.locator("#user_id").fill(username)
        await self.page.locator("#user_pw").fill(password)
        await self.page.locator("#sendLogin").click()
        
        await self.page.wait_for_load_state('networkidle', timeout=45000)
        log("ë¡œê·¸ì¸ ì„±ê³µ", "SUCCESS")

    async def _navigate_to_category(self):
        """ì¹´í…Œê³ ë¦¬ íŽ˜ì´ì§€ë¡œ ì´ë™ ë° íŒì—… ì²˜ë¦¬"""
        log("ì¢…í•©ë¬¼ê°€ì •ë³´ íŽ˜ì´ì§€ë¡œ ì´ë™ ì¤‘...")
        await self.page.goto(f"{self.base_url}/www/price/category.asp", timeout=60000, wait_until="networkidle")
        
        # íŒì—… ë‹«ê¸° ì‹œë„
        popups = await self.page.query_selector_all(".pop-btn-close")
        for popup_close in popups:
            try:
                if await popup_close.is_visible():
                    await popup_close.click(timeout=3000)
                    log("íŒì—… ë‹«ê¸° ì„±ê³µ")
            except Exception:
                continue
        
        log("ì¹´í…Œê³ ë¦¬ íŽ˜ì´ì§€ ì´ë™ ì™„ë£Œ", "SUCCESS")

    async def _crawl_categories(self):
        """
        [ìµœì¢… ìˆ˜ì •ë³¸] ëŒ€ë¶„ë¥˜ -> ì¤‘ë¶„ë¥˜ -> ì†Œë¶„ë¥˜ ìˆœì°¨ì ìœ¼ë¡œ í¬ë¡¤ë§.
        í™”ë©´ì„ ê°€ë¦¬ëŠ” ìš”ì†Œë¥¼ ì œê±°í•˜ëŠ” ë¡œì§ì„ ì¶”ê°€í•˜ì—¬ í´ë¦­ ì˜¤ë¥˜ë¥¼ í•´ê²°í•©ë‹ˆë‹¤.
        """
        major_selector = '#left_menu_kpi > ul.panel > li.file-item > a'
        major_categories_elements = await self.page.locator(major_selector).all()

        major_links = []
        for cat in major_categories_elements:
            name = await cat.inner_text()
            href = await cat.get_attribute('href')
            if name in INCLUSION_LIST:
                major_links.append({'name': name, 'href': href})

        for major in major_links:
            if self.target_major_category and major['name'] != self.target_major_category:
                continue

            log(f"ëŒ€ë¶„ë¥˜ '{major['name']}' í¬ë¡¤ë§ ì‹œìž‘...")
            await self.page.goto(f"{self.base_url}{major['href']}")
            await self.page.wait_for_load_state('networkidle', timeout=45000)

            # --- â˜…â˜…â˜… ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì½”ë“œ ì¶”ê°€ (ì‹œìž‘) â˜…â˜…â˜… ---
            # íŽ˜ì´ì§€ ì´ë™ í›„, í´ë¦­ì„ ë°©í•´í•  ìˆ˜ ìžˆëŠ” 'Right Quick' ë©”ë‰´ë¥¼ ìˆ¨ê¹ë‹ˆë‹¤.
            try:
                close_button = self.page.locator("#right_quick .q_cl")
                if await close_button.is_visible(timeout=5000): # 5ì´ˆ ë™ì•ˆ ê¸°ë‹¤ë ¤ë´„
                    await close_button.click()
                    log("  'Right Quick' ë©”ë‰´ë¥¼ ìˆ¨ê²¼ìŠµë‹ˆë‹¤.")
                    await self.page.wait_for_timeout(1000) # ìˆ¨ê²¨ì§€ëŠ” ì• ë‹ˆë©”ì´ì…˜ ëŒ€ê¸°
            except Exception:
                # ë©”ë‰´ê°€ ì—†ê±°ë‚˜ ì´ë¯¸ ìˆ¨ê²¨ì ¸ ìžˆëŠ” ê²½ìš°, ì˜¤ë¥˜ë¥¼ ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰
                log("  'Right Quick' ë©”ë‰´ê°€ ì—†ê±°ë‚˜ ì´ë¯¸ ìˆ¨ê²¨ì ¸ ìžˆì–´ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.", "DEBUG")
            # --- â˜…â˜…â˜… ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì½”ë“œ ì¶”ê°€ (ë) â˜…â˜…â˜… ---

            open_sub_button = self.page.locator('a[href="javascript:openSub();"]')
            if await open_sub_button.count() > 0:
                log("  openSub() ë²„íŠ¼ í´ë¦­í•˜ì—¬ ëª¨ë“  ë¶„ë¥˜ íŽ¼ì¹˜ëŠ” ì¤‘...")
                await open_sub_button.click()
                await self.page.wait_for_timeout(5000)

            all_middle_elements = await self.page.locator('.part-open-list').all()
            
            for middle_element in all_middle_elements:
                try:
                    middle_link_element = middle_element.locator('.part-ttl > a').first
                    middle_name = (await middle_link_element.inner_text()).strip()

                    if middle_name not in INCLUSION_LIST.get(major['name'], {}):
                        continue
                    
                    if self.target_middle_category and middle_name != self.target_middle_category:
                        continue
                    
                    log(f"  ì¤‘ë¶„ë¥˜ '{middle_name}' ì²˜ë¦¬ ì‹œìž‘...")

                    sub_links_elements = await middle_element.locator('.part-list li a').all()
                    sub_links_to_crawl = []
                    
                    for sub_link_element in sub_links_elements:
                        sub_name = (await sub_link_element.inner_text()).strip()
                        
                        if sub_name in INCLUSION_LIST.get(major['name'], {}).get(middle_name, {}):
                            if self.target_sub_category and sub_name != self.target_sub_category:
                                continue

                            href = await sub_link_element.get_attribute('href')
                            sub_links_to_crawl.append({'name': sub_name, 'href': href})

                    if sub_links_to_crawl:
                        await self._crawl_subcategories_parallel(major['name'], middle_name, sub_links_to_crawl)
                    else:
                        log(f"    ì¤‘ë¶„ë¥˜ '{middle_name}': INCLUSION_LISTì— í¬í•¨ëœ ì²˜ë¦¬í•  ì†Œë¶„ë¥˜ê°€ ì—†ìŠµë‹ˆë‹¤.")

                except Exception as e:
                    log(f"  ì¤‘ë¶„ë¥˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", "ERROR")
                    continue

    async def _crawl_subcategories_parallel(self, major_name, middle_name, sub_categories_info):
        """[ìˆ˜ì •ë³¸] ì†Œë¶„ë¥˜ ë³‘ë ¬ í¬ë¡¤ë§ í›„ ì¤‘ë¶„ë¥˜ ë‹¨ìœ„ë¡œ ì €ìž¥ ë° ìºì‹œ ë¬´íš¨í™”"""
        if not sub_categories_info:
            log(f"    ì¤‘ë¶„ë¥˜ '{middle_name}': ì²˜ë¦¬í•  ì†Œë¶„ë¥˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        log(f"    ì¤‘ë¶„ë¥˜ '{middle_name}': {len(sub_categories_info)}ê°œ ì†Œë¶„ë¥˜ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

        tasks = [self._crawl_single_subcategory(major_name, middle_name, sub_info) for sub_info in sub_categories_info]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        all_data_for_middle_category = []
        for i, result in enumerate(results):
            sub_name = sub_categories_info[i]['name']
            if isinstance(result, Exception):
                log(f"    âŒ ì†Œë¶„ë¥˜ '{sub_name}' ì²˜ë¦¬ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜: {result}", "ERROR")
            elif result: # resultê°€ Noneì´ë‚˜ ë¹ˆ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ ê²½ìš°
                all_data_for_middle_category.extend(result)
        
        if all_data_for_middle_category:
            log(f"  [DB ì €ìž¥] ì¤‘ë¶„ë¥˜ '{middle_name}'ì˜ ì „ì²´ ë°ì´í„° {len(all_data_for_middle_category)}ê°œë¥¼ ì €ìž¥í•©ë‹ˆë‹¤.")
            saved_count = await self.processor.save_to_supabase(all_data_for_middle_category, 'kpi_price_data')
            if saved_count > 0:
                log(f"  [ìºì‹œ ë¬´íš¨í™”] ì¤‘ë¶„ë¥˜ '{middle_name}' ê´€ë ¨ ìºì‹œë¥¼ ë¬´íš¨í™”í•©ë‹ˆë‹¤.")
                await self.clear_redis_cache() # ì¤‘ë¶„ë¥˜ ì™„ë£Œ í›„ ì „ì²´ ìºì‹œ ë¬´íš¨í™”
        else:
            log(f"  ì¤‘ë¶„ë¥˜ '{middle_name}'ì—ì„œ ìµœì¢… ì €ìž¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    async def _crawl_single_subcategory(self, major_name, middle_name, sub_info):
        """[ìˆ˜ì •ë³¸] ë‹¨ì¼ ì†Œë¶„ë¥˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬ List[Dict] í˜•íƒœë¡œ 'ë°˜í™˜'"""
        async with self.semaphore:
            sub_name = sub_info['name']
            sub_href = sub_info['href']
            sub_url = f"{self.base_url}/www/price/{sub_href}"

            log(f"    - '{sub_name}' ìˆ˜ì§‘ ì‹œìž‘")
            new_page = None
            max_retries = 3
            
            for attempt in range(max_retries):
                try:
                    new_page = await self.context.new_page()
                    await new_page.goto(sub_url, timeout=60000, wait_until="networkidle")
                    await new_page.click('a[href*="detail_change.asp"]', timeout=15000)
                    
                    spec_dropdown_selector = 'select#ITEM_SPEC_CD'
                    await new_page.wait_for_selector(spec_dropdown_selector, timeout=30000)

                    options = await new_page.locator(f'{spec_dropdown_selector} option').all()
                    specs_to_crawl = [{'name': (await o.inner_text()).strip(), 'value': await o.get_attribute('value')} for o in options if await o.get_attribute('value')]
                    
                    if not specs_to_crawl:
                        await new_page.close()
                        return []

                    all_crawled_data = []
                    for i, spec in enumerate(specs_to_crawl):
                        try:
                            await new_page.select_option(spec_dropdown_selector, value=spec['value'])

                            if i == 0:
                                await new_page.select_option('select#DATA_YEAR_F', value=self.start_year)
                                await new_page.select_option('select#DATA_MONTH_F', value=self.start_month)
                                
                                end_year_selector = 'select#DATA_YEAR_T'
                                latest_year = await new_page.locator(f'{end_year_selector} option').first.get_attribute('value')
                                await new_page.select_option(end_year_selector, value=latest_year)
                                
                                end_month_selector = 'select#DATA_MONTH_T'
                                latest_month = await new_page.locator(f'{end_month_selector} option').first.get_attribute('value')
                                await new_page.select_option(end_month_selector, value=latest_month)
                                log(f"      - ê¸°ê°„ ì„¤ì •: {self.start_year}-{self.start_month} ~ {latest_year}-{latest_month}")
                            
                            async with new_page.expect_response(lambda r: "detail_change.asp" in r.url, timeout=30000):
                                await new_page.click('form[name="sForm"] input[type="image"]')
                            
                            await new_page.wait_for_selector('table#priceTrendDataArea tr:nth-child(2)', timeout=15000)

                            unit = self._get_unit_from_inclusion_list(major_name, middle_name, sub_name, spec['name'])
                            
                            headers = [th.strip() for th in await new_page.locator('table#priceTrendDataArea th').all_inner_texts()]
                            
                            table_type = "region"
                            if len(headers) > 1:
                                header_sample = headers[1]
                                if 'ê°€ê²©' in header_sample or re.match(r'ê°€[â‘ -â‘©]', header_sample): table_type = "price_name"
                                elif not self._is_region_header(header_sample): table_type = "detail_spec"
                            
                            rows = await new_page.locator('table#priceTrendDataArea tr').all()
                            for row in rows[1:]:
                                cols_text = await row.locator('td').all_inner_texts()
                                if not cols_text: continue
                                date, prices_text = cols_text[0].strip(), [p.strip().replace(',', '') for p in cols_text[1:]]
                                for idx, header in enumerate(headers[1:]):
                                    if idx < len(prices_text) and prices_text[idx].isdigit():
                                        region = "ì „êµ­" if table_type != "region" else header
                                        detail_spec = header if table_type != "region" else None
                                        all_crawled_data.append(self._create_data_entry(major_name, middle_name, sub_name, spec['name'], region, detail_spec, date, prices_text[idx], unit))
                        except Exception as spec_e:
                            log(f"      - Spec '{spec.get('name', 'N/A')[:20]}...' ì²˜ë¦¬ ì˜¤ë¥˜: {spec_e}", "WARNING")
                            continue

                    await new_page.close()
                    log(f"    - '{sub_name}' ì™„ë£Œ: {len(all_crawled_data)}ê°œ ë°ì´í„° ìˆ˜ì§‘.")
                    return all_crawled_data

                except Exception as e:
                    if new_page: await new_page.close()
                    if attempt == max_retries - 1:
                        log(f"    âŒ '{sub_name}' ìµœì¢… ì‹¤íŒ¨: {str(e)}", "ERROR")
                        return []
                    log(f"    âš ï¸ '{sub_name}' ìž¬ì‹œë„ {attempt + 1}/{max_retries}: {str(e)}", "WARNING")
                    await asyncio.sleep(5)
        return []

    async def clear_redis_cache(self, major_name: str = None, middle_name: str = None):
        """[ìˆ˜ì •ë³¸] ì¤‘ë¶„ë¥˜ ë‹¨ìœ„ ë˜ëŠ” ì „ì²´ ìºì‹œ ë¬´íš¨í™”"""
        if self.redis is None:
            log("  âš ï¸ Redis ë¹„í™œì„±í™”, ìºì‹œ ì‚­ì œ ê±´ë„ˆëœ€.", "WARNING")
            return
            
        try:
            # íŠ¹ì • ì¤‘ë¶„ë¥˜ì— ëŒ€í•œ íŒ¨í„´ ìƒì„± ëŒ€ì‹ , ê°„ë‹¨í•˜ê²Œ ëª¨ë“  ìžìž¬ ê°€ê²© ìºì‹œë¥¼ ì‚­ì œ
            # ì´ëŠ” ë” ë‹¨ìˆœí•˜ê³  í™•ì‹¤í•œ ë°©ë²•
            keys_to_delete = [key async for key in self.redis.scan_iter("material_prices:*")]
            if keys_to_delete:
                await self.redis.delete(*keys_to_delete)
                log(f"  âœ… Redis ìºì‹œ ë¬´íš¨í™” ì„±ê³µ: {len(keys_to_delete)}ê°œ í‚¤ ì‚­ì œ")
            else:
                log("  âœ… ì‚­ì œí•  Redis 'material_prices:*' ìºì‹œê°€ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            log(f"  âŒ Redis ìºì‹œ ì‚­ì œ ì‹¤íŒ¨: {str(e)}", "ERROR")

    def _is_region_header(self, header_text):
        """í—¤ë”ê°€ ì¼ë°˜ì ì¸ ì§€ì—­ëª…ì¸ì§€ íŒë³„"""
        known_regions = ["ì„œìš¸", "ë¶€ì‚°", "ëŒ€êµ¬", "ì¸ì²œ", "ê´‘ì£¼", "ëŒ€ì „", "ìš¸ì‚°", "ì„¸ì¢…", "ê²½ê¸°", "ê°•ì›", "ì¶©ë¶", "ì¶©ë‚¨", "ì „ë¶", "ì „ë‚¨", "ê²½ë¶", "ê²½ë‚¨", "ì œì£¼", "ìˆ˜ì›"]
        return any(region in header_text for region in known_regions)

    def _create_data_entry(self, major, middle, sub, spec, region, detail_spec, date, price, unit):
        """ë°ì´í„°ë² ì´ìŠ¤ ì €ìž¥ì„ ìœ„í•œ ë”•ì…”ë„ˆë¦¬ ê°ì²´ ìƒì„±"""
        final_spec = f"{spec} - {detail_spec}" if detail_spec else spec
        return {'major_category': major, 'middle_category': middle, 'sub_category': sub,
                'specification': final_spec, 'region': region, 'date': f"{date.replace('.', '-')}-01",
                'price': int(price), 'unit': unit}

    def _get_unit_from_inclusion_list(self, major_name, middle_name, sub_name, spec_name=None):
        """INCLUSION_LISTì—ì„œ ë‹¨ìœ„ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        try:
            unit_data = INCLUSION_LIST.get(major_name, {}).get(middle_name, {}).get(sub_name)
            if isinstance(unit_data, dict):
                if spec_name and 'specifications' in unit_data:
                    specs = unit_data['specifications']
                    if spec_name in specs: return specs[spec_name]
                    for key, val in specs.items():
                        if spec_name in key or key in spec_name: return val
                return unit_data.get("unit") or unit_data.get("default")
            elif isinstance(unit_data, str):
                return unit_data
        except Exception as e:
            log(f"ë‹¨ìœ„ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}", "ERROR")
        return None

# --- 5. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ---
async def main():
    """ë©”ì¸ ì‹¤í–‰ ë¡œì§: ëª…ë ¹í–‰ ì¸ìž íŒŒì‹± ë° í¬ë¡¤ëŸ¬ ì‹¤í–‰"""
    args = {arg.split('=', 1)[0].strip('-'): arg.split('=', 1)[1].strip('"\'') for arg in sys.argv[1:] if '=' in arg}
    
    target_major = args.get('major')
    crawl_mode = "major_only" if target_major else "all"
    
    start_year = args.get('start-year', '2020')
    start_month = args.get('start-month', '01').zfill(2)

    log(f"í¬ë¡¤ë§ ëª¨ë“œ: {crawl_mode}, íƒ€ê²Ÿ: {target_major or 'ì „ì²´'}, ì‹œìž‘: {start_year}-{start_month}", "SUMMARY")

    if crawl_mode == "all":
        for major_name in INCLUSION_LIST.keys():
            log(f"\n=== ëŒ€ë¶„ë¥˜: {major_name} í¬ë¡¤ë§ ì‹œìž‘ ===", "SUMMARY")
            crawler = KpiCrawler(target_major=major_name, crawl_mode="major_only", start_year=start_year, start_month=start_month)
            await crawler.run()
    else:
        crawler = KpiCrawler(target_major=target_major, crawl_mode=crawl_mode, start_year=start_year, start_month=start_month)
        await crawler.run()

if __name__ == "__main__":
    # ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ í¬ë¡¤ëŸ¬ê°€ ìžˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ë¡œì§ (ì„ íƒì )
    # for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
    #     if 'kpi_crawler.py' in str(proc.info('cmdline')) and proc.pid != os.getpid():
    #         log("ì´ë¯¸ ì‹¤í–‰ ì¤‘ì¸ í¬ë¡¤ëŸ¬ê°€ ìžˆìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.", "ERROR")
    #         sys.exit(1)
            
    asyncio.run(main())
import os
import json
import re
import pandas as pd
import requests
from datetime import datetime
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv
from supabase import create_client, Client
from urllib.parse import urlparse

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv("../../.env.local")
# ìƒëŒ€ ê²½ë¡œê°€ ì‘ë™í•˜ì§€ ì•Šì„ ê²½ìš° ì ˆëŒ€ ê²½ë¡œ ì‹œë„
if not os.environ.get("NEXT_PUBLIC_SUPABASE_URL"):
    load_dotenv(os.path.join(os.path.dirname(__file__), "../../.env.local"))

# Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
SUPABASE_URL = os.environ.get("NEXT_PUBLIC_SUPABASE_URL")
SUPABASE_KEY = os.environ.get("NEXT_PUBLIC_SUPABASE_ANON_KEY")
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# Redis REST API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
redis_client = None
redis_connection_failed = False
redis_rest_url = None
redis_rest_token = None

try:
    # REST API í™˜ê²½ë³€ìˆ˜ ìš°ì„  í™•ì¸
    redis_rest_url = os.environ.get("UPSTASH_REDIS_REST_URL")
    redis_rest_token = os.environ.get("UPSTASH_REDIS_REST_TOKEN")
    
    if redis_rest_url and redis_rest_token:
        # HTTP ê¸°ë°˜ Redis ì—°ê²° í…ŒìŠ¤íŠ¸
        response = requests.get(
            f"{redis_rest_url}/ping",
            headers={"Authorization": f"Bearer {redis_rest_token}"},
            timeout=10
        )
        if response.status_code == 200:
            print(f"[{datetime.now().strftime('%H:%M:%S')}] âœ“ Redis REST API ì—°ê²° ì„±ê³µ")
        else:
            raise Exception(f"REST API ì—°ê²° ì‹¤íŒ¨: {response.status_code}")
    else:
        # ê¸°ì¡´ Redis URLì—ì„œ REST API ì •ë³´ ì¶”ì¶œ ì‹œë„
        REDIS_URL = os.environ.get("REDIS_URL")
        if REDIS_URL:
            parsed = urlparse(REDIS_URL)
            redis_rest_url = f"https://{parsed.hostname}"
            redis_rest_token = parsed.password
            
            # ì—°ê²° í…ŒìŠ¤íŠ¸
            response = requests.get(
                f"{redis_rest_url}/ping",
                headers={"Authorization": f"Bearer {redis_rest_token}"},
                timeout=10
            )
            if response.status_code == 200:
                print(f"[{datetime.now().strftime('%H:%M:%S')}] âœ“ Redis REST API ì—°ê²° ì„±ê³µ (URLì—ì„œ ì¶”ì¶œ)")
            else:
                raise Exception(f"REST API ì—°ê²° ì‹¤íŒ¨: {response.status_code}")
        else:
            raise Exception("Redis í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
except Exception as e:
    print(f"[{datetime.now().strftime('%H:%M:%S')}] âœ— Redis ì—°ê²° ì‹¤íŒ¨: {e}")
    print(f"[{datetime.now().strftime('%H:%M:%S')}] â„¹ Redis ìºì‹± ë¹„í™œì„±í™” - Supabase ì§ì ‘ ì¡°íšŒë¡œ ì§„í–‰")
    redis_client = None
    redis_connection_failed = True
    redis_rest_url = None
    redis_rest_token = None


def log(message: str, level: str = "INFO"):
    """ì‹¤í–‰ ê³¼ì • ë¡œê·¸ë¥¼ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        message: ë¡œê·¸ ë©”ì‹œì§€
        level: ë¡œê·¸ ë ˆë²¨ (INFO, SUCCESS, ERROR, SUMMARY)
    """
    # ë¡œê·¸ ë ˆë²¨ë³„ ì¶œë ¥ ì œì–´
    if level == "SUMMARY":
        print(f"[{datetime.now().strftime('%H:%M:%S')}] âœ“ {message}")
    elif level == "ERROR":
        print(f"[{datetime.now().strftime('%H:%M:%S')}] âœ— {message}")
    elif level == "SUCCESS":
        print(f"[{datetime.now().strftime('%H:%M:%S')}] âœ“ {message}")
    else:  # INFO
        print(f"[{datetime.now().strftime('%H:%M:%S')}] {message}")


class BaseDataProcessor(ABC):
    """ëª¨ë“  ì‚¬ì´íŠ¸ë³„ ë°ì´í„° ì²˜ë¦¬ê¸°ì˜ ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.raw_data_list: List[Dict[str, Any]] = []
        self.processed_data_list: List[Dict[str, Any]] = []
    
    def add_raw_data(self, data: Dict[str, Any]):
        """íŒŒì‹±ëœ ì›ë³¸ ë°ì´í„°ë¥¼ ì¶”ê°€"""
        self.raw_data_list.append(data)
    
    @abstractmethod
    def transform_to_standard_format(self, raw_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ì‚¬ì´íŠ¸ë³„ ì›ë³¸ ë°ì´í„°ë¥¼ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì¶”ìƒ ë©”ì„œë“œ"""
        pass
    
    def to_dataframe(self) -> pd.DataFrame:
        """ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ í‘œì¤€ í˜•ì‹ì˜ Pandas DataFrameìœ¼ë¡œ ë³€í™˜"""
        if not self.raw_data_list:
            return pd.DataFrame()
        
        self.processed_data_list = []
        
        # ê° ì›ë³¸ ë°ì´í„°ë¥¼ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        for raw_item in self.raw_data_list:
            transformed_items = self.transform_to_standard_format(raw_item)
            self.processed_data_list.extend(transformed_items)
        
        return pd.DataFrame(self.processed_data_list)
    
    def check_existing_data(self, major_category: str, middle_category: str, 
                           sub_category: str, specification: str, 
                           table_name: str = 'kpi_price_data') -> set:
        """Supabase ë˜ëŠ” Redis ìºì‹œì—ì„œ ê¸°ì¡´ ë°ì´í„°ì˜ (ë‚ ì§œ, ì§€ì—­, ê°€ê²©, ê·œê²©) ì¡°í•©ì„ í™•ì¸"""
        
        # global ë³€ìˆ˜ ì„ ì–¸
        global redis_connection_failed, redis_rest_url, redis_rest_token
        
        # 1. ìºì‹œ í‚¤(key) ìƒì„±: ê° í’ˆëª©ë³„ë¡œ ê³ ìœ í•œ í‚¤ë¥¼ ë§Œë“¦
        cache_key = f"existing_data:{table_name}:{major_category}:{middle_category}:{sub_category}:{specification}"

        # 2. Redis ìºì‹œ ë¨¼ì € í™•ì¸ (Cache HIT) - ì—°ê²° ìƒíƒœ í™•ì¸ í¬í•¨
        if not redis_connection_failed and redis_rest_url and redis_rest_token:
            try:
                # HTTP ê¸°ë°˜ Redis GET ìš”ì²­
                response = requests.get(
                    f"{redis_rest_url}/get/{cache_key}",
                    headers={"Authorization": f"Bearer {redis_rest_token}"},
                    timeout=10
                )
                
                if response.status_code == 200:
                    result = response.json()
                    if result.get("result"):
                        log(f"        - [Cache HIT] Redisì—ì„œ ê¸°ì¡´ ë°ì´í„° ë¡œë“œ")
                        # RedisëŠ” ë¬¸ìì—´ë¡œ ì €ì¥ë˜ë¯€ë¡œ, ì›ë˜ì˜ set í˜•íƒœë¡œ ë³€í™˜
                        loaded_list = json.loads(result["result"])
                        return {tuple(item) for item in loaded_list}
                elif response.status_code == 404:
                    # ìºì‹œì— ë°ì´í„°ê°€ ì—†ìŒ (ì •ìƒ)
                    pass
                else:
                    raise Exception(f"Redis GET ì‹¤íŒ¨: {response.status_code}")
            except Exception as e:
                log(f"        - Redis ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}", "ERROR")
                redis_connection_failed = True

        # 3. ìºì‹œê°€ ì—†ìœ¼ë©´(Cache MISS) Supabaseì—ì„œ ë°ì´í„° ì¡°íšŒ (ê¸°ì¡´ ë¡œì§)
        if redis_connection_failed:
            log(f"        - [Redis ë¹„í™œì„±í™”] Supabaseì—ì„œ ê¸°ì¡´ ë°ì´í„° ì¡°íšŒ")
        else:
            log(f"        - [Cache MISS] Supabaseì—ì„œ ê¸°ì¡´ ë°ì´í„° ì¡°íšŒ")
        
        try:
            response = supabase.table(table_name).select(
                'date, region, price, specification'
            ).eq(
                'major_category', major_category
            ).eq(
                'middle_category', middle_category
            ).eq(
                'sub_category', sub_category
            ).eq(
                'specification', specification
            ).execute()
            
            existing_data_set = set()
            if response.data:
                for item in response.data:
                    combo = (item['date'], item['region'], str(item['price']), item['specification'])
                    existing_data_set.add(combo)
                log(f"        - ê¸°ì¡´ ë°ì´í„° ë°œê²¬: {len(existing_data_set)}ê°œ")
            else:
                log("        - ê¸°ì¡´ ë°ì´í„° ì—†ìŒ: ì „ì²´ ì¶”ì¶œ í•„ìš”")
            
            # 4. ì¡°íšŒí•œ ê²°ê³¼ë¥¼ Redisì— ì €ì¥ (24ì‹œê°„ ë™ì•ˆ ìœ íš¨) - Redis ì—°ê²° ìƒíƒœ í™•ì¸
            if not redis_connection_failed and redis_rest_url and redis_rest_token:
                try:
                    # Pythonì˜ setì€ JSONìœ¼ë¡œ ë°”ë¡œ ë³€í™˜ ë¶ˆê°€í•˜ë¯€ë¡œ listë¡œ ë³€ê²½
                    data_to_cache = [list(item) for item in existing_data_set]
                    # HTTP ê¸°ë°˜ Redis SET ìš”ì²­ (TTL 24ì‹œê°„)
                    response = requests.post(
                        f"{redis_rest_url}/set/{cache_key}",
                        headers={"Authorization": f"Bearer {redis_rest_token}"},
                        json={"value": json.dumps(data_to_cache), "ex": 86400},
                        timeout=10
                    )
                    
                    if response.status_code == 200:
                        log(f"        - ì¡°íšŒëœ ë°ì´í„°ë¥¼ Redisì— 24ì‹œê°„ ë™ì•ˆ ìºì‹± ì™„ë£Œ")
                    else:
                        log(f"        - Redis ìºì‹± ì‹¤íŒ¨: {response.status_code}", "WARNING")
                except Exception as e:
                    log(f"        - Redis ìºì‹± ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰): {e}", "WARNING")
            
            return existing_data_set
                
        except Exception as e:
            log(f"        - Supabase í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return set()  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ set ë°˜í™˜
    
    def save_to_cache(self, major_category: str, year: int, month: int, data: List[Dict]):
        """
        í¬ë¡¤ë§í•œ ë°ì´í„°ë¥¼ Redis ìºì‹œì— ì €ì¥ (24ì‹œê°„ TTL)
        """
        global redis_connection_failed, redis_rest_url, redis_rest_token
        
        if redis_connection_failed or not redis_rest_url or not redis_rest_token:
            return
        
        cache_key = f"kpi_data:{major_category}:{year}:{month:02d}"
        
        try:
            # HTTP ê¸°ë°˜ Redis SET ìš”ì²­ (TTL 24ì‹œê°„)
            response = requests.post(
                f"{redis_rest_url}/set/{cache_key}",
                headers={"Authorization": f"Bearer {redis_rest_token}"},
                json={"value": json.dumps(data), "ex": 86400},
                timeout=10
            )
            
            if response.status_code == 200:
                print(f"[{datetime.now().strftime('%H:%M:%S')}] âœ“ í¬ë¡¤ë§ ë°ì´í„° Redis ìºì‹± ì™„ë£Œ ({len(data)}ê°œ í•­ëª©)")
            else:
                print(f"[{datetime.now().strftime('%H:%M:%S')}] âš  Redis ìºì‹± ì‹¤íŒ¨: {response.status_code}")
                
        except Exception as e:
            print(f"[{datetime.now().strftime('%H:%M:%S')}] âš  Redis ìºì‹± ì¤‘ ì˜¤ë¥˜: {e}")
    
    def filter_new_data_only(self, df: pd.DataFrame, table_name: str = 'kpi_price_data') -> pd.DataFrame:
        """ê¸°ì¡´ ë°ì´í„°ì™€ ë¹„êµí•˜ì—¬ ìƒˆë¡œìš´ ë°ì´í„°ë§Œ í•„í„°ë§ (ë‚ ì§œ-ì§€ì—­-ê°€ê²©-ê·œê²© ì¡°í•© ê¸°ì¤€)"""
        if df.empty:
            return df
        
        # pandasë¥¼ í™œìš©í•œ ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ë° ì •ì œ
        log("ğŸ“Š pandasë¥¼ í™œìš©í•œ ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì‹œì‘...")
        
        # 1. í•„ìˆ˜ í•„ë“œ null ê°’ ì œê±°
        original_count = len(df)
        df = df.dropna(subset=['region', 'price', 'date', 'specification'])
        after_null_check = len(df)
        if original_count != after_null_check:
            log(f"    - í•„ìˆ˜ í•„ë“œ null ì œê±°: {original_count - after_null_check}ê°œ")
        
        # 2. ìœ íš¨í•˜ì§€ ì•Šì€ ê°€ê²© ì œê±° (0 ì´í•˜ ë˜ëŠ” ë¹„ì •ìƒì ìœ¼ë¡œ í° ê°’)
        df = df[(df['price'] > 0) & (df['price'] < 999999999)]
        after_price_check = len(df)
        if after_null_check != after_price_check:
            log(f"    - ìœ íš¨í•˜ì§€ ì•Šì€ ê°€ê²© ì œê±°: {after_null_check - after_price_check}ê°œ")
        
        # 3. ì§€ì—­ëª… í‘œì¤€í™” ë° ê²€ì¦
        valid_regions = ['ê°•ì›', 'ê²½ê¸°', 'ê²½ë‚¨', 'ê²½ë¶', 'ê´‘ì£¼', 'ëŒ€êµ¬', 'ëŒ€ì „', 
                        'ë¶€ì‚°', 'ì„œìš¸', 'ì„¸ì¢…', 'ìš¸ì‚°', 'ì¸ì²œ', 'ì „ë‚¨', 'ì „ë¶', 
                        'ì œì£¼', 'ì¶©ë‚¨', 'ì¶©ë¶', 'ìˆ˜ì›', 'ì„±ë‚¨', 'ì¶˜ì²œ']
        
        # ì§€ì—­ëª… ì²˜ë¦¬ ë° ê²€ì¦
        def process_region_name(region_name):
            if not region_name or pd.isna(region_name):
                return 'ì „êµ­'  # ì§€ì—­ ì •ë³´ê°€ ì—†ìœ¼ë©´ 'ì „êµ­'ìœ¼ë¡œ ì²˜ë¦¬
            
            region_str = str(region_name).strip()
            
            # ìœ íš¨í•œ ì§€ì—­ëª…ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            if any(valid_region in region_str for valid_region in valid_regions):
                return region_str
            
            # ê³µí†µì§€ì—­ì´ë‚˜ íŠ¹ì • íŒ¨í„´ í™•ì¸ (ì˜ˆ: ë‚ ì§œì™€ ê°€ê²©ë§Œ ìˆëŠ” ê²½ìš°)
            # ìˆ«ìë‚˜ íŠ¹ìˆ˜ë¬¸ìë§Œ ìˆê±°ë‚˜, ê°€ê²© íŒ¨í„´ì´ ìˆëŠ” ê²½ìš° 'ì „êµ­'ìœ¼ë¡œ ì²˜ë¦¬
            if (region_str.replace(',', '').replace('.', '').replace('-', '').isdigit() or
                'ì›' in region_str or 
                len(region_str) < 2 or
                region_str in ['ê³µí†µ', 'ì „ì²´', 'ê¸°íƒ€', 'ì¼ë°˜']):
                return 'ì „êµ­'
            
            # ê¸°íƒ€ ê²½ìš°ë„ 'ì „êµ­'ìœ¼ë¡œ ì²˜ë¦¬ (ë„ˆë¬´ ì—„ê²©í•œ ê²€ì¦ ë°©ì§€)
            return 'ì „êµ­'
        
        # ì§€ì—­ëª… ì²˜ë¦¬ ì ìš©
        df['region'] = df['region'].apply(process_region_name)
        after_region_check = len(df)
        
        # ì²˜ë¦¬ ê²°ê³¼ ë¡œê¹…
        region_counts = df['region'].value_counts()
        nationwide_count = region_counts.get('ì „êµ­', 0)
        if nationwide_count > 0:
            log(f"    - 'ì „êµ­'ìœ¼ë¡œ ì²˜ë¦¬ëœ ë°ì´í„°: {nationwide_count}ê°œ")
        
        # ì´ì œ ëª¨ë“  ë°ì´í„°ê°€ ìœ íš¨í•œ ì§€ì—­ëª…ì„ ê°€ì§€ë¯€ë¡œ ì œê±°ë˜ëŠ” ë°ì´í„°ëŠ” ì—†ìŒ
        if after_price_check != after_region_check:
            log(f"    - ì§€ì—­ëª… ì²˜ë¦¬ ì™„ë£Œ: {after_price_check}ê°œ â†’ {after_region_check}ê°œ")
        
        # 4. ì¤‘ë³µ ë°ì´í„° ì œê±° (ê°™ì€ ë‚ ì§œ, ì§€ì—­, ê·œê²©, ê°€ê²©)
        df = df.drop_duplicates(subset=['date', 'region', 'specification', 'price'])
        after_duplicate_check = len(df)
        if after_region_check != after_duplicate_check:
            log(f"    - ì¤‘ë³µ ë°ì´í„° ì œê±°: {after_region_check - after_duplicate_check}ê°œ")
        
        log(f"ğŸ“Š ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì™„ë£Œ: {original_count}ê°œ â†’ {after_duplicate_check}ê°œ")
        
        if df.empty:
            log("ğŸ“Š í’ˆì§ˆ ê²€ì¦ í›„ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return df
        
        category_groups = df.groupby(['major_category', 'middle_category', 'sub_category', 'specification'])
        
        new_records = []
        total_records = len(df)
        skipped_count = 0
        
        for (major_cat, middle_cat, sub_cat, spec), group_df in category_groups:
            log(f"    - ì¤‘ë³µ ì²´í¬: {major_cat} > {middle_cat} > {sub_cat} > {spec}")
            
            existing_data = self.check_existing_data(
                major_cat, middle_cat, sub_cat, spec, table_name
            )
            
            group_new_count = 0
            group_duplicate_count = 0
            
            for _, record in group_df.iterrows():
                record_key = (record['date'], record['region'], str(record['price']), record['specification'])
                if record_key not in existing_data:
                    new_records.append(record.to_dict())
                    group_new_count += 1
                else:
                    group_duplicate_count += 1
                    skipped_count += 1
            
            if group_duplicate_count > 0:
                log(f"        - ì¤‘ë³µ SKIP: {group_duplicate_count}ê°œ")
            log(f"        - ê·¸ë£¹ ê²°ê³¼: ì‹ ê·œ {group_new_count}ê°œ, ì¤‘ë³µ {group_duplicate_count}ê°œ")
        
        processed_count = len(new_records) + skipped_count
        if new_records:
            log(f"ğŸ“Š ì „ì²´ {total_records}ê°œ ì¤‘ ì‹ ê·œ {len(new_records)}ê°œ, ì¤‘ë³µ {skipped_count}ê°œ")
        else:
            if processed_count == total_records:
                log(f"ğŸ“Š ì „ì²´ {total_records}ê°œ ëª¨ë‘ ì¤‘ë³µ - ì €ì¥í•  ë°ì´í„° ì—†ìŒ")
            else:
                # ì‹¤ì œë¡œëŠ” ë‚˜ë¨¸ì§€ ë°ì´í„°ê°€ ì‹ ê·œ ë°ì´í„°ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
                unprocessed_count = total_records - processed_count
                log(f"ğŸ“Š ì „ì²´ {total_records}ê°œ ì¤‘ ì¤‘ë³µ {skipped_count}ê°œ, ì‹ ê·œ {unprocessed_count}ê°œ (ì¤‘ë³µ ì²´í¬ ë¯¸ì™„ë£Œ)")
        
        return pd.DataFrame(new_records)
    
    def save_to_supabase(self, df: pd.DataFrame, table_name: str, check_duplicates: bool = True):
        """DataFrameì„ Supabase í…Œì´ë¸”ì— ì €ì¥"""
        if df.empty:
            log("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return 0
        
        if check_duplicates:
            df_to_save = self.filter_new_data_only(df, table_name)
            if df_to_save.empty:
                log("ì €ì¥í•  ì‹ ê·œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return 0
        else:
            df_to_save = df
        
        try:
            records = df_to_save.to_dict(orient='records')
            valid_records = []
            for record in records:
                if self._is_valid_record(record):
                    # ë‚ ì§œ ì •ê·œí™”
                    record['date'] = self._normalize_date(record['date'])
                    
                    # unit í•„ë“œê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì„¤ì •
                    if 'unit' not in record or not record['unit']:
                        record['unit'] = 'kg'  # ê¸°ë³¸ ë‹¨ìœ„
                    
                    valid_records.append(record)
                else:
                    log(f"ìœ íš¨í•˜ì§€ ì•Šì€ ë°ì´í„° ì œì™¸: {record}")
            
            if not valid_records:
                log("ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ì–´ ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                return 0
            
            # Supabase upsertëŠ” ê¸°ë³¸ì ìœ¼ë¡œ 500~1000ê°œ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ë³´ë‚´ëŠ” ê²ƒì´ ì•ˆì •ì 
            chunk_size = 500
            total_chunks = (len(valid_records) + chunk_size - 1) // chunk_size
            saved_count = 0
            
            log(f"ğŸ”„ Supabase ì €ì¥ ì‹œì‘: {len(valid_records)}ê°œ ë°ì´í„°ë¥¼ {total_chunks}ê°œ ì²­í¬ë¡œ ë¶„í• ")
            
            for i in range(0, len(valid_records), chunk_size):
                chunk = valid_records[i:i + chunk_size]
                chunk_num = i // chunk_size + 1
                
                try:
                    log(f"ğŸ“¤ ì²­í¬ {chunk_num}/{total_chunks} ì €ì¥ ì‹œë„ ì¤‘... ({len(chunk)}ê°œ ë°ì´í„°)")
                    response = supabase.table(table_name).upsert(chunk).execute()
                    # Supabase Python í´ë¼ì´ì–¸íŠ¸ëŠ” response.dataì™€ response.countë¥¼ ë°˜í™˜
                    if response.data is not None:
                        chunk_saved = len(response.data)
                        saved_count += chunk_saved
                        log(f"ğŸ“¦ ì²­í¬ {chunk_num}/{total_chunks} ì €ì¥ ì™„ë£Œ ({chunk_saved}ê°œ)")
                    else:
                        log(f"âš ï¸ ì²­í¬ {chunk_num}/{total_chunks} ì €ì¥ ì‘ë‹µì´ ë¹„ì–´ìˆìŒ", "ERROR")
                        log(f"ğŸ” ì‘ë‹µ ìƒì„¸: {response}", "DEBUG")
                except Exception as chunk_error:
                    log(f"âŒ Supabase ì €ì¥ ì‹¤íŒ¨ (ì²­í¬ {chunk_num}): {str(chunk_error)}", "ERROR")
                    log(f"ğŸ” ì‹¤íŒ¨í•œ ì²­í¬ ë°ì´í„° ìƒ˜í”Œ: {chunk[0] if chunk else 'None'}", "DEBUG")
                    # ì‹¤íŒ¨í•œ ì²­í¬ëŠ” ê±´ë„ˆë›°ê³  ê³„ì† ì§„í–‰
                    continue

            log(f"ğŸ“Š ì´ {saved_count}ê°œ ë°ì´í„° â†’ '{table_name}' í…Œì´ë¸” ì €ì¥ ì™„ë£Œ")
            return saved_count
        except Exception as e:
            log(f"âŒ Supabase ì €ì¥ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}", "ERROR")
            log(f"ğŸ” ì˜ˆì™¸ ìƒì„¸: {type(e).__name__}: {str(e)}", "DEBUG")
            return 0
    
    def _is_valid_record(self, record: Dict[str, Any]) -> bool:
        """ë ˆì½”ë“œì˜ ìœ íš¨ì„±ì„ ê²€ì¦"""
        required_fields = ['major_category', 'middle_category', 'sub_category', 
                          'specification', 'region', 'date']
        
        for field in required_fields:
            if field not in record or pd.isna(record[field]) or not record[field]:
                return False
        
        if not self._is_valid_date_value(record['date']):
            return False
        
        price = record.get('price')
        if price is not None and not isinstance(price, (int, float)):
             # NaN ê°’ë„ ìœ íš¨í•˜ì§€ ì•Šì€ ê²ƒìœ¼ë¡œ ì²˜ë¦¬
            if pd.isna(price):
                return False
            return False
        
        return True
    
    def _is_valid_date_value(self, date_value: Any) -> bool:
        """ë‚ ì§œ ê°’ì´ ìœ íš¨í•œì§€ í™•ì¸"""
        if date_value is None or date_value == '':
            return False
        
        if isinstance(date_value, str):
            date_str = date_value.strip()
            
            # "2025. 1" í˜•ì‹ í—ˆìš©
            if re.match(r'^\d{4}\.\s*\d{1,2}$', date_str):
                return True
            
            # "2025-01-01" í˜•ì‹ í—ˆìš©
            if re.match(r'^\d{4}-\d{1,2}-\d{1,2}$', date_str):
                return True
            
            # "2025/1/1" í˜•ì‹ í—ˆìš©
            if re.match(r'^\d{4}/\d{1,2}/\d{1,2}$', date_str):
                return True
            
            return False
        
        # datetime ê°ì²´ëŠ” ìœ íš¨
        if hasattr(date_value, 'strftime'):
            return True
        
        return False
    
    def _normalize_date(self, date_value: Any) -> str:
        """ë‚ ì§œë¥¼ YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”"""
        if hasattr(date_value, 'strftime'):
            return date_value.strftime('%Y-%m-%d')
        elif isinstance(date_value, str):
            date_str = date_value.strip()
            
            # "2025. 1" í˜•ì‹ì„ "2025-01-01"ë¡œ ë³€í™˜
            if re.match(r'^\d{4}\.\s*\d{1,2}$', date_str):
                year, month = date_str.replace('.', '').split()
                return f"{year}-{int(month):02d}-01"
            
            # "2025/1" ë˜ëŠ” "2025-1" í˜•ì‹ ì²˜ë¦¬
            if '/' in date_str or '-' in date_str:
                date_str = date_str.replace('/', '-')
                parts = date_str.split('-')
                if len(parts) >= 2:
                    year, month = parts[0], parts[1]
                    day = parts[2] if len(parts) > 2 else '01'
                    return f"{year}-{int(month):02d}-{int(day):02d}"
            
            return date_str
        else:
            return str(date_value)
    
    def get_comparison_json(self) -> str:
        """ì›ë³¸ ë°ì´í„°ì™€ ê°€ê³µëœ ë°ì´í„°ë¥¼ ë¹„êµí•˜ëŠ” JSON ìƒì„±"""
        processed_df = self.to_dataframe()
        raw_data_converted = self._convert_datetime_to_string(self.raw_data_list)
        processed_data_converted = self._convert_datetime_to_string(
            processed_df.to_dict(orient='records'))
        
        return json.dumps({
            "raw_crawled_data": raw_data_converted,
            "pandas_processed_data": processed_data_converted
        }, ensure_ascii=False, indent=4)
    
    def _convert_datetime_to_string(self, obj: Any) -> Any:
        """datetime ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ëŠ” ì¬ê·€ í•¨ìˆ˜"""
        if isinstance(obj, datetime):
            return obj.isoformat()
        elif isinstance(obj, dict):
            return {key: self._convert_datetime_to_string(value) 
                   for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_datetime_to_string(item) for item in obj]
        else:
            return obj

# --- ì´í•˜ KpiDataProcessor, MaterialDataProcessor, create_data_processor í•¨ìˆ˜ëŠ” ë³€ê²½í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤. ---

class KpiDataProcessor(BaseDataProcessor):
    """í•œêµ­ë¬¼ê°€ì •ë³´(KPI) ì‚¬ì´íŠ¸ ì „ìš© ë°ì´í„° ì²˜ë¦¬ê¸°"""
    
    def _normalize_region_name(self, region_name: str) -> str:
        """ì§€ì—­ëª…ì„ ì •ê·œí™”í•˜ê³  ë¹ˆ ê°’ì´ë‚˜ Noneì„ ì²˜ë¦¬"""
        # Noneì´ë‚˜ ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬
        if not region_name or region_name == 'None' or str(region_name).strip() == '':
            return 'ì „êµ­'  # ê¸°ë³¸ê°’ìœ¼ë¡œ 'ì „êµ­' ì„¤ì •
            
        region_str = str(region_name).strip()
        
        # 'ê³µí†µì§€ì—­'ì„ 'ì „êµ­'ìœ¼ë¡œ ë³€í™˜
        if region_str == 'ê³µí†µì§€ì—­':
            return 'ì „êµ­'
            
        # íŒ¨í„´: ì§€ì—­ëª… ì²«ê¸€ì + ìˆ«ì + ì§€ì—­ëª… ë‚˜ë¨¸ì§€ (ì˜ˆ: ì„œ1ìš¸ â†’ ì„œìš¸1)
        pattern = r'^([ê°€-í£])(\d+)([ê°€-í£]+)$'
        match = re.match(pattern, region_str)
        
        if match:
            first_char, number, rest = match.groups()
            return f"{first_char}{rest}{number}"  # ì„œ1ìš¸ â†’ ì„œìš¸1
        
        return region_str  # ë³€í™˜ ë¶ˆê°€ëŠ¥í•œ ê²½ìš° ì›ë³¸ ë°˜í™˜
    
    def _extract_material_name_from_specification(self, specification: str) -> str:
        """SPECIFICATIONì—ì„œ ìì¬ëª…ì„ ì¶”ì¶œí•˜ëŠ” ê·œì¹™"""
        if not specification:
            return specification
        
        spec_str = str(specification).strip()
        
        # ê·œì¹™ 1: HDPE DC ê³ ì••ê´€ ê´€ë ¨
        if 'HDPE' in spec_str and 'DC' in spec_str and 'ê³ ì••ê´€' in spec_str:
            return f"{spec_str} - DCê³ ì••ê´€"
        
        # ê·œì¹™ 2: PVC ê´€ë ¨
        if 'PVC' in spec_str:
            if 'ìƒìˆ˜ë„ê´€' in spec_str:
                return f"{spec_str} - PVCìƒìˆ˜ë„ê´€"
            elif 'í•˜ìˆ˜ë„ê´€' in spec_str:
                return f"{spec_str} - PVCí•˜ìˆ˜ë„ê´€"
            elif 'ë°°ìˆ˜ê´€' in spec_str:
                return f"{spec_str} - PVCë°°ìˆ˜ê´€"
        
        # ê·œì¹™ 3: ì² ê·¼ ê´€ë ¨
        if 'ì² ê·¼' in spec_str:
            if 'SD' in spec_str:
                return f"{spec_str} - SDì² ê·¼"
            elif 'ì´í˜•' in spec_str:
                return f"{spec_str} - ì´í˜•ì² ê·¼"
        
        # ê·œì¹™ 4: ë ˆë¯¸ì½˜ ê´€ë ¨
        if 'ë ˆë¯¸ì½˜' in spec_str or 'ì½˜í¬ë¦¬íŠ¸' in spec_str:
            if 'ê³ ê°•ë„' in spec_str:
                return f"{spec_str} - ê³ ê°•ë„ì½˜í¬ë¦¬íŠ¸"
            elif 'ì¼ë°˜' in spec_str:
                return f"{spec_str} - ì¼ë°˜ì½˜í¬ë¦¬íŠ¸"
        
        # ê·œì¹™ 5: ì•„ìŠ¤íŒ”íŠ¸ ê´€ë ¨
        if 'ì•„ìŠ¤íŒ”íŠ¸' in spec_str:
            if 'í¬ì¥ìš©' in spec_str:
                return f"{spec_str} - í¬ì¥ìš©ì•„ìŠ¤íŒ”íŠ¸"
            elif 'ë°©ìˆ˜ìš©' in spec_str:
                return f"{spec_str} - ë°©ìˆ˜ìš©ì•„ìŠ¤íŒ”íŠ¸"
        
        # ê·œì¹™ 6: ê³¨ì¬ ê´€ë ¨
        if 'ê³¨ì¬' in spec_str:
            if 'ì‡„ì„' in spec_str:
                return f"{spec_str} - ì‡„ì„ê³¨ì¬"
            elif 'ëª¨ë˜' in spec_str:
                return f"{spec_str} - ëª¨ë˜ê³¨ì¬"
        
        # ê·œì¹™ 7: ì‹œë©˜íŠ¸ ê´€ë ¨
        if 'ì‹œë©˜íŠ¸' in spec_str:
            if 'í¬í‹€ëœë“œ' in spec_str:
                return f"{spec_str} - í¬í‹€ëœë“œì‹œë©˜íŠ¸"
            elif 'í˜¼í•©' in spec_str:
                return f"{spec_str} - í˜¼í•©ì‹œë©˜íŠ¸"
        
        # ê·œì¹™ 8: í˜•ê°• ê´€ë ¨
        if 'í˜•ê°•' in spec_str:
            if 'Hí˜•ê°•' in spec_str or 'H-' in spec_str:
                return f"{spec_str} - Hí˜•ê°•"
            elif 'Ií˜•ê°•' in spec_str or 'I-' in spec_str:
                return f"{spec_str} - Ií˜•ê°•"
        
        # ê·œì¹™ 9: ê°•ê´€ ê´€ë ¨
        if 'ê°•ê´€' in spec_str:
            if 'ë°°ê´€ìš©' in spec_str:
                return f"{spec_str} - ë°°ê´€ìš©ê°•ê´€"
            elif 'êµ¬ì¡°ìš©' in spec_str:
                return f"{spec_str} - êµ¬ì¡°ìš©ê°•ê´€"
        
        # ê·œì¹™ 10: ì „ì„  ê´€ë ¨
        if 'ì „ì„ ' in spec_str or 'ì¼€ì´ë¸”' in spec_str:
            if 'CV' in spec_str:
                return f"{spec_str} - CVì¼€ì´ë¸”"
            elif 'HIV' in spec_str:
                return f"{spec_str} - HIVì¼€ì´ë¸”"
            elif 'í†µì‹ ' in spec_str:
                return f"{spec_str} - í†µì‹ ì¼€ì´ë¸”"
        
        # ê¸°ë³¸ê°’: ì›ë³¸ specification ë°˜í™˜
        return spec_str

    async def process_data(self, major_category: str, middle_category: str, sub_category: str) -> List[Dict[str, Any]]:
        """ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë°ì´í„° ê°€ê³µ ë©”ì„œë“œ"""
        try:
            filtered_data = []
            for raw_data in self.raw_data_list:
                if (raw_data.get('major_category_name') == major_category and
                    raw_data.get('middle_category_name') == middle_category and
                    raw_data.get('sub_category_name') == sub_category):
                    filtered_data.append(raw_data)
            
            if not filtered_data:
                return []
            
            processed_items = []
            for raw_item in filtered_data:
                transformed_items = self.transform_to_standard_format(raw_item)
                processed_items.extend(transformed_items)
            
            return processed_items
            
        except Exception as e:
            log(f"ë°ì´í„° ê°€ê³µ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "ERROR")
            return []
    
    async def save_to_supabase(self, processed_data: List[Dict[str, Any]], table_name: str = 'kpi_price_data', check_duplicates: bool = True) -> int:
        """ê°€ê³µëœ ë°ì´í„°ë¥¼ Supabaseì— ì €ì¥"""
        if not processed_data:
            log("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return 0
        
        try:
            df = pd.DataFrame(processed_data)
            log(f"ğŸ“Š ì €ì¥ ì‹œë„: {len(df)}ê°œ ë°ì´í„° â†’ '{table_name}' í…Œì´ë¸”")
            
            # ë¶€ëª¨ í´ë˜ìŠ¤ì˜ save_to_supabase ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ì—¬ ì¤‘ë³µ ì œê±° ë° ì €ì¥ ë¡œì§ ì‹¤í–‰
            actual_saved_count = super().save_to_supabase(df, table_name, check_duplicates=check_duplicates)
            
            # ì‹¤ì œ ì €ì¥ëœ ê°œìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë©”ì‹œì§€ ì¶œë ¥
            if actual_saved_count > 0:
                log(f"âœ… ì €ì¥ ì™„ë£Œ: {actual_saved_count}ê°œ ë°ì´í„°")
            else:
                log(f"â„¹ï¸ ì €ì¥ ì™„ë£Œ: ì‹ ê·œ ë°ì´í„° ì—†ìŒ (ì „ì²´ {len(df)}ê°œ ëª¨ë‘ ì¤‘ë³µ)")
            
            return actual_saved_count
            
        except Exception as e:
            log(f"âŒ Supabase ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "ERROR")
            return 0
    
    def transform_to_standard_format(self, raw_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """KPI ì‚¬ì´íŠ¸ì˜ ì›ë³¸ ë°ì´í„°ë¥¼ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        transformed_items = []
        
        for spec_data in raw_data.get('spec_data', []):
            has_direct_price = (
                'spec_name' in spec_data and 'region' in spec_data and
                'date' in spec_data and 'price' in spec_data)
            
            if has_direct_price:
                price_value = spec_data.get('price')
                if price_value is not None:
                    try:
                        price_value = float(str(price_value).replace(',', ''))
                    except (ValueError, TypeError):
                        price_value = None
                
                # SPECIFICATIONì—ì„œ ìì¬ëª… ì¶”ì¶œ ì ìš©
                original_spec = spec_data['spec_name']
                enhanced_spec = self._extract_material_name_from_specification(original_spec)
                
                transformed_items.append({
                    'major_category': raw_data['major_category_name'],
                    'middle_category': raw_data['middle_category_name'],
                    'sub_category': raw_data['sub_category_name'],
                    'specification': enhanced_spec,
                    'unit': 'ì›/í†¤',
                    'region': self._normalize_region_name(spec_data['region']),
                    'date': spec_data['date'],
                    'price': price_value
                })
            else:
                for price_info in spec_data.get('prices', []):
                    price_value = None
                    if price_info.get('price'):
                        try:
                            price_value = float(str(price_info['price']).replace(',', ''))
                        except (ValueError, AttributeError):
                            price_value = None
                    
                    # SPECIFICATIONì—ì„œ ìì¬ëª… ì¶”ì¶œ ì ìš©
                    original_spec = spec_data['specification_name']
                    enhanced_spec = self._extract_material_name_from_specification(original_spec)
                    
                    transformed_items.append({
                        'major_category': raw_data['major_category_name'],
                        'middle_category': raw_data['middle_category_name'],
                        'sub_category': raw_data['sub_category_name'],
                        'specification': enhanced_spec,
                        'unit': spec_data.get('unit', 'ì›/í†¤'),
                        'region': self._normalize_region_name(price_info['region']),
                        'date': price_info['date'],
                        'price': price_value
                    })
        
        return transformed_items


class MaterialDataProcessor(BaseDataProcessor):
    """ë‹¤ë¥¸ ìì¬ ì‚¬ì´íŠ¸ìš© ë°ì´í„° ì²˜ë¦¬ê¸° (ì˜ˆì‹œ)"""
    
    def transform_to_standard_format(self, raw_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        transformed_items = []
        category = raw_data.get('category', '')
        product_name = raw_data.get('product_name', '')
        
        for price_item in raw_data.get('price_data', []):
            transformed_items.append({
                'major_category': category,
                'middle_category': '',
                'sub_category': product_name,
                'specification': product_name,
                'unit': 'ì›/í†¤',
                'region': price_item.get('location', ''),
                'date': price_item.get('date', ''),
                'price': price_item.get('cost')
            })
        
        return transformed_items


def create_data_processor(site_type: str) -> BaseDataProcessor:
    """ì‚¬ì´íŠ¸ íƒ€ì…ì— ë”°ë¥¸ ë°ì´í„° ì²˜ë¦¬ê¸° ìƒì„±"""
    processors = {
        'kpi': KpiDataProcessor,
        'material': MaterialDataProcessor,
    }
    
    processor_class = processors.get(site_type)
    if not processor_class:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‚¬ì´íŠ¸ íƒ€ì…: {site_type}")
    
    return processor_class()

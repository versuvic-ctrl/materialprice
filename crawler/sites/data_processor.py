import sys
import os
import json
import re
import pandas as pd
import requests
from datetime import datetime
from typing import List, Dict, Any
from abc import ABC, abstractmethod
from dotenv import load_dotenv
from supabase import create_client, Client
from urllib.parse import urlparse
from unit_validation import UnitValidator
from api_monitor import create_monitored_supabase_client
import redis
from upstash_redis import Redis


# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv("../../.env.local")
# ìƒëŒ€ ê²½ë¡œê°€ ì‘ë™í•˜ì§€ ì•Šì„ ê²½ìš° ì ˆëŒ€ ê²½ë¡œ ì‹œë„
if not os.environ.get("NEXT_PUBLIC_SUPABASE_URL"):
    load_dotenv(os.path.join(os.path.dirname(__file__), "../../.env.local"))

# ======================================================================
# 1. log í•¨ìˆ˜ ì •ì˜ë¥¼ ì´ê³³ìœ¼ë¡œ ì´ë™ì‹œí‚µë‹ˆë‹¤.
# ======================================================================
def log(message: str, level: str = "INFO"):
    """ì‹¤í–‰ ê³¼ì • ë¡œê·¸ë¥¼ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        message: ë¡œê·¸ ë©”ì‹œì§€
        level: ë¡œê·¸ ë ˆë²¨ (INFO, SUCCESS, ERROR, SUMMARY, WARNING)
    """
    # ë¡œê·¸ ë ˆë²¨ë³„ ì¶œë ¥ ì œì–´
    now = datetime.now().strftime('%H:%M:%S')
    if level == "SUMMARY":
        print(f"[{now}] âœ“ {message}", flush=True)
    elif level == "ERROR":
        print(f"[{now}] âœ— {message}", flush=True)
    elif level == "SUCCESS":
        print(f"[{now}] âœ“ {message}", flush=True)
    elif level == "WARNING":
        print(f"[{now}] âš ï¸ {message}", flush=True)
    else:  # INFO
        print(f"[{now}] {message}", flush=True)

# ======================================================================
# 2. ì´ì œ Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì½”ë“œê°€ log í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# ======================================================================
SUPABASE_URL = os.environ.get("NEXT_PUBLIC_SUPABASE_URL")
SUPABASE_KEY = os.environ.get("NEXT_PUBLIC_SUPABASE_ANON_KEY") # anon í‚¤ì˜ ì´ë¦„ì€ SUPABASE_KEYë¡œ ë³€ê²½í•´ë„ ë¬´ë°©í•©ë‹ˆë‹¤.
SUPABASE_SERVICE_KEY = os.environ.get("SUPABASE_SERVICE_KEY")

# ì„œë¹„ìŠ¤ í‚¤ê°€ ìˆìœ¼ë©´ ì„œë¹„ìŠ¤ í‚¤ë¥¼ ì‚¬ìš©, ì—†ìœ¼ë©´ anon í‚¤ë¥¼ ì‚¬ìš©
if SUPABASE_SERVICE_KEY:
    log("ğŸ”‘ Supabase Service Role í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
    _supabase_client = create_client(SUPABASE_URL, SUPABASE_SERVICE_KEY)
else:
    log("âš ï¸ Supabase ìµëª… í‚¤(anon key)ë¥¼ ì‚¬ìš©í•˜ì—¬ í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.", "WARNING")
    _supabase_client = create_client(SUPABASE_URL, SUPABASE_KEY)

# ======================================================================

# API ëª¨ë‹ˆí„°ë§ì´ ì ìš©ëœ í´ë¼ì´ì–¸íŠ¸ ìƒì„±
api_monitor = create_monitored_supabase_client(
    _supabase_client, 
    max_calls_per_minute=200,  # ë¶„ë‹¹ ìµœëŒ€ 200íšŒ
    max_calls_per_hour=2000    # ì‹œê°„ë‹¹ ìµœëŒ€ 2000íšŒ
)
supabase = api_monitor.client

# Supabase í´ë¼ì´ì–¸íŠ¸ íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ table ì ‘ê·¼ ë°©ë²•ì„ ì œê³µí•˜ëŠ” í—¬í¼ í•¨ìˆ˜
def get_supabase_table(client, table_name):
    """
    Supabase í´ë¼ì´ì–¸íŠ¸ íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ table ë©”ì„œë“œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    MonitoredSupabaseClientì˜ ê²½ìš° client.table()ì„, ì¼ë°˜ Clientì˜ ê²½ìš° table()ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    if hasattr(client, 'client') and hasattr(client.client, 'table'):
        # MonitoredSupabaseClientì¸ ê²½ìš°
        return client.client.table(table_name)
    elif hasattr(client, 'table'):
        # ì¼ë°˜ Clientì¸ ê²½ìš°
        return client.table(table_name)
    else:
        raise AttributeError(f"í´ë¼ì´ì–¸íŠ¸ ê°ì²´ì—ì„œ table ë©”ì„œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {type(client)}")
# Redis í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
try:
    # ë¨¼ì € UPSTASH_REDIS_REST_URL í™˜ê²½ ë³€ìˆ˜ í™•ì¸
    import os
    if 'UPSTASH_REDIS_REST_URL' in os.environ:
        redis = Redis.from_env()
    elif 'REDIS_URL' in os.environ:
        # GitHub Actionsì—ì„œ ì‚¬ìš©í•˜ëŠ” REDIS_URL í™˜ê²½ ë³€ìˆ˜ ì²˜ë¦¬
        redis_url = os.environ['REDIS_URL']
        redis_token = os.environ.get('REDIS_TOKEN', '')
        redis = Redis(url=redis_url, token=redis_token)
    else:
        # Redis í™˜ê²½ ë³€ìˆ˜ê°€ ì—†ëŠ” ê²½ìš° Noneìœ¼ë¡œ ì„¤ì •
        redis = None
        log("âš ï¸ Redis í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìºì‹œ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.", "WARNING")
except Exception as e:
    redis = None
    log(f"âš ï¸ Redis ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}. ìºì‹œ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.", "WARNING")
class BaseDataProcessor(ABC):
    """ëª¨ë“  ì‚¬ì´íŠ¸ë³„ ë°ì´í„° ì²˜ë¦¬ê¸°ì˜ ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.raw_data_list: List[Dict[str, Any]] = []
        self.processed_data_list: List[Dict[str, Any]] = []
        self.unit_validator = UnitValidator()  # ë‹¨ìœ„ ê²€ì¦ê¸° ì´ˆê¸°í™”
    
    def add_raw_data(self, data: Dict[str, Any]):
        """íŒŒì‹±ëœ ì›ë³¸ ë°ì´í„°ë¥¼ ì¶”ê°€"""
        self.raw_data_list.append(data)
    
    @abstractmethod
    def transform_to_standard_format(self, raw_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ì‚¬ì´íŠ¸ë³„ ì›ë³¸ ë°ì´í„°ë¥¼ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì¶”ìƒ ë©”ì„œë“œ"""
        pass
    
    def to_dataframe(self) -> pd.DataFrame:
        """ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ í‘œì¤€ í˜•ì‹ì˜ Pandas DataFrameìœ¼ë¡œ ë³€í™˜"""
        if not self.raw_data_list:
            return pd.DataFrame()
        
        self.processed_data_list = []
        
        # ê° ì›ë³¸ ë°ì´í„°ë¥¼ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        for raw_item in self.raw_data_list:
            transformed_items = self.transform_to_standard_format(raw_item)
            self.processed_data_list.extend(transformed_items)
        
        return pd.DataFrame(self.processed_data_list)
    
    def check_existing_data_smart(self, major_category: str, middle_category: str, 
                                 sub_category: str, specification: str, 
                                 table_name: str = 'kpi_price_data') -> Dict[str, Any]:
        """
        Supabaseì—ì„œ ê¸°ì¡´ ë°ì´í„°ë¥¼ ìŠ¤ë§ˆíŠ¸í•˜ê²Œ ë¶„ì„
        ë°˜í™˜ê°’:
        - existing_combinations: ê¸°ì¡´ (ë‚ ì§œ, ì§€ì—­, ê°€ê²©, ê·œê²©, ë‹¨ìœ„) ì¡°í•© set
        - existing_dates: ê¸°ì¡´ ë‚ ì§œ set
        - existing_units: ê¸°ì¡´ ë‹¨ìœ„ set
        - has_data: ë°ì´í„° ì¡´ì¬ ì—¬ë¶€
        """
        
        log(f"        - Supabaseì—ì„œ ê¸°ì¡´ ë°ì´í„° ìŠ¤ë§ˆíŠ¸ ë¶„ì„ ì¤‘")
        
        try:
            response = get_supabase_table(supabase, table_name).select(
                'date, region, price, specification, unit'
            ).eq(
                'major_category', major_category
            ).eq(
                'middle_category', middle_category
            ).eq(
                'sub_category', sub_category
            ).eq(
                'specification', specification
            ).execute()
            
            existing_combinations = set()
            existing_dates = set()
            existing_units = set()
            
            if response.data:
                for item in response.data:
                    combo = (item['date'], item['region'], str(item['price']), 
                            item['specification'], item['unit'])
                    existing_combinations.add(combo)
                    existing_dates.add(item['date'])
                    existing_units.add(item['unit'])
                
                log(f"        - ê¸°ì¡´ ë°ì´í„° ë¶„ì„ ì™„ë£Œ: {len(existing_combinations)}ê°œ ì¡°í•©, "
                    f"{len(existing_dates)}ê°œ ë‚ ì§œ, {len(existing_units)}ê°œ ë‹¨ìœ„")
            else:
                log("        - ê¸°ì¡´ ë°ì´í„° ì—†ìŒ: ì „ì²´ ì¶”ì¶œ í•„ìš”")
            
            return {
                'existing_combinations': existing_combinations,
                'existing_dates': existing_dates,
                'existing_units': existing_units,
                'has_data': bool(response.data)
            }
                
        except Exception as e:
            log(f"        - Supabase í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "ERROR")
            return {
                'existing_combinations': set(),
                'existing_dates': set(),
                'existing_units': set(),
                'has_data': False
            }
    
    def check_existing_data(self, major_category: str, middle_category: str, 
                           sub_category: str, specification: str, 
                           table_name: str = 'kpi_price_data') -> set:
        """Supabaseì—ì„œ ê¸°ì¡´ ë°ì´í„°ì˜ (ë‚ ì§œ, ì§€ì—­, ê°€ê²©, ê·œê²©, ë‹¨ìœ„) ì¡°í•©ì„ í™•ì¸"""
        
        log(f"        - Supabaseì—ì„œ ê¸°ì¡´ ë°ì´í„° ì¡°íšŒ")
        
        try:
            response = get_supabase_table(supabase, table_name).select(
                'date, region, price, specification, unit'
            ).eq(
                'major_category', major_category
            ).eq(
                'middle_category', middle_category
            ).eq(
                'sub_category', sub_category
            ).eq(
                'specification', specification
            ).execute()
            
            existing_data_set = set()
            if response.data:
                for item in response.data:
                    combo = (item['date'], item['region'], str(item['price']), item['specification'], item['unit'])
                    existing_data_set.add(combo)
                log(f"        - ê¸°ì¡´ ë°ì´í„° ë°œê²¬: {len(existing_data_set)}ê°œ (ë‚ ì§œ-ì§€ì—­-ê°€ê²©-ê·œê²©-ë‹¨ìœ„ ì¡°í•©)")
            else:
                log("        - ê¸°ì¡´ ë°ì´í„° ì—†ìŒ: ì „ì²´ ì¶”ì¶œ í•„ìš”")
            
            return existing_data_set
                
        except Exception as e:
            log(f"        - Supabase í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return set()  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ set ë°˜í™˜
    
    def check_existing_data_batch(self, major_category: str, middle_category: str, 
                                 sub_category: str, target_date_range: tuple = None,
                                 table_name: str = 'kpi_price_data') -> dict:
        """
        ì „ì²´ ì†Œë¶„ë¥˜ì— ëŒ€í•´ 1íšŒë§Œ ì¡°íšŒí•˜ì—¬ ê¸°ì¡´ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ìºì‹œ
        API í˜¸ì¶œì„ ê·œê²©ë³„ ê°œë³„ ì¡°íšŒì—ì„œ ì „ì²´ ì†Œë¶„ë¥˜ 1íšŒ ì¡°íšŒë¡œ ìµœì í™”
        """
        
        log(f"ğŸ” ë°°ì¹˜ ì¤‘ë³µ ê²€ì‚¬: ì „ì²´ ì†Œë¶„ë¥˜ ë°ì´í„° ì¡°íšŒ ì‹œì‘")
        
        try:
            # ì „ì²´ ì†Œë¶„ë¥˜ ë°ì´í„°ë¥¼ 1íšŒë§Œ ì¡°íšŒ
            query = get_supabase_table(supabase, table_name).select(
                'date, region, price, specification, unit'
            ).eq('major_category', major_category)\
             .eq('middle_category', middle_category)\
             .eq('sub_category', sub_category)
            
            # ë‚ ì§œ ë²”ìœ„ í•„í„°ë§ìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™”
            if target_date_range:
                start_date, end_date = target_date_range
                query = query.gte('date', start_date).lte('date', end_date)
                log(f"    ğŸ“… ë‚ ì§œ ë²”ìœ„ í•„í„°: {start_date} ~ {end_date}")
            
            response = query.execute()
            
            # ë©”ëª¨ë¦¬ ê¸°ë°˜ ì¤‘ë³µ ê²€ì‚¬ìš© ìë£Œêµ¬ì¡° ìƒì„±
            existing_data_cache = {
                'combinations': set(),      # (date, region, price, spec, unit) ì¡°í•©
                'by_specification': {},     # ê·œê²©ë³„ ê·¸ë£¹í™”
                'by_date': {},             # ë‚ ì§œë³„ ê·¸ë£¹í™”
                'total_count': 0
            }
            
            if response.data:
                for item in response.data:
                    # ì¤‘ë³µ ê²€ì‚¬ìš© í‚¤ ìƒì„±
                    combo_key = (
                item['date'], 
                item['region'], 
                item['specification'], 
                item['unit']
            )
                    existing_data_cache['combinations'].add(combo_key)
                    
                    # ê·œê²©ë³„ ê·¸ë£¹í™”
                    spec = item['specification']
                    if spec not in existing_data_cache['by_specification']:
                        existing_data_cache['by_specification'][spec] = set()
                    existing_data_cache['by_specification'][spec].add(combo_key)
                    
                    # ë‚ ì§œë³„ ê·¸ë£¹í™”  
                    date = item['date']
                    if date not in existing_data_cache['by_date']:
                        existing_data_cache['by_date'][date] = set()
                    existing_data_cache['by_date'][date].add(combo_key)
                    
                existing_data_cache['total_count'] = len(response.data)
                
                log(f"âœ… ê¸°ì¡´ ë°ì´í„° ìºì‹œ ìƒì„± ì™„ë£Œ:")
                log(f"    ğŸ“Š ì´ ë°ì´í„°: {existing_data_cache['total_count']}ê°œ")
                log(f"    ğŸ”§ ê·œê²© ìˆ˜: {len(existing_data_cache['by_specification'])}ê°œ")
                log(f"    ğŸ“… ë‚ ì§œ ìˆ˜: {len(existing_data_cache['by_date'])}ê°œ")
            else:
                log("ğŸ“­ ê¸°ì¡´ ë°ì´í„° ì—†ìŒ: ì „ì²´ ì‹ ê·œ ë°ì´í„°ë¡œ ì²˜ë¦¬")
            
            return existing_data_cache
                
        except Exception as e:
            log(f"âŒ ë°°ì¹˜ ì¤‘ë³µ ê²€ì‚¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "ERROR")
            return {
                'combinations': set(),
                'by_specification': {},
                'by_date': {},
                'total_count': 0
            }
    
    def filter_duplicates_from_cache(self, new_data: list, 
                                    existing_cache: dict) -> list:
        """
        ë©”ëª¨ë¦¬ ìºì‹œë¥¼ ì‚¬ìš©í•˜ì—¬ ì¤‘ë³µ ë°ì´í„° í•„í„°ë§
        O(1) ì‹œê°„ë³µì¡ë„ë¡œ ê³ ì† ì¤‘ë³µ ì²´í¬
        """
        
        if not new_data:
            return []
            
        filtered_data = []
        duplicate_count = 0
        
        log(f"ğŸ”„ ë©”ëª¨ë¦¬ ê¸°ë°˜ ì¤‘ë³µ í•„í„°ë§ ì‹œì‘: {len(new_data)}ê°œ ë°ì´í„° ì²˜ë¦¬")
        
        for record in new_data:
            # ìƒˆ ë°ì´í„°ì˜ í‚¤ ìƒì„±
            new_key = (
                record['date'],
                record['region'], 
                record['specification'],
                record['unit']
            )
            
            # ë©”ëª¨ë¦¬ì—ì„œ O(1) ì‹œê°„ë³µì¡ë„ë¡œ ì¤‘ë³µ ì²´í¬
            if new_key not in existing_cache['combinations']:
                filtered_data.append(record)
                # ìºì‹œì— ìƒˆ ë°ì´í„° ì¶”ê°€ (ë‹¤ìŒ ì²­í¬ë¥¼ ìœ„í•´)
                existing_cache['combinations'].add(new_key)
            else:
                duplicate_count += 1
        
        log(f"âœ… ì¤‘ë³µ í•„í„°ë§ ì™„ë£Œ:")
        log(f"    ğŸ—‘ï¸  ì¤‘ë³µ ì œê±°: {duplicate_count}ê°œ")
        log(f"    âœ¨ ì‹ ê·œ ë°ì´í„°: {len(filtered_data)}ê°œ")
        
        return filtered_data
    
    def save_to_cache(self, major_category: str, year: int, month: int, data: List[Dict]):
        """
        ë°ì´í„°ë¥¼ ìºì‹œì— ì €ì¥ (í˜„ì¬ëŠ” ë¹„í™œì„±í™”)
        """
        # Redis ìºì‹œ ì‚¬ìš©ì„ ì¤‘ë‹¨í•˜ê³  í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì‚¬ìš©í•˜ë„ë¡ ë³€ê²½
        return True
    
    def filter_new_data_only(self, df: pd.DataFrame, table_name: str = 'kpi_price_data') -> pd.DataFrame:
        """
        ê¸°ì¡´ ë°ì´í„°ì™€ ë¹„êµí•˜ì—¬ ìŠ¤ë§ˆíŠ¸í•œ ì¤‘ë³µ ì²´í¬ ìˆ˜í–‰
        - ì™„ì „ ì¤‘ë³µ: ê±´ë„ˆë›°ê¸°
        - ë¶€ë¶„ ì—…ë°ì´íŠ¸: í•´ë‹¹ ë‚ ì§œë§Œ í¬ë¡¤ë§ í•„ìš”
        - ë‹¨ìœ„ ë³€ê²½: ì „ì²´ ë°ì´í„° ë®ì–´ì“°ê¸° í•„ìš”
        """
        if df.empty:
            return df
        
        original_count = len(df)
        log(f"ğŸ“Š ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì‹œì‘: {original_count}ê°œ")
        
        # 1. ê°€ê²© ë°ì´í„° ê²€ì¦ (NaN ì œê±°)
        df = df.dropna(subset=['price'])
        after_price_check = len(df)
        if original_count != after_price_check:
            log(f"    - ê°€ê²© ë°ì´í„° ì—†ëŠ” í–‰ ì œê±°: {original_count - after_price_check}ê°œ")
        
        # 2. ì§€ì—­ëª… ì²˜ë¦¬ ë° ì •ê·œí™”
        valid_regions = [
            'ì„œìš¸', 'ë¶€ì‚°', 'ëŒ€êµ¬', 'ì¸ì²œ', 'ê´‘ì£¼', 'ëŒ€ì „', 'ìš¸ì‚°', 'ì„¸ì¢…',
            'ê²½ê¸°', 'ê°•ì›', 'ì¶©ë¶', 'ì¶©ë‚¨', 'ì „ë¶', 'ì „ë‚¨', 'ê²½ë¶', 'ê²½ë‚¨', 'ì œì£¼',
            'ì „êµ­', 'ê³µí†µ'
        ]
        
        def process_region_name(region_str):
            if pd.isna(region_str) or not region_str:
                return 'ì „êµ­'
            
            region_str = str(region_str).strip()
            
            # ìœ íš¨í•œ ì§€ì—­ëª…ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            if any(valid_region in region_str for valid_region in valid_regions):
                return region_str
            
            # ê³µí†µì§€ì—­ì´ë‚˜ íŠ¹ì • íŒ¨í„´ í™•ì¸ (ì˜ˆ: ë‚ ì§œì™€ ê°€ê²©ë§Œ ìˆëŠ” ê²½ìš°)
            # ìˆ«ìë‚˜ íŠ¹ìˆ˜ë¬¸ìë§Œ ìˆê±°ë‚˜, ê°€ê²© íŒ¨í„´ì´ ìˆëŠ” ê²½ìš° 'ì „êµ­'ìœ¼ë¡œ ì²˜ë¦¬
            if (region_str.replace(',', '').replace('.', '').replace('-', '').isdigit() or
                'ì›' in region_str or 
                len(region_str) < 2 or
                region_str in ['ê³µí†µ', 'ì „ì²´', 'ê¸°íƒ€', 'ì¼ë°˜']):
                return 'ì „êµ­'
            
            # ê¸°íƒ€ ê²½ìš°ë„ 'ì „êµ­'ìœ¼ë¡œ ì²˜ë¦¬ (ë„ˆë¬´ ì—„ê²©í•œ ê²€ì¦ ë°©ì§€)
            return 'ì „êµ­'
        
        # ì§€ì—­ëª… ì²˜ë¦¬ ì ìš©
        df['region'] = df['region'].apply(process_region_name)
        after_region_check = len(df)
        
        # ì²˜ë¦¬ ê²°ê³¼ ë¡œê¹…
        region_counts = df['region'].value_counts()
        nationwide_count = region_counts.get('ì „êµ­', 0)
        if nationwide_count > 0:
            log(f"    - 'ì „êµ­'ìœ¼ë¡œ ì²˜ë¦¬ëœ ë°ì´í„°: {nationwide_count}ê°œ")
        
        # 3. ì¤‘ë³µ ë°ì´í„° ì œê±° (ê°™ì€ ë‚ ì§œ, ì§€ì—­, ê·œê²©, ê°€ê²©, ë‹¨ìœ„)
        df = df.drop_duplicates(subset=['date', 'region', 'specification', 'price', 'unit'])
        after_duplicate_check = len(df)
        if after_region_check != after_duplicate_check:
            log(f"    - ì¤‘ë³µ ë°ì´í„° ì œê±°: {after_region_check - after_duplicate_check}ê°œ")
        
        log(f"ğŸ“Š ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì™„ë£Œ: {original_count}ê°œ â†’ {after_duplicate_check}ê°œ")
        
        # 4. ë‹¨ìœ„ ê²€ì¦ ë° ìë™ ìˆ˜ì •
        log("ğŸ” ë‹¨ìœ„ ê²€ì¦ ë° ìë™ ìˆ˜ì • ì‹œì‘...")
        df = self.unit_validator.validate_and_fix_units(df)
        after_unit_validation = len(df)
        log(f"ğŸ“Š ë‹¨ìœ„ ê²€ì¦ ì™„ë£Œ: {after_duplicate_check}ê°œ â†’ {after_unit_validation}ê°œ")
        
        if df.empty:
            log("ğŸ“Š í’ˆì§ˆ ê²€ì¦ í›„ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return df
        
        # 5. ìŠ¤ë§ˆíŠ¸ ì¤‘ë³µ ì²´í¬ ë° ì—…ë°ì´íŠ¸ ì „ëµ ê²°ì •
        category_groups = df.groupby(['major_category', 'middle_category', 'sub_category', 'specification'])
        
        new_records = []
        total_records = len(df)
        skipped_count = 0
        partial_update_count = 0
        full_update_count = 0
        
        for (major_cat, middle_cat, sub_cat, spec), group_df in category_groups:
            log(f"    - ìŠ¤ë§ˆíŠ¸ ë¶„ì„: {major_cat} > {middle_cat} > {sub_cat} > {spec}")
            
            # ê¸°ì¡´ ë°ì´í„° ìŠ¤ë§ˆíŠ¸ ë¶„ì„
            existing_analysis = self.check_existing_data_smart(
                major_cat, middle_cat, sub_cat, spec, table_name
            )
            
            if not existing_analysis['has_data']:
                # ê¸°ì¡´ ë°ì´í„° ì—†ìŒ - ì „ì²´ ì¶”ê°€
                for _, record in group_df.iterrows():
                    new_records.append(record.to_dict())
                log(f"        - ì‹ ê·œ ë°ì´í„°: ì „ì²´ {len(group_df)}ê°œ ì¶”ê°€")
                continue
            
            # ë‹¨ìœ„ ë³€ê²½ ê°ì§€
            new_units = set(group_df['unit'].unique())
            existing_units = existing_analysis['existing_units']
            
            if new_units != existing_units:
                # ë‹¨ìœ„ê°€ ë³€ê²½ë¨ - ì „ì²´ ë®ì–´ì“°ê¸° í•„ìš”
                log(f"        - ë‹¨ìœ„ ë³€ê²½ ê°ì§€: {existing_units} â†’ {new_units}")
                log(f"        - ì „ì²´ ë®ì–´ì“°ê¸° í•„ìš”: {len(group_df)}ê°œ")
                
                # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ë§ˆí‚¹ (ì‹¤ì œ ì‚­ì œëŠ” save_to_supabaseì—ì„œ)
                for _, record in group_df.iterrows():
                    record_dict = record.to_dict()
                    record_dict['_force_update'] = True  # ê°•ì œ ì—…ë°ì´íŠ¸ í”Œë˜ê·¸
                    new_records.append(record_dict)
                full_update_count += len(group_df)
                continue
            
            # ë‚ ì§œë³„ ì¤‘ë³µ ì²´í¬
            existing_combinations = existing_analysis['existing_combinations']
            existing_dates = existing_analysis['existing_dates']
            new_dates = set(group_df['date'].unique())
            
            group_new_count = 0
            group_duplicate_count = 0
            
            for _, record in group_df.iterrows():
                record_key = (record['date'], record['region'], str(record['price']), 
                             record['specification'], record['unit'])
                
                if record_key not in existing_combinations:
                    new_records.append(record.to_dict())
                    group_new_count += 1
                else:
                    group_duplicate_count += 1
                    skipped_count += 1
            
            if group_duplicate_count > 0:
                log(f"        - ì™„ì „ ì¤‘ë³µ SKIP: {group_duplicate_count}ê°œ")
            if group_new_count > 0:
                if any(date not in existing_dates for date in new_dates):
                    log(f"        - ë¶€ë¶„ ì—…ë°ì´íŠ¸: ì‹ ê·œ {group_new_count}ê°œ")
                    partial_update_count += group_new_count
                else:
                    log(f"        - ì‹ ê·œ ë°ì´í„°: {group_new_count}ê°œ")
        
        # ê²°ê³¼ ìš”ì•½
        log(f"ğŸ“Š ìŠ¤ë§ˆíŠ¸ ë¶„ì„ ê²°ê³¼:")
        log(f"    - ì „ì²´ {total_records}ê°œ ì¤‘")
        log(f"    - ì™„ì „ ì¤‘ë³µ SKIP: {skipped_count}ê°œ")
        log(f"    - ë¶€ë¶„ ì—…ë°ì´íŠ¸: {partial_update_count}ê°œ")
        log(f"    - ì „ì²´ ë®ì–´ì“°ê¸°: {full_update_count}ê°œ")
        log(f"    - ìµœì¢… ì²˜ë¦¬: {len(new_records)}ê°œ")
        
        return pd.DataFrame(new_records)

    def save_to_supabase(self, data: List[Dict[str, Any]], table_name: str = 'kpi_price_data') -> int:
        """
        Supabaseì— ë°ì´í„° ì €ì¥ - ìµœì í™”ëœ ë°°ì¹˜ ì¤‘ë³µ ê²€ì‚¬ ì ìš© ë° í™˜ê²½ ë³€ìˆ˜ ê¸°ë°˜ URL ì‚¬ìš©
        """
        if not data:
            log("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return 0
        
        # ìœ íš¨ì„± ê²€ì¦
        valid_records = [record for record in data if self._is_valid_record(record)]
        
        if not valid_records:
            log("âŒ ìœ íš¨í•œ ë ˆì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return 0
        
        log(f"âœ… ìœ íš¨ì„± ê²€ì¦ ì™„ë£Œ: {len(valid_records)}ê°œ")
        
        # ì¹´í…Œê³ ë¦¬ë³„ë¡œ ê·¸ë£¹í™”
        category_groups = {}
        for record in valid_records:
            key = (record['major_category'], record['middle_category'], record['sub_category'])
            if key not in category_groups:
                category_groups[key] = []
            category_groups[key].append(record)
        
        total_saved = 0
        
        # --- ìˆ˜ì • ì‹œì‘ ---
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ í”„ë¡ íŠ¸ì—”ë“œ URLì„ ê°€ì ¸ì˜´. ì—†ìœ¼ë©´ ë¡œì»¬ ì£¼ì†Œë¥¼ ê¸°ë³¸ê°’ìœ¼ë¡œ ì‚¬ìš©.
        frontend_url = os.environ.get("FRONTEND_URL", "http://localhost:3000")
        cache_invalidation_url = f"{frontend_url}/api/cache/invalidate"
        # --- ìˆ˜ì • ë ---
        
        # ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ìµœì í™”ëœ ë°°ì¹˜ ì²˜ë¦¬
        for (major_cat, middle_cat, sub_cat), group_records in category_groups.items():
            log(f"ğŸ” ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬: {major_cat} > {middle_cat} > {sub_cat} ({len(group_records)}ê°œ)")
            log(f"    [Supabase] ì €ì¥ ì‹œì‘: {table_name} í…Œì´ë¸”")
            
            target_dates = list(set(record['date'] for record in group_records))
            date_range = (min(target_dates), max(target_dates)) if target_dates else None
            
            existing_cache = self.check_existing_data_batch(
                major_cat, middle_cat, sub_cat, 
                target_date_range=date_range,
                table_name=table_name
            )
            
            filtered_records = self.filter_duplicates_from_cache(group_records, existing_cache)
            
            if not filtered_records:
                log(f"    ğŸ“­ ì‹ ê·œ ë°ì´í„° ì—†ìŒ: ëª¨ë“  ë°ì´í„°ê°€ ì¤‘ë³µ")
                continue
            
            chunk_size = 1000
            chunks = [filtered_records[i:i + chunk_size] for i in range(0, len(filtered_records), chunk_size)]
            
            category_saved = 0
            for i, chunk in enumerate(chunks, 1):
                try:
                    log(f"    [Supabase] Upsert ì‹œë„: {len(chunk)}ê°œ ë ˆì½”ë“œ")
                    insert_response = get_supabase_table(supabase, table_name).upsert(chunk).execute()
                    log(f"    [Supabase] Upsert ì‘ë‹µ ì„±ê³µ")
                    
                    try:
                        # --- ìˆ˜ì •ëœ URL ì‚¬ìš© ---
                        cache_payload = {
                            "type": "material_prices",
                            "materials": list(set([record.get('specification', '') for record in chunk if record.get('specification')]))
                        }
                        cache_response = requests.post(cache_invalidation_url, json=cache_payload, timeout=5)
                        if cache_response.status_code == 200:
                            log(f"    âœ… Redis ìºì‹œ ë¬´íš¨í™” ì„±ê³µ")
                        else:
                            log(f"    âš ï¸ Redis ìºì‹œ ë¬´íš¨í™” ì‹¤íŒ¨: {cache_response.status_code}")
                    except Exception as cache_error:
                        if "Connection refused" in str(cache_error) or "Failed to establish" in str(cache_error):
                            log(f"    âš ï¸ Redis ìºì‹œ ë¬´íš¨í™” ê±´ë„ˆëœ€: í”„ë¡ íŠ¸ì—”ë“œ ì„œë²„({frontend_url}) ë¯¸ì‹¤í–‰", "WARNING")
                        else:
                            log(f"    âš ï¸ Redis ìºì‹œ ë¬´íš¨í™” ì˜¤ë¥˜: {str(cache_error)}", "WARNING")
                    
                    if insert_response.data is not None:
                        chunk_saved = len(insert_response.data) if insert_response.data else 0
                        category_saved += chunk_saved
                        log(f"    âœ… ì²­í¬ {i}: {chunk_saved}ê°œ ì €ì¥ ì™„ë£Œ")
                    else:
                        log(f"    âŒ ì²­í¬ {i}: ì €ì¥ ì‹¤íŒ¨ - ì‘ë‹µ ë°ì´í„° ì—†ìŒ")
                
                except Exception as e:
                    log(f"âŒ ì²­í¬ {i} ì €ì¥ ì‹¤íŒ¨: {str(e)}", "ERROR")
                    log(f"    [Supabase] ì˜¤ë¥˜ ìƒì„¸: {e.args}", "ERROR")
                    continue
            
            total_saved += category_saved
            log(f"    ğŸ“Š ì¹´í…Œê³ ë¦¬ ì €ì¥ ì™„ë£Œ: {category_saved}ê°œ")
        
        log(f"ğŸ‰ ìµœì í™”ëœ ë°°ì¹˜ ì €ì¥ ì™„ë£Œ: ì´ {total_saved}ê°œ ë°ì´í„°")
        return total_saved

    def save_to_supabase_legacy(self, data: List[Dict[str, Any]], table_name: str = 'kpi_price_data') -> int:
        """
        ê¸°ì¡´ save_to_supabase ë©”ì„œë“œ (ë ˆê±°ì‹œ ë²„ì „) - í˜¸í™˜ì„±ì„ ìœ„í•´ ë³´ì¡´ ë° URL ìˆ˜ì •
        """
        if not data:
            log("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return 0
        
        force_update_data = []
        normal_data = []
        
        for record in data:
            if record.get('_force_update', False):
                clean_record = {k: v for k, v in record.items() if k != '_force_update'}
                force_update_data.append(clean_record)
            else:
                normal_data.append(record)
        
        total_saved = 0
        
        # --- ìˆ˜ì • ì‹œì‘ ---
        frontend_url = os.environ.get("FRONTEND_URL", "http://localhost:3000")
        cache_invalidation_url = f"{frontend_url}/api/cache/invalidate"
        # --- ìˆ˜ì • ë ---
        
        if force_update_data:
            log(f"ğŸ”„ ê°•ì œ ì—…ë°ì´íŠ¸ ë°ì´í„° ì²˜ë¦¬: {len(force_update_data)}ê°œ")
            # ... (ì´í•˜ ë¡œì§ì€ ê±°ì˜ ë™ì¼, cache_invalidation_url ë³€ìˆ˜ë§Œ ì‚¬ìš©í•˜ë„ë¡) ...
            for (major_cat, middle_cat, sub_cat, spec), group_records in force_groups.items():
                try:
                    # ... (ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ë¡œì§) ...
                    if valid_records:
                        # ... (ìƒˆ ë°ì´í„° ì‚½ì… ë¡œì§) ...
                        try:
                            # --- ìˆ˜ì •ëœ URL ì‚¬ìš© ---
                            cache_payload = {"type": "material_prices", "materials": [spec]}
                            cache_response = requests.post(cache_invalidation_url, json=cache_payload, timeout=5)
                            # ... (ì‘ë‹µ ì²˜ë¦¬ ë¡œì§) ...
                        except Exception as cache_error:
                            log(f"    âš ï¸ Redis ìºì‹œ ë¬´íš¨í™” ì˜¤ë¥˜: {str(cache_error)}", "WARNING")
                    
                except Exception as e:
                    log(f"âŒ ê°•ì œ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")

        if normal_data:
            log(f"ğŸ’¾ ì¼ë°˜ ë°ì´í„° ì €ì¥: {len(normal_data)}ê°œ")
            # ... (ì´í•˜ ë¡œì§ì€ ê±°ì˜ ë™ì¼, cache_invalidation_url ë³€ìˆ˜ë§Œ ì‚¬ìš©í•˜ë„ë¡) ...
            for i, chunk in enumerate(chunks, 1):
                try:
                    # ... (ì¤‘ë³µ ì œê±° ë° ì‚½ì… ë¡œì§) ...
                    if insert_response.data:
                        # ...
                        try:
                            # --- ìˆ˜ì •ëœ URL ì‚¬ìš© ---
                            cache_payload = {
                                "type": "material_prices",
                                "materials": list(set([record.get('specification', '') for record in chunk if record.get('specification')]))
                            }
                            cache_response = requests.post(cache_invalidation_url, json=cache_payload, timeout=5)
                            # ... (ì‘ë‹µ ì²˜ë¦¬ ë¡œì§) ...
                        except Exception as cache_error:
                            log(f"    âš ï¸ Redis ìºì‹œ ë¬´íš¨í™” ì˜¤ë¥˜: {str(cache_error)}", "WARNING")
                    # ...
                except Exception as e:
                    log(f"âŒ ì²­í¬ {i} ì €ì¥ ì‹¤íŒ¨: {str(e)}")
                    continue
        
        log(f"ğŸ’¾ ì´ {total_saved}ê°œ ë°ì´í„° ì €ì¥ ì™„ë£Œ")
        return total_saved
        
        # 2. ì¼ë°˜ ë°ì´í„° ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§)
        if normal_data:
            log(f"ğŸ’¾ ì¼ë°˜ ë°ì´í„° ì €ì¥: {len(normal_data)}ê°œ")
            
            # ìœ íš¨ì„± ê²€ì¦
            valid_records = [record for record in normal_data if self._is_valid_record(record)]
            
            if not valid_records:
                log("âŒ ìœ íš¨í•œ ë ˆì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return total_saved
            
            log(f"âœ… ìœ íš¨ì„± ê²€ì¦ ì™„ë£Œ: {len(valid_records)}ê°œ")
            
            # ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬ (1000ê°œì”©)
            chunk_size = 1000
            chunks = [valid_records[i:i + chunk_size] for i in range(0, len(valid_records), chunk_size)]
            
            for i, chunk in enumerate(chunks, 1):
                try:
                    # ì¤‘ë³µ ê²€ì‚¬ë¥¼ ìœ„í•œ í‚¤ ìƒì„±
                    chunk_keys = set()
                    for record in chunk:
                        key = (record['date'], record['region'], str(record['price']), 
                              record['specification'], record['unit'])
                        chunk_keys.add(key)
                    
                    # ê¸°ì¡´ ë°ì´í„°ì—ì„œ ì¤‘ë³µë˜ëŠ” í•­ëª© ì‚­ì œ
                    if chunk_keys:
                        # ë‚ ì§œ, ì§€ì—­, ê°€ê²©, ê·œê²©, ë‹¨ìœ„ ì¡°í•©ìœ¼ë¡œ ê¸°ì¡´ ë°ì´í„° ì¡°íšŒ
                        existing_query = get_supabase_table(supabase, table_name).select('*')
                        
                        # ì²­í¬ì˜ ë‚ ì§œ ë²”ìœ„ë¡œ í•„í„°ë§í•˜ì—¬ ì„±ëŠ¥ ìµœì í™”
                        chunk_dates = list(set(record['date'] for record in chunk))
                        if len(chunk_dates) == 1:
                            existing_query = existing_query.eq('date', chunk_dates[0])
                        else:
                            existing_query = existing_query.in_('date', chunk_dates)
                        
                        existing_response = existing_query.execute()
                        
                        if existing_response.data:
                            # ì¤‘ë³µë˜ëŠ” ê¸°ì¡´ ë°ì´í„°ì˜ ID ìˆ˜ì§‘
                            ids_to_delete = []
                            for existing_record in existing_response.data:
                                existing_key = (
                                    existing_record['date'], 
                                    existing_record['region'], 
                                    str(existing_record['price']), 
                                    existing_record['specification'], 
                                    existing_record['unit']
                                )
                                if existing_key in chunk_keys:
                                    ids_to_delete.append(existing_record['id'])
                            
                            # ì¤‘ë³µ ë°ì´í„° ì‚­ì œ
                            if ids_to_delete:
                                delete_response = get_supabase_table(supabase, table_name).delete().in_('id', ids_to_delete).execute()
                                deleted_count = len(delete_response.data) if delete_response.data else 0
                                log(f"    - ì²­í¬ {i}: ì¤‘ë³µ ë°ì´í„° {deleted_count}ê°œ ì‚­ì œ")
                    
                    # ìƒˆ ë°ì´í„° ì‚½ì…
                    insert_response = get_supabase_table(supabase, table_name).insert(chunk).execute()
                    
                    if insert_response.data:
                        chunk_saved = len(insert_response.data)
                        total_saved += chunk_saved
                        log(f"    - ì²­í¬ {i}: {chunk_saved}ê°œ ì €ì¥ ì™„ë£Œ")
                        
                        # Redis ìºì‹œ ë¬´íš¨í™” API í˜¸ì¶œ
                        try:
                            cache_invalidation_url = "http://localhost:3000/api/cache/invalidate"
                            cache_payload = {
                                "type": "material_prices",
                                "materials": list(set([record.get('specification', '') for record in chunk if record.get('specification')]))
                            }
                            cache_response = requests.post(cache_invalidation_url, json=cache_payload, timeout=5)
                            if cache_response.status_code == 200:
                                log(f"    âœ… Redis ìºì‹œ ë¬´íš¨í™” ì„±ê³µ")
                            else:
                                log(f"    âš ï¸ Redis ìºì‹œ ë¬´íš¨í™” ì‹¤íŒ¨: {cache_response.status_code}")
                        except Exception as cache_error:
                            log(f"    âš ï¸ Redis ìºì‹œ ë¬´íš¨í™” ì˜¤ë¥˜: {str(cache_error)}", "WARNING")
                    else:
                        log(f"    - ì²­í¬ {i}: ì €ì¥ ì‹¤íŒ¨ - ì‘ë‹µ ë°ì´í„° ì—†ìŒ")
                
                except Exception as e:
                    log(f"âŒ ì²­í¬ {i} ì €ì¥ ì‹¤íŒ¨: {str(e)}")
                    continue
        
        log(f"ğŸ’¾ ì´ {total_saved}ê°œ ë°ì´í„° ì €ì¥ ì™„ë£Œ")
        return total_saved

    def _is_valid_record(self, record: Dict[str, Any]) -> bool:
        """ë ˆì½”ë“œì˜ ìœ íš¨ì„±ì„ ê²€ì¦"""
        required_fields = ['major_category', 'middle_category', 'sub_category', 
                          'specification', 'region', 'date']
        
        for field in required_fields:
            if field not in record or pd.isna(record[field]) or not record[field]:
                return False
        
        if not self._is_valid_date_value(record['date']):
            return False
        
        price = record.get('price')
        if price is not None and not isinstance(price, (int, float)):
             # NaN ê°’ë„ ìœ íš¨í•˜ì§€ ì•Šì€ ê²ƒìœ¼ë¡œ ì²˜ë¦¬
            if pd.isna(price):
                return False
            return False
        
        return True
    
    def _is_valid_date_value(self, date_value: Any) -> bool:
        """ë‚ ì§œ ê°’ì´ ìœ íš¨í•œì§€ í™•ì¸"""
        if date_value is None or date_value == '':
            return False
        
        if isinstance(date_value, str):
            date_str = date_value.strip()
            
            # "2025. 1" í˜•ì‹ í—ˆìš©
            if re.match(r'^\d{4}\.\s*\d{1,2}$', date_str):
                return True

            # ì¶”ê°€: ë…„-ì›” í˜•ì‹ í—ˆìš© (ì˜ˆ: 2025-09, 2025-9)
            if re.match(r'^\d{4}-\d{1,2}$', date_str):
                return True

            # "2025-01-01" í˜•ì‹ í—ˆìš©
            if re.match(r'^\d{4}-\d{1,2}-\d{1,2}$', date_str):
                return True
            
            # "2025/1/1" í˜•ì‹ í—ˆìš©
            if re.match(r'^\d{4}/\d{1,2}/\d{1,2}$', date_str):
                return True
            
            return False
        
        # datetime ê°ì²´ëŠ” ìœ íš¨
        if hasattr(date_value, 'strftime'):
            return True
        
        return False
    
    def _normalize_date(self, date_value: Any) -> str:
        """ë‚ ì§œë¥¼ YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ì •ê·œí™”"""
        if hasattr(date_value, 'strftime'):
            return date_value.strftime('%Y-%m-%d')
        elif isinstance(date_value, str):
            date_str = date_value.strip()
            
            # "2025. 1" í˜•ì‹ì„ "2025-01-01"ë¡œ ë³€í™˜
            if re.match(r'^\d{4}\.\s*\d{1,2}$', date_str):
                year, month = date_str.replace('.', '').split()
                return f"{year}-{int(month):02d}-01"
            
            # "2025/1" ë˜ëŠ” "2025-1" í˜•ì‹ ì²˜ë¦¬
            if '/' in date_str or '-' in date_str:
                date_str = date_str.replace('/', '-')
                parts = date_str.split('-')
                if len(parts) >= 2:
                    year, month = parts[0], parts[1]
                    day = parts[2] if len(parts) > 2 else '01'
                    return f"{year}-{int(month):02d}-{int(day):02d}"
            
            return date_str
        else:
            return str(date_value)
    
    def get_comparison_json(self) -> str:
        """ì›ë³¸ ë°ì´í„°ì™€ ê°€ê³µëœ ë°ì´í„°ë¥¼ ë¹„êµí•˜ëŠ” JSON ìƒì„±"""
        processed_df = self.to_dataframe()
        raw_data_converted = self._convert_datetime_to_string(self.raw_data_list)
        processed_data_converted = self._convert_datetime_to_string(
            processed_df.to_dict(orient='records'))
        
        return json.dumps({
            "raw_crawled_data": raw_data_converted,
            "pandas_processed_data": processed_data_converted
        }, ensure_ascii=False, indent=4)
    
    def _convert_datetime_to_string(self, obj: Any) -> Any:
        """datetime ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ëŠ” ì¬ê·€ í•¨ìˆ˜"""
        if isinstance(obj, datetime):
            return obj.isoformat()
        elif isinstance(obj, dict):
            return {key: self._convert_datetime_to_string(value) 
                   for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_datetime_to_string(item) for item in obj]
        else:
            return obj

# --- ì´í•˜ KpiDataProcessor, MaterialDataProcessor, create_data_processor í•¨ìˆ˜ëŠ” ë³€ê²½í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤. ---

class KpiDataProcessor(BaseDataProcessor):
    """í•œêµ­ë¬¼ê°€ì •ë³´(KPI) ì‚¬ì´íŠ¸ ì „ìš© ë°ì´í„° ì²˜ë¦¬ê¸°"""
    
    def _normalize_region_name(self, region_name: str) -> str:
        """ì§€ì—­ëª…ì„ ì •ê·œí™”í•˜ê³  ë¹ˆ ê°’ì´ë‚˜ Noneì„ ì²˜ë¦¬"""
        # Noneì´ë‚˜ ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬
        if not region_name or region_name == 'None' or str(region_name).strip() == '':
            return 'ì „êµ­'  # ê¸°ë³¸ê°’ìœ¼ë¡œ 'ì „êµ­' ì„¤ì •
            
        region_str = str(region_name).strip()
        
        # 'ê³µí†µì§€ì—­'ì„ 'ì „êµ­'ìœ¼ë¡œ ë³€í™˜
        if region_str == 'ê³µí†µì§€ì—­':
            return 'ì „êµ­'
            
        # íŒ¨í„´: ì§€ì—­ëª… ì²«ê¸€ì + ìˆ«ì + ì§€ì—­ëª… ë‚˜ë¨¸ì§€ (ì˜ˆ: ì„œ1ìš¸ â†’ ì„œìš¸1)
        pattern = r'^([ê°€-í£])(\d+)([ê°€-í£]+)$'
        match = re.match(pattern, region_str)
        
        if match:
            first_char, number, rest = match.groups()
            return f"{first_char}{rest}{number}"  # ì„œ1ìš¸ â†’ ì„œìš¸1
        
        return region_str  # ë³€í™˜ ë¶ˆê°€ëŠ¥í•œ ê²½ìš° ì›ë³¸ ë°˜í™˜
    
    def _extract_material_name_from_specification(self, specification: str) -> str:
        """SPECIFICATIONì—ì„œ ìì¬ëª…ì„ ì¶”ì¶œí•˜ëŠ” ê·œì¹™"""
        if not specification:
            return specification
        
        spec_str = str(specification).strip()
        
        # ê·œì¹™ 1: HDPE DC ê³ ì••ê´€ ê´€ë ¨
        if 'HDPE' in spec_str and 'DC' in spec_str and 'ê³ ì••ê´€' in spec_str:
            return f"{spec_str} - DCê³ ì••ê´€"
        
        # ê·œì¹™ 2: PVC ê´€ë ¨
        if 'PVC' in spec_str:
            if 'ìƒìˆ˜ë„ê´€' in spec_str:
                return f"{spec_str} - PVCìƒìˆ˜ë„ê´€"
            elif 'í•˜ìˆ˜ë„ê´€' in spec_str:
                return f"{spec_str} - PVCí•˜ìˆ˜ë„ê´€"
            elif 'ë°°ìˆ˜ê´€' in spec_str:
                return f"{spec_str} - PVCë°°ìˆ˜ê´€"
        
        # ê·œì¹™ 3: ì² ê·¼ ê´€ë ¨
        if 'ì² ê·¼' in spec_str:
            if 'SD' in spec_str:
                return f"{spec_str} - SDì² ê·¼"
            elif 'ì´í˜•' in spec_str:
                return f"{spec_str} - ì´í˜•ì² ê·¼"
        
        # ê·œì¹™ 4: ë ˆë¯¸ì½˜ ê´€ë ¨
        if 'ë ˆë¯¸ì½˜' in spec_str or 'ì½˜í¬ë¦¬íŠ¸' in spec_str:
            if 'ê³ ê°•ë„' in spec_str:
                return f"{spec_str} - ê³ ê°•ë„ì½˜í¬ë¦¬íŠ¸"
            elif 'ì¼ë°˜' in spec_str:
                return f"{spec_str} - ì¼ë°˜ì½˜í¬ë¦¬íŠ¸"
        
        # ê·œì¹™ 5: ì•„ìŠ¤íŒ”íŠ¸ ê´€ë ¨
        if 'ì•„ìŠ¤íŒ”íŠ¸' in spec_str:
            if 'í¬ì¥ìš©' in spec_str:
                return f"{spec_str} - í¬ì¥ìš©ì•„ìŠ¤íŒ”íŠ¸"
            elif 'ë°©ìˆ˜ìš©' in spec_str:
                return f"{spec_str} - ë°©ìˆ˜ìš©ì•„ìŠ¤íŒ”íŠ¸"
        
        # ê·œì¹™ 6: ê³¨ì¬ ê´€ë ¨
        if 'ê³¨ì¬' in spec_str:
            if 'ì‡„ì„' in spec_str:
                return f"{spec_str} - ì‡„ì„ê³¨ì¬"
            elif 'ëª¨ë˜' in spec_str:
                return f"{spec_str} - ëª¨ë˜ê³¨ì¬"
        
        # ê·œì¹™ 7: ì‹œë©˜íŠ¸ ê´€ë ¨
        if 'ì‹œë©˜íŠ¸' in spec_str:
            if 'í¬í‹€ëœë“œ' in spec_str:
                return f"{spec_str} - í¬í‹€ëœë“œì‹œë©˜íŠ¸"
            elif 'í˜¼í•©' in spec_str:
                return f"{spec_str} - í˜¼í•©ì‹œë©˜íŠ¸"
        
        # ê·œì¹™ 8: í˜•ê°• ê´€ë ¨
        if 'í˜•ê°•' in spec_str:
            if 'Hí˜•ê°•' in spec_str or 'H-' in spec_str:
                return f"{spec_str} - Hí˜•ê°•"
            elif 'Ií˜•ê°•' in spec_str or 'I-' in spec_str:
                return f"{spec_str} - Ií˜•ê°•"
        
        # ê·œì¹™ 9: ê°•ê´€ ê´€ë ¨
        if 'ê°•ê´€' in spec_str:
            if 'ë°°ê´€ìš©' in spec_str:
                return f"{spec_str} - ë°°ê´€ìš©ê°•ê´€"
            elif 'êµ¬ì¡°ìš©' in spec_str:
                return f"{spec_str} - êµ¬ì¡°ìš©ê°•ê´€"
        
        # ê·œì¹™ 10: ì „ì„  ê´€ë ¨
        if 'ì „ì„ ' in spec_str or 'ì¼€ì´ë¸”' in spec_str:
            if 'CV' in spec_str:
                return f"{spec_str} - CVì¼€ì´ë¸”"
            elif 'HIV' in spec_str:
                return f"{spec_str} - HIVì¼€ì´ë¸”"
            elif 'í†µì‹ ' in spec_str:
                return f"{spec_str} - í†µì‹ ì¼€ì´ë¸”"
        
        # ê¸°ë³¸ê°’: ì›ë³¸ specification ë°˜í™˜
        return spec_str

    async def process_data(self, major_category: str, middle_category: str, sub_category: str) -> List[Dict[str, Any]]:
        """ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë°ì´í„° ê°€ê³µ ë©”ì„œë“œ"""
        try:
            filtered_data = []
            for raw_data in self.raw_data_list:
                if (raw_data.get('major_category_name') == major_category and
                    raw_data.get('middle_category_name') == middle_category and
                    raw_data.get('sub_category_name') == sub_category):
                    filtered_data.append(raw_data)
            
            if not filtered_data:
                return []
            
            processed_items = []
            for raw_item in filtered_data:
                transformed_items = self.transform_to_standard_format(raw_item)
                processed_items.extend(transformed_items)
            
            return processed_items
            
        except Exception as e:
            log(f"ë°ì´í„° ê°€ê³µ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "ERROR")
            return []
    
    async def save_to_supabase(self, processed_data: List[Dict[str, Any]], table_name: str = 'kpi_price_data', check_duplicates: bool = True) -> int:
        """ê°€ê³µëœ ë°ì´í„°ë¥¼ Supabaseì— ì €ì¥"""
        if not processed_data:
            log("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return 0
        
        try:
            log(f"ğŸ“Š ì €ì¥ ì‹œë„: {len(processed_data)}ê°œ ë°ì´í„° â†’ '{table_name}' í…Œì´ë¸”")
            
            # ë¶€ëª¨ í´ë˜ìŠ¤ì˜ save_to_supabase ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ì—¬ ì¤‘ë³µ ì œê±° ë° ì €ì¥ ë¡œì§ ì‹¤í–‰
            actual_saved_count = super().save_to_supabase(processed_data, table_name)
            
            # ì‹¤ì œ ì €ì¥ëœ ê°œìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë©”ì‹œì§€ ì¶œë ¥
            if actual_saved_count > 0:
                log(f"âœ… ì €ì¥ ì™„ë£Œ: {actual_saved_count}ê°œ ë°ì´í„°")
            else:
                log(f"â„¹ï¸ ì €ì¥ ì™„ë£Œ: ì‹ ê·œ ë°ì´í„° ì—†ìŒ (ì „ì²´ {len(processed_data)}ê°œ ëª¨ë‘ ì¤‘ë³µ)")
            
            return actual_saved_count
            
        except Exception as e:
            log(f"âŒ Supabase ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "ERROR")
            return 0
    
    def transform_to_standard_format(self, raw_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """KPI ì‚¬ì´íŠ¸ì˜ ì›ë³¸ ë°ì´í„°ë¥¼ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        transformed_items = []
        
        for spec_data in raw_data.get('spec_data', []):
            has_direct_price = (
                'spec_name' in spec_data and 'region' in spec_data and
                'date' in spec_data and 'price' in spec_data)
            
            if has_direct_price:
                price_value = spec_data.get('price')
                if price_value is not None:
                    try:
                        price_value = float(str(price_value).replace(',', ''))
                    except (ValueError, TypeError):
                        price_value = None
                
                # SPECIFICATIONì—ì„œ ìì¬ëª… ì¶”ì¶œ ì ìš©
                original_spec = spec_data['spec_name']
                enhanced_spec = self._extract_material_name_from_specification(original_spec)
                
                # í¬ë¡¤ë§ëœ ì‹¤ì œ ë‹¨ìœ„ ì •ë³´ ì‚¬ìš© (í•˜ë“œì½”ë”©ëœ 'ì›/í†¤' ëŒ€ì‹ )
                actual_unit = raw_data.get('unit', 'ì›/í†¤')
                
                transformed_items.append({
                    'major_category': raw_data['major_category_name'],
                    'middle_category': raw_data['middle_category_name'],
                    'sub_category': raw_data['sub_category_name'],
                    'specification': enhanced_spec,
                    'unit': actual_unit,
                    'region': self._normalize_region_name(spec_data['region']),
                    'date': spec_data['date'],
                    'price': price_value
                })
            else:
                for price_info in spec_data.get('prices', []):
                    price_value = None
                    if price_info.get('price'):
                        try:
                            price_value = float(str(price_info['price']).replace(',', ''))
                        except (ValueError, AttributeError):
                            price_value = None
                    
                    # SPECIFICATIONì—ì„œ ìì¬ëª… ì¶”ì¶œ ì ìš©
                    original_spec = spec_data['specification_name']
                    enhanced_spec = self._extract_material_name_from_specification(original_spec)
                    
                    # í¬ë¡¤ë§ëœ ì‹¤ì œ ë‹¨ìœ„ ì •ë³´ ì‚¬ìš© (spec_dataì˜ unitì´ ì—†ìœ¼ë©´ raw_dataì˜ unit ì‚¬ìš©)
                    actual_unit = spec_data.get('unit') or raw_data.get('unit', 'ì›/í†¤')
                    
                    transformed_items.append({
                        'major_category': raw_data['major_category_name'],
                        'middle_category': raw_data['middle_category_name'],
                        'sub_category': raw_data['sub_category_name'],
                        'specification': enhanced_spec,
                        'unit': actual_unit,
                        'region': self._normalize_region_name(price_info['region']),
                        'date': price_info['date'],
                        'price': price_value
                    })
        
        return transformed_items
    
    def get_api_usage_summary(self) -> Dict[str, Any]:
        """API ì‚¬ìš©ëŸ‰ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        return api_monitor.get_usage_summary()
    
    def print_api_usage_summary(self):
        """API ì‚¬ìš©ëŸ‰ ìš”ì•½ì„ ì½˜ì†”ì— ì¶œë ¥"""
        api_monitor.print_usage_summary()
    
    def save_api_stats(self, filename: str = None):
        """API ì‚¬ìš©ëŸ‰ í†µê³„ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        if filename is None:
            filename = f"api_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        api_monitor.save_stats(filename)


class MaterialDataProcessor(BaseDataProcessor):
    """ë‹¤ë¥¸ ìì¬ ì‚¬ì´íŠ¸ìš© ë°ì´í„° ì²˜ë¦¬ê¸° (ì˜ˆì‹œ)"""
    
    def transform_to_standard_format(self, raw_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        transformed_items = []
        category = raw_data.get('category', '')
        product_name = raw_data.get('product_name', '')
        
        for price_item in raw_data.get('price_data', []):
            transformed_items.append({
                'major_category': category,
                'middle_category': '',
                'sub_category': product_name,
                'specification': product_name,
                'unit': 'ì›/í†¤',
                'region': price_item.get('location', ''),
                'date': price_item.get('date', ''),
                'price': price_item.get('cost')
            })
        
        return transformed_items


def create_data_processor(site_type: str) -> BaseDataProcessor:
    """ì‚¬ì´íŠ¸ íƒ€ì…ì— ë”°ë¥¸ ë°ì´í„° ì²˜ë¦¬ê¸° ìƒì„±"""
    processors = {
        'kpi': KpiDataProcessor,
        'material': MaterialDataProcessor,
    }
    
    processor_class = processors.get(site_type)
    if not processor_class:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‚¬ì´íŠ¸ íƒ€ì…: {site_type}")
    
    return processor_class()

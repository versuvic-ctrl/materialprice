# ê±´ì„¤ìì¬ ê°€ê²© ëŒ€ì‹œë³´ë“œ - í•µì‹¬ ì•„í‚¤í…ì²˜ ìš”ì•½

## ğŸ—ï¸ í”„ë¡œì íŠ¸ ê°œìš”
- **ëª©ì **: ê±´ì„¤ìì¬ ê°€ê²© ì •ë³´ ìˆ˜ì§‘, ì €ì¥, ì‹œê°í™” ëŒ€ì‹œë³´ë“œ
- **ê¸°ìˆ  ìŠ¤íƒ**: Next.js 15 + TypeScript + Supabase + Redis + Python
- **ë°°í¬**: Vercel (í”„ë¡ íŠ¸ì—”ë“œ) + GitHub Actions (ìë™í™”)

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡° ë° íŒŒì¼ ì—­í• 

```
materials-dashboard/
â”œâ”€â”€ ğŸ“‚ .github/workflows/
â”‚   â””â”€â”€ crawler.yml                    # GitHub Actions ìë™ í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ëŸ¬
â”œâ”€â”€ ğŸ“‚ backend/                        # ë ˆê±°ì‹œ FastAPI ë°±ì—”ë“œ (í˜„ì¬ ë¯¸ì‚¬ìš©)
â”‚   â””â”€â”€ __pycache__/                   # Python ìºì‹œ íŒŒì¼ë§Œ ë‚¨ìŒ
â”œâ”€â”€ ğŸ“‚ crawler/                        # ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œ
â”‚   â””â”€â”€ sites/
â”‚       â”œâ”€â”€ kpi_crawler.py            # KPI ì‚¬ì´íŠ¸ í¬ë¡¤ëŸ¬ (Selenium)
â”‚       â””â”€â”€ data_processor.py         # ë°ì´í„° ì²˜ë¦¬ ë° DB ì €ì¥ (pandas)
â”œâ”€â”€ ğŸ“‚ src/                           # Next.js í”„ë¡ íŠ¸ì—”ë“œ
â”‚   â”œâ”€â”€ app/                          # App Router í˜ì´ì§€ë“¤
â”‚   â”‚   â”œâ”€â”€ page.tsx                  # ë©”ì¸ ëŒ€ì‹œë³´ë“œ
â”‚   â”‚   â”œâ”€â”€ providers.tsx             # React Query ì „ì—­ ì„¤ì •
â”‚   â”‚   â””â”€â”€ iso-piping-editor/        # ISO ë°°ê´€ë„ ì—ë””í„°
â”‚   â”œâ”€â”€ components/                   # ì¬ì‚¬ìš© ì»´í¬ë„ŒíŠ¸
â”‚   â”‚   â”œâ”€â”€ dashboard/                # ëŒ€ì‹œë³´ë“œ ì°¨íŠ¸ ì»´í¬ë„ŒíŠ¸
â”‚   â”‚   â”œâ”€â”€ materials/                # ìì¬ ì„ íƒ/ì°¨íŠ¸ ì»´í¬ë„ŒíŠ¸
â”‚   â”‚   â””â”€â”€ ui/                       # shadcn/ui ê¸°ë³¸ ì»´í¬ë„ŒíŠ¸
â”‚   â”œâ”€â”€ store/
â”‚   â”‚   â””â”€â”€ materialStore.ts          # Zustand ì „ì—­ ìƒíƒœ ê´€ë¦¬
â”‚   â””â”€â”€ lib/
â”‚       â”œâ”€â”€ supabase.ts               # Supabase í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
â”‚       â””â”€â”€ api.ts                    # API í˜¸ì¶œ í•¨ìˆ˜ë“¤
â”œâ”€â”€ ğŸ“‚ supabase/                      # Supabase ì„¤ì •
â”‚   â”œâ”€â”€ migrations/                   # ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ë³€ê²½
â”‚   â””â”€â”€ config.toml                   # Supabase ë¡œì»¬ ì„¤ì •
â”œâ”€â”€ .env.local                        # í™˜ê²½ë³€ìˆ˜ (API í‚¤, DB ì—°ê²° ì •ë³´)
â”œâ”€â”€ package.json                      # í”„ë¡ íŠ¸ì—”ë“œ ì˜ì¡´ì„±
â””â”€â”€ requirements.txt                  # Python ì˜ì¡´ì„±
```

## ğŸ”§ í•µì‹¬ ê¸°ëŠ¥ ë° ì•„í‚¤í…ì²˜

### 1. ğŸ“Š ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œ (Python + Selenium)
**íŒŒì¼**: `crawler/sites/kpi_crawler.py`, `data_processor.py`

- **í¬ë¡¤ë§**: Seleniumìœ¼ë¡œ KPI ì‚¬ì´íŠ¸ì—ì„œ ê±´ì„¤ìì¬ ê°€ê²© ë°ì´í„° ìˆ˜ì§‘
- **ë°ì´í„° ì²˜ë¦¬**: pandasë¡œ ë°ì´í„° ì •ì œ, ë³€í™˜, ê²€ì¦
- **ì¤‘ë³µ ì œê±°**: ê¸°ì¡´ ë°ì´í„°ì™€ ë¹„êµí•˜ì—¬ ì‹ ê·œ ë°ì´í„°ë§Œ ì €ì¥
- **ìë™í™”**: GitHub Actionsë¡œ ë§¤ì¼ ìë™ ì‹¤í–‰

```python
# data_processor.py í•µì‹¬ ë¡œì§
def process_and_save_data(self, data_list, major_category, middle_category, sub_category, specification):
    # pandas DataFrameìœ¼ë¡œ ë³€í™˜
    df = pd.DataFrame(data_list)
    
    # ê¸°ì¡´ ë°ì´í„° í™•ì¸ (Redis ìºì‹œ ìš°ì„ )
    existing_data = self.check_existing_data(...)
    
    # ì‹ ê·œ ë°ì´í„°ë§Œ í•„í„°ë§
    new_data = df[~df.apply(lambda row: tuple(row) in existing_data, axis=1)]
    
    # Supabaseì— ì €ì¥
    if not new_data.empty:
        supabase.table('kpi_price_data').insert(new_data.to_dict('records')).execute()
```

### 2. âš¡ Redis ìºì‹± ì‹œìŠ¤í…œ (24ì‹œê°„ TTL)
**ì„¤ì •**: `.env.local` - `REDIS_URL=rediss://...upstash.io:6380`

- **ëª©ì **: Supabase API í˜¸ì¶œ ì œí•œ (5ë§ŒíšŒ/ì›”) í•´ê²°
- **ìºì‹œ í‚¤**: `existing_data:{table}:{major}:{middle}:{sub}:{spec}`
- **TTL**: 86400ì´ˆ (24ì‹œê°„)
- **ì‘ë™ ë°©ì‹**:
  1. ë°ì´í„° ì¡°íšŒ ì‹œ Redis ìºì‹œ ë¨¼ì € í™•ì¸ (Cache HIT)
  2. ìºì‹œ ì—†ìœ¼ë©´ Supabase ì¡°íšŒ í›„ Redisì— ì €ì¥ (Cache MISS)
  3. 24ì‹œê°„ í›„ ìë™ ë§Œë£Œ

```python
# Redis ìºì‹± ë¡œì§
def check_existing_data(self, ...):
    cache_key = f"existing_data:{table_name}:{major_category}:..."
    
    # 1. Redis ìºì‹œ í™•ì¸
    if redis_client:
        cached_data = redis_client.get(cache_key)
        if cached_data:
            return json.loads(cached_data)  # Cache HIT
    
    # 2. Supabase ì¡°íšŒ
    response = supabase.table(table_name).select(...).execute()
    
    # 3. Redisì— 24ì‹œê°„ ìºì‹±
    redis_client.set(cache_key, json.dumps(data), ex=86400)
```

**âš ï¸ Redis ì—°ê²° ìƒíƒœ**: í˜„ì¬ Timeout ì˜¤ë¥˜ ë°œìƒ - Upstash Redis ì„œë¹„ìŠ¤ í™•ì¸ í•„ìš”

### 3. ğŸ¯ Zustand ìƒíƒœ ê´€ë¦¬
**íŒŒì¼**: `src/store/materialStore.ts`

- **ì „ì—­ ìƒíƒœ**: ìì¬ ì„ íƒ, ë‚ ì§œ ë²”ìœ„, ì°¨íŠ¸ ì„¤ì • ê´€ë¦¬
- **ê³„ì¸µì  ì„ íƒ**: ëŒ€ë¶„ë¥˜ â†’ ì¤‘ë¶„ë¥˜ â†’ ì†Œë¶„ë¥˜ â†’ ê·œê²© (4ë‹¨ê³„)
- **ì°¨íŠ¸ ê´€ë¦¬**: ì„ íƒëœ ìì¬ë“¤ì˜ í‘œì‹œ/ìˆ¨ê¹€ ìƒíƒœ ê´€ë¦¬

```typescript
interface MaterialState {
  // ìì¬ ì„ íƒ ìƒíƒœ
  selectedLevel1: string;  // ëŒ€ë¶„ë¥˜ (ì˜ˆ: ê³µí†µìì¬)
  selectedLevel2: string;  // ì¤‘ë¶„ë¥˜ (ì˜ˆ: ë´‰ê°•)
  selectedLevel3: string;  // ì†Œë¶„ë¥˜ (ì˜ˆ: ìŠ¤íŒŒì´ëŸ´ì² ê·¼)
  selectedLevel4: string;  // ê·œê²© (ì˜ˆ: D10)
  
  // ì°¨íŠ¸ ì„¤ì •
  selectedMaterialsForChart: string[];  // ì°¨íŠ¸ì— í‘œì‹œí•  ìì¬ë“¤
  hiddenMaterials: Set<string>;         // ìˆ¨ê²¨ì§„ ìì¬ë“¤
  
  // ë‚ ì§œ/ê¸°ê°„ ì„¤ì •
  interval: 'weekly' | 'monthly' | 'yearly';
  startDate: string;
  endDate: string;
}
```

### 4. ğŸ¤– GitHub Actions ìë™í™”
**íŒŒì¼**: `.github/workflows/crawler.yml`

- **ìŠ¤ì¼€ì¤„**: ë§¤ì¼ ì˜¤ì „ 9ì‹œ (KST) ìë™ ì‹¤í–‰
- **íŠ¸ë¦¬ê±°**: ìˆ˜ë™ ì‹¤í–‰ (workflow_dispatch) ì§€ì›
- **í™˜ê²½**: Ubuntu ìµœì‹  + Python 3.9 + Chrome
- **ë³´ì•ˆ**: GitHub Secretsë¡œ ë¯¼ê° ì •ë³´ ê´€ë¦¬

```yaml
name: Daily Material Price Crawler
on:
  schedule:
    - cron: '0 0 * * *'  # ë§¤ì¼ UTC 00:00 (KST 09:00)
  workflow_dispatch:     # ìˆ˜ë™ ì‹¤í–‰ ê°€ëŠ¥

jobs:
  crawl:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
      - name: Install dependencies
        run: pip install -r requirements.txt
      - name: Run crawler
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
          REDIS_URL: ${{ secrets.REDIS_URL }}
        run: python crawler/sites/kpi_crawler.py
```

### 5. ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¡° (Supabase PostgreSQL)

**ì£¼ìš” í…Œì´ë¸”**:
- `kpi_price_data`: ìì¬ ê°€ê²© ë°ì´í„°
  - `major_category`, `middle_category`, `sub_category`, `specification`
  - `date`, `region`, `price`
  - `created_at`, `updated_at`

**RPC í•¨ìˆ˜**:
- `get_price_data()`: í•„í„°ë§ëœ ê°€ê²© ë°ì´í„° ì¡°íšŒ
- `get_categories()`: ì¹´í…Œê³ ë¦¬ ê³„ì¸µ êµ¬ì¡° ì¡°íšŒ

### 6. ğŸ“ˆ í”„ë¡ íŠ¸ì—”ë“œ ì•„í‚¤í…ì²˜ (Next.js 15)

**í•µì‹¬ ì»´í¬ë„ŒíŠ¸**:
- `MaterialsChart.tsx`: Recharts ê¸°ë°˜ ê°€ê²© ì°¨íŠ¸
- `DashboardMiniChart.tsx`: ëŒ€ì‹œë³´ë“œ ë¯¸ë‹ˆ ì°¨íŠ¸
- `MaterialSelector.tsx`: 4ë‹¨ê³„ ìì¬ ì„ íƒê¸°

**ìƒíƒœ ê´€ë¦¬ íë¦„**:
1. Zustand Storeì—ì„œ ì„ íƒ ìƒíƒœ ê´€ë¦¬
2. React Queryë¡œ ì„œë²„ ë°ì´í„° ìºì‹±
3. Rechartsë¡œ ì‹¤ì‹œê°„ ì°¨íŠ¸ ë Œë”ë§

## ğŸš€ ë°ì´í„° íë¦„

```
1. GitHub Actions (ë§¤ì¼ 09:00)
   â†“
2. Python Crawler (Selenium)
   â†“
3. ë°ì´í„° ì²˜ë¦¬ (pandas)
   â†“
4. Redis ìºì‹œ í™•ì¸ (24ì‹œê°„ TTL)
   â†“
5. Supabase ì €ì¥/ì¡°íšŒ
   â†“
6. Next.js ëŒ€ì‹œë³´ë“œ (React Query + Zustand)
   â†“
7. ì‚¬ìš©ì ì°¨íŠ¸ ì‹œê°í™” (Recharts)
```

## âš ï¸ í˜„ì¬ ì´ìŠˆ ë° í•´ê²° ë°©ì•ˆ

### 1. Redis ì—°ê²° ë¬¸ì œ
- **ë¬¸ì œ**: Upstash Redis ì—°ê²° íƒ€ì„ì•„ì›ƒ
- **ì˜í–¥**: ìºì‹± ê¸°ëŠ¥ ë¹„í™œì„±í™”, Supabase API í˜¸ì¶œ ì¦ê°€
- **í•´ê²°**: Upstash ëŒ€ì‹œë³´ë“œì—ì„œ Redis ì¸ìŠ¤í„´ìŠ¤ ìƒíƒœ í™•ì¸ í•„ìš”

### 2. API í˜¸ì¶œ ìµœì í™”
- **í˜„ì¬**: Redis ìºì‹±ìœ¼ë¡œ 24ì‹œê°„ ë™ì•ˆ ì¤‘ë³µ ì¡°íšŒ ë°©ì§€
- **íš¨ê³¼**: Supabase ë¬´ë£Œ í”Œëœ 5ë§ŒíšŒ/ì›” ì œí•œ ë‚´ ìš´ì˜ ê°€ëŠ¥

## ğŸ¯ ì£¼ìš” íŠ¹ì§•

1. **ì™„ì „ ìë™í™”**: ë°ì´í„° ìˆ˜ì§‘ë¶€í„° ì‹œê°í™”ê¹Œì§€ ë¬´ì¸ ìš´ì˜
2. **ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ**: ìµœì‹  ê±´ì„¤ìì¬ ê°€ê²© ì •ë³´ ì œê³µ
3. **í™•ì¥ ê°€ëŠ¥**: ìƒˆë¡œìš´ ìì¬ ì¹´í…Œê³ ë¦¬ ì‰½ê²Œ ì¶”ê°€ ê°€ëŠ¥
4. **ë¹„ìš© íš¨ìœ¨**: ë¬´ë£Œ ì„œë¹„ìŠ¤ ì¡°í•©ìœ¼ë¡œ ìš´ì˜ë¹„ ìµœì†Œí™”
5. **ì‚¬ìš©ì ì¹œí™”**: ì§ê´€ì ì¸ 4ë‹¨ê³„ ìì¬ ì„ íƒ ì‹œìŠ¤í…œ